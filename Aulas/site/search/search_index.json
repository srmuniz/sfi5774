{"config":{"lang":["pt"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SFI5774 - Mec\u00e2nica Qu\u00e2ntica Aplicada (2020) Prof. S\u00e9rgio R. Muniz @ IFSC/USP Aten\u00e7\u00e3o!! Esta p\u00e1gina est\u00e1 em constru\u00e7\u00e3o e atualiza\u00e7\u00e3o permanente... Embora ainda num est\u00e1gio preliminar, este site oferece acesso \u00e0s notas de aulas mais recentes da disciplina. Basta acessar os links abaixo (ou as barras de navega\u00e7\u00e3o, ao lado). Notas de aulas Aula 8 Aula 9 Aula 10 Aula 11 Aula 12 Aula 13 Aula 14 Conte\u00fados oficiais desta disciplina O conte\u00fado oficial e atividades desta disciplina est\u00e3o dispon\u00edveis no e-Disciplinas .","title":"Home"},{"location":"#sfi5774-mecanica-quantica-aplicada-2020","text":"Prof. S\u00e9rgio R. Muniz @ IFSC/USP Aten\u00e7\u00e3o!! Esta p\u00e1gina est\u00e1 em constru\u00e7\u00e3o e atualiza\u00e7\u00e3o permanente... Embora ainda num est\u00e1gio preliminar, este site oferece acesso \u00e0s notas de aulas mais recentes da disciplina. Basta acessar os links abaixo (ou as barras de navega\u00e7\u00e3o, ao lado).","title":"SFI5774 - Mec\u00e2nica Qu\u00e2ntica Aplicada (2020)"},{"location":"#notas-de-aulas","text":"Aula 8 Aula 9 Aula 10 Aula 11 Aula 12 Aula 13 Aula 14","title":"Notas de aulas"},{"location":"#conteudos-oficiais-desta-disciplina","text":"O conte\u00fado oficial e atividades desta disciplina est\u00e3o dispon\u00edveis no e-Disciplinas .","title":"Conte\u00fados oficiais desta disciplina"},{"location":"Aula10/","text":"\\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} Algebra de Dirac Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor \\psi\\rangle \\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). 5.6 Operadores lineares Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hhilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)! Exemplos importantes Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} Representa\u00e7\u00e3o de operadores A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) Valores esperados O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado. Autovalores e autovetores Quando um operador age sobre um dado vetor e o resultado \u00e9 o mesmo vetor multiplicado por um escalar, o vetor \u00e9 chamado de autovetor e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico. ...","title":"Aula10"},{"location":"Aula10/#algebra-de-dirac","text":"Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor \\psi\\rangle \\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i).","title":"Algebra de Dirac"},{"location":"Aula10/#56-operadores-lineares","text":"Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hhilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)!","title":"5.6 Operadores lineares"},{"location":"Aula10/#exemplos-importantes","text":"Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1}","title":"Exemplos importantes"},{"location":"Aula10/#representacao-de-operadores","text":"A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)","title":"Representa\u00e7\u00e3o de operadores"},{"location":"Aula10/#valores-esperados","text":"O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado.","title":"Valores esperados"},{"location":"Aula10/#autovalores-e-autovetores","text":"Quando um operador age sobre um dado vetor e o resultado \u00e9 o mesmo vetor multiplicado por um escalar, o vetor \u00e9 chamado de autovetor e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico. ...","title":"Autovalores e autovetores"},{"location":"Aula13_Lab1/","text":"Lab 1: Introdu\u00e7\u00e3o ao Jupyter notebook Neste notebook veremos uma breve introdu\u00e7\u00e3o ao ambiente Jupyter e a linguagem Python . Veremos tamb\u00e9m como utilizar alguns elementos b\u00e1sicos de interesse do nosso curso. Como, por exemplo, expressar n\u00fameros complexos, vetores e matrizes, al\u00e9m de escrever e usar fun\u00e7\u00f5es matem\u00e1ticas e produzir gr\u00e1ficos em Python. Na pr\u00f3xima aula pr\u00e1tica, veremos como usar ferramentas de computa\u00e7\u00e3o alg\u00e9brica, com express\u00f5es anal\u00edticas. Notebook Basics O Jupyter notebook permite reunir num mesmo documento um ambiente interativo com textos e f\u00f3rmulas matem\u00e1ticas , figuras e gr\u00e1ficos , c\u00f3digo em Python (ou outras linguagens), c\u00f3digo HTML e widgets interativos , tudo num s\u00f3 lugar, interagindo de forma integral. \u00c9 \u00f3timo tanto para testar e explorar ideias, como comunic\u00e1-las a outras pessoas. Para ter informa\u00e7\u00e3os de como funciona o ambiente do notebook Jupyter, acesse os links: What is Jupyter Notebook Notebook basics Working with Markdown cells Typesetting Equations (with LaTeX) Para facilitar, ainda mais e tamb\u00e9m permitir voc\u00ea explorar/modificar os c\u00f3digos fontes, eu inclu\u00ed uma c\u00f3pia do diret\u00f3rios de exemplos aqui, na sua \u00e1rea neste servidor. Basta procurar o diret\u00f3rio Jupyter_Tutorial/examples/Notebook no diret\u00f3rio principal, quando voc\u00ea faz o login neste servidor. Alternativamente, poder\u00e1 acessar esse exemplos no GitHub . N\u00e3o deixe de acessar e explorar o link acima (al\u00e9m de acessar o c\u00f3digo neste servidor, voc\u00ea pode baixar o notebook e explorar o c\u00f3digo localmente, para entender seus elementos -- \u00e9 uma \u00f3tima forma de aprender rapidamente!). Para acessar o Jupyter (Python) no seu computador (localmente), eu recomendo instalar a distribui\u00e7\u00e3o Anaconda . A seguir n\u00f3s iremos nos concentra em como usar esse ambiente para as atividades e tarefas desta disciplina. N\u00fameros e opera\u00e7\u00f5es b\u00e1sicas Python \u00e9 uma linguagem interpretada, em contraste com linguagens compiladas, o que significa que, al\u00e9m de gerar programas (scripts) voc\u00ea tamb\u00e9m pode interagir diretamente com o interpretador e, por exemplo, fazer c\u00e1lculos como numa calculadora. 3 + 4 7 3 * 7 21 7 / 3 2.3333333333333335 7 // 3 #divis\u00e3o inteira 2 N\u00fameros complexos 3 + 4 j (3+4j) ( 3 + 4 j ) + ( 7 + 3 j ) (10+7j) ( 3 + 4 j ) * ( 7 + 3 j ) + 511 (520+37j) Podemos definir vari\u00e1veis, atribuindo valores num\u00e9ricos (tanto reais, como complexos) a essas vari\u00e1veis r = 7 z = 3 + 4 j z (3+4j) Usando o m\u00e9todo .conjugate() \u00e9 poss\u00edvel acessar o complexo conjugado do valor associado a uma vari\u00e1vel. z . conjugate () (3-4j) r . conjugate () 7 (\ud83d\ude03) Vari\u00e1veis din\u00e2micas: em Python a atribui\u00e7\u00e3o de vari\u00e1veis \u00e9 feita dinamicamente, no sentido que o tipo da vari\u00e1vel (i.e., como ela \u00e9 armazenada internamente na mem\u00f3ria) \u00e9 determinado automaticamente, dependo do valor atribu\u00eddo a ela. Por isso n\u00e3o \u00e9 necess\u00e1rio escolher a priori o tipo da vari\u00e1vel. \u00c9 s\u00f3 usar... ( mas tome cuidado! ) Listas lista = [ 1 , 4 , 9 , 16 , 25 ] # atribui a lista a uma vari\u00e1vel lista #mostra os resultados (valor da vari\u00e1vel) [1, 4, 9, 16, 25] Voc\u00ea pode acessar e \"manipular\" os valores dos elementos na lista, usando seu \u00edndice. Os \u00edndices em Python sempre come\u00e7am no elemento \"zero\"... lista [ 2 ] # acessa o terceiro elemento da lista 9 lista [ 0 ] # este \u00e9 o primeiro elemento 1 lista [ - 1 ] # e este, consegue imaginar?? 25 Listas podem ter diferentes tipos de elementos, n\u00e3o apenas n\u00fameros: lista2 = [ 'a' , 'b' , 'c' , 'd' , 'texto tamb\u00e9m pode!' , '\ud83d\ude09' , '\ud83d\udc4d\ud83d\ude03\u270c' ] lista + lista2 # o que ser\u00e1 que vai sair disso?? (que opera\u00e7\u00e3o \u00e9 essa?) [1, 4, 9, 16, 25, 'a', 'b', 'c', 'd', 'texto tamb\u00e9m pode!', '\ud83d\ude09', '\ud83d\udc4d\ud83d\ude03\u270c'] Para mais informa\u00e7\u00f5es sobre como usar listas, acesse este link para o tutorial do Python . Estendendo a funcionalidade com bibliotecas A linguagem Python tem um n\u00famero gigantesco de bibliotecas (extens\u00f5es da linguagem) especializadas em tarefas espec\u00edficas. Elas aumentam muito o funcionalidade e os recursos da linguagem. Al\u00e9m da Biblioteca Padr\u00e3o , existe um grande n\u00famero de bibliotecas produzidas pela comunidade de c\u00f3digo aberto, especialmente na \u00e1rea cient\u00edfica . A filosofia da linguagem \u00e9 diferente de v\u00e1rias outras linguagens, mantendo apenas um core (n\u00facleo) m\u00ednimo de palavras reservadas (\"keywords\" e identificadores ), deixando funcionalidades mais espec\u00edficas para essas bibliotecas especializadas. Assim, especialmente para o uso cient\u00edfico do Python, n\u00f3s precisaremos usar essas extens\u00f5es. Usaremos v\u00e1rias delas ao longo deste curso, principalmente Numpy , Scipy , Matplotlib e SymPy , al\u00e9m, claro do pr\u00f3prio Jupyter, que tamb\u00e9m \u00e9 uma extens\u00e3o da linguagem, como resultado da evolu\u00e7\u00e3o do IPython . Abaixo eu mostro como carregar algumas delas. from numpy import array , matrix , dot , outer , sqrt , sin from numpy.linalg import eig , eigvals from matplotlib.pyplot import hist , plot % matplotlib inline Outra forma de fazer isso \u00e9: import numpy as np # aqui atribui-se um \"apelido\" ('alias') para o nome da biblioteca import numpy.linalg as la Vetores Praticamente toda linguagem de programa\u00e7\u00e3o possui algum tipo de estrutura para armazenar dados de um determinado tipo (geralmente n\u00fameros, mas tamb\u00e9m caracteres). Geralmente s\u00e3o chamados de \" arrays \" (que no caso do Python, est\u00e3o na biblioteca Numpy , que foi carregado acima). No nosso idioma eles s\u00e3o normalmente traduzido como \"vetores\". Esses vetores s\u00e3o parecidos, mas nem sempre equivalentes , ao conceito de vetores usado na F\u00edsica ou na \u00c1lgebra Linear. Podemos, por\u00e9m, definir e usar formalmente algo an\u00e1logo aos vetores que temos usado neste curso, mesmo no sentido de espa\u00e7os vetoriais complexos... Abaixo eu mostro como definir vetores linhas e colunas , como os usados na MQ. vl = array ([ 1 , 2 , 3 ]) # um vetor linha vl array([1, 2, 3]) vc = array ([[ 4 ],[ 5 ],[ 6 ]]) # um vetor coluna vc array([[4], [5], [6]]) dot ( vl , vc ) # produto escalar (produto interno): retorna um escalar array([32]) dot ( vc , vl ) # CUIDADO!! A ordem faz diferen\u00e7a... (porque??!) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-21-84108f792119> in <module> ----> 1 dot(vc,vl) # CUIDADO!! A ordem faz diferen\u00e7a... (porque??!) <__array_function__ internals> in dot(*args, **kwargs) ValueError: shapes (3,1) and (3,) not aligned: 1 (dim 1) != 3 (dim 0) Acima voc\u00ea v\u00ea como \u00e9 uma mensagem de erro no Python... \ud83d\udd3a Observe atentamente e ver\u00e1 que tem a ver com as dimens\u00f5es das matrizes que se tentou multiplicar. outer ( vc , vl ) # retorna um operador linear (n\u00e3o um escalar!) array([[ 4, 8, 12], [ 5, 10, 15], [ 6, 12, 18]]) outer ( vl , vc ) # note que a ordem dos vetores mudou, assim como a matriz do operador (como?) array([[ 4, 5, 6], [ 8, 10, 12], [12, 15, 18]]) Voc\u00ea pode aplicar opera\u00e7\u00e3o matem\u00e1ticas sobre vetores e matrizes, atuando em cada elemento, como seria de se esperar... sin ( vc ) # calcula o seno de cada elemento do vetor vc array([[-0.7568025 ], [-0.95892427], [-0.2794155 ]]) sin ( outer ( vc , vl )) # calcula o seno de cada elemento da matriz array([[-0.7568025 , 0.98935825, -0.53657292], [-0.95892427, -0.54402111, 0.65028784], [-0.2794155 , -0.53657292, -0.75098725]]) Vetores de n\u00fameros complexos (como no espa\u00e7o de Hilbert) v1 = array ([ 1 + 2 j , 3 + 2 j , 5 + 1 j , 4 + 0 j ]) v1 * v1 # produto (direto) de dois vetores complexos (voc\u00ea entendeu o resultado??!) array([-3. +4.j, 5.+12.j, 24.+10.j, 16. +0.j]) v1 * v1 . conjugate () # multiplicando pelo complexo conjugado (retorna um real) array([ 5.+0.j, 13.+0.j, 26.+0.j, 16.+0.j]) ( 1 + 2 j ) * ( 1 + 2 j ) (-3+4j) dot ( v1 . conjugate (), v1 ) # use o dot() para obter o produtor interno (escalar) (60+0j) Matrizes Podemos tamb\u00e9m, claro, definir matrizes, que s\u00e3o objetos muito importantes neste curso. Na linguagem Python (usando a Numpy), podemos definir matrizes de duas formas. Uma \u00e9 usando o mesmo comando array , que, na verdade, funciona para \"vetores multidimensionais\" dentro da linguagem. A outra forma ser\u00e1 mostrada tamb\u00e9m nos exemplos. # a two-dimensional array m1 = array ([[ 1 , 0 ],[ 1 , 2 ]]) m1 array([[1, 0], [1, 2]]) # pode calcular a matriz transpostas, com o m\u00e9todo T m1 . T array([[1, 1], [0, 2]]) # podemos tamb\u00e9m calcular (facilmente!) os autovalores e autovetores!! eig ( m1 ) (array([2., 1.]), array([[ 0. , 0.70710678], [ 1. , -0.70710678]])) # \u00e9 f\u00e1cil verificar o resultado tamb\u00e9m dot ( m1 , array ([ 0 , 1 ])) array([0, 2]) Buscando ajuda com os comandos \" ? \" e \" help() \" # use o sinal de interroga\u00e7\u00e3o para acessar o comando help() eig ? # assim tamb\u00e9m funciona... (mas o resultado \u00e9 diferente) help ( eig ) Help on function eig in module numpy.linalg: eig(a) Compute the eigenvalues and right eigenvectors of a square array. Parameters ---------- a : (..., M, M) array Matrices for which the eigenvalues and right eigenvectors will be computed Returns ------- w : (..., M) array The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. The resulting array will be of complex type, unless the imaginary part is zero in which case it will be cast to a real type. When `a` is real the resulting eigenvalues will be real (0 imaginary part) or occur in conjugate pairs v : (..., M, M) array The normalized (unit \"length\") eigenvectors, such that the column ``v[:,i]`` is the eigenvector corresponding to the eigenvalue ``w[i]``. Raises ------ LinAlgError If the eigenvalue computation does not converge. See Also -------- eigvals : eigenvalues of a non-symmetric array. eigh : eigenvalues and eigenvectors of a real symmetric or complex Hermitian (conjugate symmetric) array. eigvalsh : eigenvalues of a real symmetric or complex Hermitian (conjugate symmetric) array. Notes ----- .. versionadded:: 1.8.0 Broadcasting rules apply, see the `numpy.linalg` documentation for details. This is implemented using the ``_geev`` LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays. The number `w` is an eigenvalue of `a` if there exists a vector `v` such that ``dot(a,v) = w * v``. Thus, the arrays `a`, `w`, and `v` satisfy the equations ``dot(a[:,:], v[:,i]) = w[i] * v[:,i]`` for :math:`i \\in \\{0,...,M-1\\}`. The array `v` of eigenvectors may not be of maximum rank, that is, some of the columns may be linearly dependent, although round-off error may obscure that fact. If the eigenvalues are all different, then theoretically the eigenvectors are linearly independent. Likewise, the (complex-valued) matrix of eigenvectors `v` is unitary if the matrix `a` is normal, i.e., if ``dot(a, a.H) = dot(a.H, a)``, where `a.H` denotes the conjugate transpose of `a`. Finally, it is emphasized that `v` consists of the *right* (as in right-hand side) eigenvectors of `a`. A vector `y` satisfying ``dot(y.T, a) = z * y.T`` for some number `z` is called a *left* eigenvector of `a`, and, in general, the left and right eigenvectors of a matrix are not necessarily the (perhaps conjugate) transposes of each other. References ---------- G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, Various pp. Examples -------- >>> from numpy import linalg as LA (Almost) trivial example with real e-values and e-vectors. >>> w, v = LA.eig(np.diag((1, 2, 3))) >>> w; v array([1., 2., 3.]) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) Real matrix possessing complex e-values and e-vectors; note that the e-values are complex conjugates of each other. >>> w, v = LA.eig(np.array([[1, -1], [1, 1]])) >>> w; v array([1.+1.j, 1.-1.j]) array([[0.70710678+0.j , 0.70710678-0.j ], [0. -0.70710678j, 0. +0.70710678j]]) Complex-valued matrix with real e-values (but complex-valued e-vectors); note that ``a.conj().T == a``, i.e., `a` is Hermitian. >>> a = np.array([[1, 1j], [-1j, 1]]) >>> w, v = LA.eig(a) >>> w; v array([2.+0.j, 0.+0.j]) array([[ 0. +0.70710678j, 0.70710678+0.j ], # may vary [ 0.70710678+0.j , -0. +0.70710678j]]) Be careful about round-off error! >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]]) >>> # Theor. e-values are 1 +/- 1e-9 >>> w, v = LA.eig(a) >>> w; v array([1., 1.]) array([[1., 0.], [0., 1.]]) Exemplos e exerc\u00edcios para praticar Exemplo 1 Lembra aquele exemplo... M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = matrix ([[ 1 , 2 ],[ 1 , 0 ]]) M matrix([[1, 2], [1, 0]]) Quais os autovalores e autovetores? eig ( M ) (array([ 2., -1.]), matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]])) Vamos explorar os elementos desse objeto, atribuindo-o a uma vari\u00e1vel. autos = eig ( M ) Ao fazer isso, a fun\u00e7\u00e3o eig(M) retorna um objeto diferente dos que vimos at\u00e9 agora, chamado Tuple . Esse objeto \u00e9 tamb\u00e9m uma cole\u00e7\u00e3o de elementos ordenados, parecido com uma lista, mas \u00e9 indicado pelo s\u00edmbolo de par\u00eanteses, ao inv\u00e9s dos colchetes. Podemos acessar os elementos de um tuple , de forma parecida com as listas e arrays . Veja os exemplos: autos [ 0 ] array([ 2., -1.]) autos [ 1 ] matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]]) Observer que este elemento (o segundo elemento do tuple) \u00e9 uma matriz Numpy. Seus elementos podem ser acessados como qualquer matriz (ou array) no numpy. Por exemplo, para acessar o primeiro vetor linha autos [ 1 ][ 0 ] matrix([[ 0.89442719, -0.70710678]]) dot ( autos [ 1 ][ 0 ], M ) # multiplica\u00e7\u00e3o \u00e0 esquerda de um vetor linha pelo operador M matrix([[0.18732041, 1.78885438]]) Voc\u00ea j\u00e1 deve ter notado que eig(M) retorna os autovalores de M , no primeiro elemento do tuple, e os autovetores, no segundo elemento do tuple. Os autovetores est\u00e3o na forma de vetores colunas. autoV = autos [ 1 ] # transpondo a matriz autoV matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]]) podemos calcular a norma dos vetores colunas atrav\u00e9s do m\u00e9todo numpy.linalg.norm() , que aqui pode ser acessado de uma forma abreviada, pois j\u00e1 carregamos essa numpy.linalg com o alias \" \\textrm{la} \\textrm{la} \" la . norm ( autoV . T [ 1 ]) 1.0 Talvez lhe seja mais familiar, se dividirmos por \\sqrt2 \\sqrt2 e \\sqrt5 \\sqrt5 (fatorando esses termos), como fizemos nas aulas. autoV . T [ 1 ] . T / sqrt ( 2 ) matrix([[-0.5], [ 0.5]]) autoV . T [ 0 ] . T / sqrt ( 5 ) matrix([[0.4], [0.2]]) podemos agora testar se os autovetores est\u00e3o corretos, aplicando o operador sobre eles dot ( M , autoV . T [ 1 ] . T ) / sqrt ( 2 ) # resulta no autovetor multiplicado por seu autovalor matrix([[ 0.5], [-0.5]]) dot ( M , autoV . T [ 0 ] . T ) / sqrt ( 5 ) # resulta no autovetor multiplicado por seu autovalor matrix([[0.8], [0.4]]) Vale lembrar que um autovetor continua sendo um autovetor quando multiplicado por um escalar (a dire\u00e7\u00e3o n\u00e3o muda!), portanto fatores multiplicativos n\u00e3o s\u00e3o relevantes para autovetores. Voc\u00ea pode observar, por exemplo, que o o primeiro autovetores acima, tem um fator de 2 com rela\u00e7\u00e3o ao que hav\u00edamos determinado manualmente, na aula. Transforma\u00e7\u00e3o de similaridade Podemos faze mais um exemplo de uso desse recursos para demostrar o m\u00e9todo de diagonaliza\u00e7\u00e3o discutido nas \u00faltimas aulas. Vimos que a a transforma\u00e7\u00e3o S pode ser constru\u00edda com os autovetores do operador (neste caso, a matriz M ). Neste caso, a matriz S ser\u00e1 exatamente a matriz autoV, calculada acima... S = autoV Si = la . inv ( S ) # calcula a matriz inversa dot ( Si , dot ( M , S )) matrix([[ 2.00000000e+00, 2.22044605e-16], [ 0.00000000e+00, -1.00000000e+00]]) # voc\u00ea pode \"limpar\" os erros num\u00e9ricos de arredondamento, se preferir dot ( Si , dot ( M , S )) . round () matrix([[ 2., 0.], [ 0., -1.]]) Onde podemos ver que a matriz est\u00e1 agora na forma sua forma diagonal. \ud83d\ude09 Exemplo 2 Para praticar um pouco, agora \u00e9 sua vez! Vamos usar as matrizes do Problema 3 da Lista 3 : A= \\begin{pmatrix} -1 & 2i & 0 \\\\ 0 & 4 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} A= \\begin{pmatrix} -1 & 2i & 0 \\\\ 0 & 4 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} B= \\begin{pmatrix} 0 & 2 & i \\\\ -i & 2i & 0 \\\\ 0 & 1 & 4 \\end{pmatrix} B= \\begin{pmatrix} 0 & 2 & i \\\\ -i & 2i & 0 \\\\ 0 & 1 & 4 \\end{pmatrix} Agora \u00e9 sua vez: comece escrevendo essas matriz num formato que a linguagem Python entenda. Depois calcule as duas inversas (na lista foi pedido apenas de A) e verifique se elas comutam . Percebe agora os \" super poderes \" que isso te d\u00e1?? Poderia ser uma matriz de 10x10 ou 1000x1000 que o \\underline{seu} \\underline{seu} trabalho seria essencialmente o mesmo! \ud83d\ude09 Numpy faria todo o trabalho pesado para voc\u00ea!! Parece-me motiva\u00e7\u00e3o suficiente para aprender usar, n\u00e3o? Exemplo 3 Como \u00faltimo exemplo, vamos considerar agora o uma aplica\u00e7\u00e3o de distribui\u00e7\u00e3o estat\u00edstica. Por exemplo, considere que foram feitas v\u00e1rias medidas de um observ\u00e1vel o resultado \u00e9 dado por uma lista de valores, como a abaixo. m = [ 10 , 13 , 14 , 14 , 6 , 8 , 7 , 9 , 12 , 14 , 13 , 11 , 10 , 7 , 7 ] vetor = array ( m ) / 2 #importante converter para array primeiro! print ( m ) print ( vetor ) # usando a fun\u00e7\u00e3o print() para mostrar os valores das vari\u00e1veis m e vetor [10, 13, 14, 14, 6, 8, 7, 9, 12, 14, 13, 11, 10, 7, 7] [5. 6.5 7. 7. 3. 4. 3.5 4.5 6. 7. 6.5 5.5 5. 3.5 3.5] # podemos construi um histograma das medidas usando a fun\u00e7\u00e3o hist() n1 , bins1 , patches1 = hist ( m , bins = 5 , range = ( 5 , 14 )) # a frequ\u00eancia (# de ocorr\u00eancias) em cada bin \u00e9 dada por n1 array([1., 4., 3., 2., 5.]) # observe que m \u00e9 uma lista enquanto vetor \u00e9 um array, mas isso n\u00e3o faz diferen\u00e7a p/ hist() n2 , bins2 , patches2 = hist ( vetor , bins = 5 , range = ( 1 , 14 )) n2 array([4., 6., 5., 0., 0.]) Podemos agora facilmente calcular a probabilidade (a partir das medidas, n\u00e3o a te\u00f3rica) dos resultados dessas medidas. pvals1 = n1 / n1 . sum () pvals1 array([0.06666667, 0.26666667, 0.2 , 0.13333333, 0.33333333]) pvals2 = n2 / n2 . sum () pvals2 array([0.26666667, 0.4 , 0.33333333, 0. , 0. ]) # note que as somas est\u00e3o normalizadas... print ( \"A soma de todas as probabilidades pvals1 =\" , np . sum ( pvals1 )) print ( \"A soma de todas as probabilidades pvals2 =\" , np . sum ( pvals2 )) A soma de todas as probabilidades pvals1 = 1.0 A soma de todas as probabilidades pvals2 = 1.0 Vamos nos aprofundar um pouco mais nisso, pois vale a pena... # o valore m\u00e9dio de <m> m = 2 * vetor sm = m . sum () sm 155.0 np . sum ( m ) 155.0 p = 2 * vetor / sm m1 = dot ( p , m ) # <m> : valor m\u00e9dio de m m1 11.09032258064516 \\langle m \\rangle = \\sum_i p_i\\ m_i \\langle m \\rangle = \\sum_i p_i\\ m_i \\langle m \\rangle^2 = \\sum_i p_i\\ m_i^2 \\langle m \\rangle^2 = \\sum_i p_i\\ m_i^2 \\sigma^2 = \\langle m^2 \\rangle - \\langle m \\rangle^2 \\sigma^2 = \\langle m^2 \\rangle - \\langle m \\rangle^2 m2 = dot ( p , m ** 2 ) m2 130.13548387096773 sigma = sqrt ( m2 - m1 ** 2 ) sigma 2.6721206799468824 pm = np . random . normal ( m1 , sigma , 10000 ) hist ( pm , 100 ); p(x)=\\frac{1}{\\sigma \\sqrt{2\\pi} } \\exp \\left(\\frac{-(x - \\langle m \\rangle)^2}{2 \\sigma^2} \\right) p(x)=\\frac{1}{\\sigma \\sqrt{2\\pi} } \\exp \\left(\\frac{-(x - \\langle m \\rangle)^2}{2 \\sigma^2} \\right) import matplotlib.pyplot as plt count , bins , ignored = plt . hist ( pm , 100 , density = True ) plt . plot ( bins , 1 / ( sigma * np . sqrt ( 2 * np . pi )) * np . exp ( - ( bins - m1 ) ** 2 / ( 2 * sigma ** 2 ) ), linewidth = 2 , color = 'r' ); Sugest\u00e3o: tenta agora usar esses dados para calcular a probabilidade de se obter um determinado valor m_k m_k (ou, mais estritamente falando, um intervalo entre m_k m_k e m_k + dm m_k + dm .). Agora voc\u00ea teria, provavelmente, muito mais chances de fazer uma boa compara\u00e7\u00e3o com os valores te\u00f3ricos calculados (analiticamente) para um operador observ\u00e1vel f\u00edsico.","title":"Aula 13 - Lab 1"},{"location":"Aula13_Lab1/#lab-1-introducao-ao-jupyter-notebook","text":"Neste notebook veremos uma breve introdu\u00e7\u00e3o ao ambiente Jupyter e a linguagem Python . Veremos tamb\u00e9m como utilizar alguns elementos b\u00e1sicos de interesse do nosso curso. Como, por exemplo, expressar n\u00fameros complexos, vetores e matrizes, al\u00e9m de escrever e usar fun\u00e7\u00f5es matem\u00e1ticas e produzir gr\u00e1ficos em Python. Na pr\u00f3xima aula pr\u00e1tica, veremos como usar ferramentas de computa\u00e7\u00e3o alg\u00e9brica, com express\u00f5es anal\u00edticas.","title":"Lab 1: Introdu\u00e7\u00e3o ao Jupyter notebook"},{"location":"Aula13_Lab1/#notebook-basics","text":"O Jupyter notebook permite reunir num mesmo documento um ambiente interativo com textos e f\u00f3rmulas matem\u00e1ticas , figuras e gr\u00e1ficos , c\u00f3digo em Python (ou outras linguagens), c\u00f3digo HTML e widgets interativos , tudo num s\u00f3 lugar, interagindo de forma integral. \u00c9 \u00f3timo tanto para testar e explorar ideias, como comunic\u00e1-las a outras pessoas. Para ter informa\u00e7\u00e3os de como funciona o ambiente do notebook Jupyter, acesse os links: What is Jupyter Notebook Notebook basics Working with Markdown cells Typesetting Equations (with LaTeX) Para facilitar, ainda mais e tamb\u00e9m permitir voc\u00ea explorar/modificar os c\u00f3digos fontes, eu inclu\u00ed uma c\u00f3pia do diret\u00f3rios de exemplos aqui, na sua \u00e1rea neste servidor. Basta procurar o diret\u00f3rio Jupyter_Tutorial/examples/Notebook no diret\u00f3rio principal, quando voc\u00ea faz o login neste servidor. Alternativamente, poder\u00e1 acessar esse exemplos no GitHub . N\u00e3o deixe de acessar e explorar o link acima (al\u00e9m de acessar o c\u00f3digo neste servidor, voc\u00ea pode baixar o notebook e explorar o c\u00f3digo localmente, para entender seus elementos -- \u00e9 uma \u00f3tima forma de aprender rapidamente!). Para acessar o Jupyter (Python) no seu computador (localmente), eu recomendo instalar a distribui\u00e7\u00e3o Anaconda . A seguir n\u00f3s iremos nos concentra em como usar esse ambiente para as atividades e tarefas desta disciplina.","title":"Notebook Basics"},{"location":"Aula13_Lab1/#numeros-e-operacoes-basicas","text":"Python \u00e9 uma linguagem interpretada, em contraste com linguagens compiladas, o que significa que, al\u00e9m de gerar programas (scripts) voc\u00ea tamb\u00e9m pode interagir diretamente com o interpretador e, por exemplo, fazer c\u00e1lculos como numa calculadora. 3 + 4 7 3 * 7 21 7 / 3 2.3333333333333335 7 // 3 #divis\u00e3o inteira 2","title":"N\u00fameros e opera\u00e7\u00f5es b\u00e1sicas"},{"location":"Aula13_Lab1/#numeros-complexos","text":"3 + 4 j (3+4j) ( 3 + 4 j ) + ( 7 + 3 j ) (10+7j) ( 3 + 4 j ) * ( 7 + 3 j ) + 511 (520+37j) Podemos definir vari\u00e1veis, atribuindo valores num\u00e9ricos (tanto reais, como complexos) a essas vari\u00e1veis r = 7 z = 3 + 4 j z (3+4j) Usando o m\u00e9todo .conjugate() \u00e9 poss\u00edvel acessar o complexo conjugado do valor associado a uma vari\u00e1vel. z . conjugate () (3-4j) r . conjugate () 7 (\ud83d\ude03) Vari\u00e1veis din\u00e2micas: em Python a atribui\u00e7\u00e3o de vari\u00e1veis \u00e9 feita dinamicamente, no sentido que o tipo da vari\u00e1vel (i.e., como ela \u00e9 armazenada internamente na mem\u00f3ria) \u00e9 determinado automaticamente, dependo do valor atribu\u00eddo a ela. Por isso n\u00e3o \u00e9 necess\u00e1rio escolher a priori o tipo da vari\u00e1vel. \u00c9 s\u00f3 usar... ( mas tome cuidado! )","title":"N\u00fameros complexos"},{"location":"Aula13_Lab1/#listas","text":"lista = [ 1 , 4 , 9 , 16 , 25 ] # atribui a lista a uma vari\u00e1vel lista #mostra os resultados (valor da vari\u00e1vel) [1, 4, 9, 16, 25] Voc\u00ea pode acessar e \"manipular\" os valores dos elementos na lista, usando seu \u00edndice. Os \u00edndices em Python sempre come\u00e7am no elemento \"zero\"... lista [ 2 ] # acessa o terceiro elemento da lista 9 lista [ 0 ] # este \u00e9 o primeiro elemento 1 lista [ - 1 ] # e este, consegue imaginar?? 25 Listas podem ter diferentes tipos de elementos, n\u00e3o apenas n\u00fameros: lista2 = [ 'a' , 'b' , 'c' , 'd' , 'texto tamb\u00e9m pode!' , '\ud83d\ude09' , '\ud83d\udc4d\ud83d\ude03\u270c' ] lista + lista2 # o que ser\u00e1 que vai sair disso?? (que opera\u00e7\u00e3o \u00e9 essa?) [1, 4, 9, 16, 25, 'a', 'b', 'c', 'd', 'texto tamb\u00e9m pode!', '\ud83d\ude09', '\ud83d\udc4d\ud83d\ude03\u270c'] Para mais informa\u00e7\u00f5es sobre como usar listas, acesse este link para o tutorial do Python .","title":"Listas"},{"location":"Aula13_Lab1/#estendendo-a-funcionalidade-com-bibliotecas","text":"A linguagem Python tem um n\u00famero gigantesco de bibliotecas (extens\u00f5es da linguagem) especializadas em tarefas espec\u00edficas. Elas aumentam muito o funcionalidade e os recursos da linguagem. Al\u00e9m da Biblioteca Padr\u00e3o , existe um grande n\u00famero de bibliotecas produzidas pela comunidade de c\u00f3digo aberto, especialmente na \u00e1rea cient\u00edfica . A filosofia da linguagem \u00e9 diferente de v\u00e1rias outras linguagens, mantendo apenas um core (n\u00facleo) m\u00ednimo de palavras reservadas (\"keywords\" e identificadores ), deixando funcionalidades mais espec\u00edficas para essas bibliotecas especializadas. Assim, especialmente para o uso cient\u00edfico do Python, n\u00f3s precisaremos usar essas extens\u00f5es. Usaremos v\u00e1rias delas ao longo deste curso, principalmente Numpy , Scipy , Matplotlib e SymPy , al\u00e9m, claro do pr\u00f3prio Jupyter, que tamb\u00e9m \u00e9 uma extens\u00e3o da linguagem, como resultado da evolu\u00e7\u00e3o do IPython . Abaixo eu mostro como carregar algumas delas. from numpy import array , matrix , dot , outer , sqrt , sin from numpy.linalg import eig , eigvals from matplotlib.pyplot import hist , plot % matplotlib inline Outra forma de fazer isso \u00e9: import numpy as np # aqui atribui-se um \"apelido\" ('alias') para o nome da biblioteca import numpy.linalg as la","title":"Estendendo a funcionalidade com bibliotecas"},{"location":"Aula13_Lab1/#vetores","text":"Praticamente toda linguagem de programa\u00e7\u00e3o possui algum tipo de estrutura para armazenar dados de um determinado tipo (geralmente n\u00fameros, mas tamb\u00e9m caracteres). Geralmente s\u00e3o chamados de \" arrays \" (que no caso do Python, est\u00e3o na biblioteca Numpy , que foi carregado acima). No nosso idioma eles s\u00e3o normalmente traduzido como \"vetores\". Esses vetores s\u00e3o parecidos, mas nem sempre equivalentes , ao conceito de vetores usado na F\u00edsica ou na \u00c1lgebra Linear. Podemos, por\u00e9m, definir e usar formalmente algo an\u00e1logo aos vetores que temos usado neste curso, mesmo no sentido de espa\u00e7os vetoriais complexos... Abaixo eu mostro como definir vetores linhas e colunas , como os usados na MQ. vl = array ([ 1 , 2 , 3 ]) # um vetor linha vl array([1, 2, 3]) vc = array ([[ 4 ],[ 5 ],[ 6 ]]) # um vetor coluna vc array([[4], [5], [6]]) dot ( vl , vc ) # produto escalar (produto interno): retorna um escalar array([32]) dot ( vc , vl ) # CUIDADO!! A ordem faz diferen\u00e7a... (porque??!) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-21-84108f792119> in <module> ----> 1 dot(vc,vl) # CUIDADO!! A ordem faz diferen\u00e7a... (porque??!) <__array_function__ internals> in dot(*args, **kwargs) ValueError: shapes (3,1) and (3,) not aligned: 1 (dim 1) != 3 (dim 0) Acima voc\u00ea v\u00ea como \u00e9 uma mensagem de erro no Python... \ud83d\udd3a Observe atentamente e ver\u00e1 que tem a ver com as dimens\u00f5es das matrizes que se tentou multiplicar. outer ( vc , vl ) # retorna um operador linear (n\u00e3o um escalar!) array([[ 4, 8, 12], [ 5, 10, 15], [ 6, 12, 18]]) outer ( vl , vc ) # note que a ordem dos vetores mudou, assim como a matriz do operador (como?) array([[ 4, 5, 6], [ 8, 10, 12], [12, 15, 18]]) Voc\u00ea pode aplicar opera\u00e7\u00e3o matem\u00e1ticas sobre vetores e matrizes, atuando em cada elemento, como seria de se esperar... sin ( vc ) # calcula o seno de cada elemento do vetor vc array([[-0.7568025 ], [-0.95892427], [-0.2794155 ]]) sin ( outer ( vc , vl )) # calcula o seno de cada elemento da matriz array([[-0.7568025 , 0.98935825, -0.53657292], [-0.95892427, -0.54402111, 0.65028784], [-0.2794155 , -0.53657292, -0.75098725]])","title":"Vetores"},{"location":"Aula13_Lab1/#vetores-de-numeros-complexos-como-no-espaco-de-hilbert","text":"v1 = array ([ 1 + 2 j , 3 + 2 j , 5 + 1 j , 4 + 0 j ]) v1 * v1 # produto (direto) de dois vetores complexos (voc\u00ea entendeu o resultado??!) array([-3. +4.j, 5.+12.j, 24.+10.j, 16. +0.j]) v1 * v1 . conjugate () # multiplicando pelo complexo conjugado (retorna um real) array([ 5.+0.j, 13.+0.j, 26.+0.j, 16.+0.j]) ( 1 + 2 j ) * ( 1 + 2 j ) (-3+4j) dot ( v1 . conjugate (), v1 ) # use o dot() para obter o produtor interno (escalar) (60+0j)","title":"Vetores de n\u00fameros complexos (como no espa\u00e7o de Hilbert)"},{"location":"Aula13_Lab1/#matrizes","text":"Podemos tamb\u00e9m, claro, definir matrizes, que s\u00e3o objetos muito importantes neste curso. Na linguagem Python (usando a Numpy), podemos definir matrizes de duas formas. Uma \u00e9 usando o mesmo comando array , que, na verdade, funciona para \"vetores multidimensionais\" dentro da linguagem. A outra forma ser\u00e1 mostrada tamb\u00e9m nos exemplos. # a two-dimensional array m1 = array ([[ 1 , 0 ],[ 1 , 2 ]]) m1 array([[1, 0], [1, 2]]) # pode calcular a matriz transpostas, com o m\u00e9todo T m1 . T array([[1, 1], [0, 2]]) # podemos tamb\u00e9m calcular (facilmente!) os autovalores e autovetores!! eig ( m1 ) (array([2., 1.]), array([[ 0. , 0.70710678], [ 1. , -0.70710678]])) # \u00e9 f\u00e1cil verificar o resultado tamb\u00e9m dot ( m1 , array ([ 0 , 1 ])) array([0, 2])","title":"Matrizes"},{"location":"Aula13_Lab1/#buscando-ajuda-com-os-comandos-e-help","text":"# use o sinal de interroga\u00e7\u00e3o para acessar o comando help() eig ? # assim tamb\u00e9m funciona... (mas o resultado \u00e9 diferente) help ( eig ) Help on function eig in module numpy.linalg: eig(a) Compute the eigenvalues and right eigenvectors of a square array. Parameters ---------- a : (..., M, M) array Matrices for which the eigenvalues and right eigenvectors will be computed Returns ------- w : (..., M) array The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. The resulting array will be of complex type, unless the imaginary part is zero in which case it will be cast to a real type. When `a` is real the resulting eigenvalues will be real (0 imaginary part) or occur in conjugate pairs v : (..., M, M) array The normalized (unit \"length\") eigenvectors, such that the column ``v[:,i]`` is the eigenvector corresponding to the eigenvalue ``w[i]``. Raises ------ LinAlgError If the eigenvalue computation does not converge. See Also -------- eigvals : eigenvalues of a non-symmetric array. eigh : eigenvalues and eigenvectors of a real symmetric or complex Hermitian (conjugate symmetric) array. eigvalsh : eigenvalues of a real symmetric or complex Hermitian (conjugate symmetric) array. Notes ----- .. versionadded:: 1.8.0 Broadcasting rules apply, see the `numpy.linalg` documentation for details. This is implemented using the ``_geev`` LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays. The number `w` is an eigenvalue of `a` if there exists a vector `v` such that ``dot(a,v) = w * v``. Thus, the arrays `a`, `w`, and `v` satisfy the equations ``dot(a[:,:], v[:,i]) = w[i] * v[:,i]`` for :math:`i \\in \\{0,...,M-1\\}`. The array `v` of eigenvectors may not be of maximum rank, that is, some of the columns may be linearly dependent, although round-off error may obscure that fact. If the eigenvalues are all different, then theoretically the eigenvectors are linearly independent. Likewise, the (complex-valued) matrix of eigenvectors `v` is unitary if the matrix `a` is normal, i.e., if ``dot(a, a.H) = dot(a.H, a)``, where `a.H` denotes the conjugate transpose of `a`. Finally, it is emphasized that `v` consists of the *right* (as in right-hand side) eigenvectors of `a`. A vector `y` satisfying ``dot(y.T, a) = z * y.T`` for some number `z` is called a *left* eigenvector of `a`, and, in general, the left and right eigenvectors of a matrix are not necessarily the (perhaps conjugate) transposes of each other. References ---------- G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, Various pp. Examples -------- >>> from numpy import linalg as LA (Almost) trivial example with real e-values and e-vectors. >>> w, v = LA.eig(np.diag((1, 2, 3))) >>> w; v array([1., 2., 3.]) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) Real matrix possessing complex e-values and e-vectors; note that the e-values are complex conjugates of each other. >>> w, v = LA.eig(np.array([[1, -1], [1, 1]])) >>> w; v array([1.+1.j, 1.-1.j]) array([[0.70710678+0.j , 0.70710678-0.j ], [0. -0.70710678j, 0. +0.70710678j]]) Complex-valued matrix with real e-values (but complex-valued e-vectors); note that ``a.conj().T == a``, i.e., `a` is Hermitian. >>> a = np.array([[1, 1j], [-1j, 1]]) >>> w, v = LA.eig(a) >>> w; v array([2.+0.j, 0.+0.j]) array([[ 0. +0.70710678j, 0.70710678+0.j ], # may vary [ 0.70710678+0.j , -0. +0.70710678j]]) Be careful about round-off error! >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]]) >>> # Theor. e-values are 1 +/- 1e-9 >>> w, v = LA.eig(a) >>> w; v array([1., 1.]) array([[1., 0.], [0., 1.]])","title":"Buscando ajuda com os comandos \"?\"  e \" help() \""},{"location":"Aula13_Lab1/#exemplos-e-exercicios-para-praticar","text":"","title":"Exemplos e exerc\u00edcios para praticar"},{"location":"Aula13_Lab1/#exemplo-1","text":"Lembra aquele exemplo... M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = matrix ([[ 1 , 2 ],[ 1 , 0 ]]) M matrix([[1, 2], [1, 0]]) Quais os autovalores e autovetores? eig ( M ) (array([ 2., -1.]), matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]])) Vamos explorar os elementos desse objeto, atribuindo-o a uma vari\u00e1vel. autos = eig ( M ) Ao fazer isso, a fun\u00e7\u00e3o eig(M) retorna um objeto diferente dos que vimos at\u00e9 agora, chamado Tuple . Esse objeto \u00e9 tamb\u00e9m uma cole\u00e7\u00e3o de elementos ordenados, parecido com uma lista, mas \u00e9 indicado pelo s\u00edmbolo de par\u00eanteses, ao inv\u00e9s dos colchetes. Podemos acessar os elementos de um tuple , de forma parecida com as listas e arrays . Veja os exemplos: autos [ 0 ] array([ 2., -1.]) autos [ 1 ] matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]]) Observer que este elemento (o segundo elemento do tuple) \u00e9 uma matriz Numpy. Seus elementos podem ser acessados como qualquer matriz (ou array) no numpy. Por exemplo, para acessar o primeiro vetor linha autos [ 1 ][ 0 ] matrix([[ 0.89442719, -0.70710678]]) dot ( autos [ 1 ][ 0 ], M ) # multiplica\u00e7\u00e3o \u00e0 esquerda de um vetor linha pelo operador M matrix([[0.18732041, 1.78885438]]) Voc\u00ea j\u00e1 deve ter notado que eig(M) retorna os autovalores de M , no primeiro elemento do tuple, e os autovetores, no segundo elemento do tuple. Os autovetores est\u00e3o na forma de vetores colunas. autoV = autos [ 1 ] # transpondo a matriz autoV matrix([[ 0.89442719, -0.70710678], [ 0.4472136 , 0.70710678]]) podemos calcular a norma dos vetores colunas atrav\u00e9s do m\u00e9todo numpy.linalg.norm() , que aqui pode ser acessado de uma forma abreviada, pois j\u00e1 carregamos essa numpy.linalg com o alias \" \\textrm{la} \\textrm{la} \" la . norm ( autoV . T [ 1 ]) 1.0 Talvez lhe seja mais familiar, se dividirmos por \\sqrt2 \\sqrt2 e \\sqrt5 \\sqrt5 (fatorando esses termos), como fizemos nas aulas. autoV . T [ 1 ] . T / sqrt ( 2 ) matrix([[-0.5], [ 0.5]]) autoV . T [ 0 ] . T / sqrt ( 5 ) matrix([[0.4], [0.2]]) podemos agora testar se os autovetores est\u00e3o corretos, aplicando o operador sobre eles dot ( M , autoV . T [ 1 ] . T ) / sqrt ( 2 ) # resulta no autovetor multiplicado por seu autovalor matrix([[ 0.5], [-0.5]]) dot ( M , autoV . T [ 0 ] . T ) / sqrt ( 5 ) # resulta no autovetor multiplicado por seu autovalor matrix([[0.8], [0.4]]) Vale lembrar que um autovetor continua sendo um autovetor quando multiplicado por um escalar (a dire\u00e7\u00e3o n\u00e3o muda!), portanto fatores multiplicativos n\u00e3o s\u00e3o relevantes para autovetores. Voc\u00ea pode observar, por exemplo, que o o primeiro autovetores acima, tem um fator de 2 com rela\u00e7\u00e3o ao que hav\u00edamos determinado manualmente, na aula. Transforma\u00e7\u00e3o de similaridade Podemos faze mais um exemplo de uso desse recursos para demostrar o m\u00e9todo de diagonaliza\u00e7\u00e3o discutido nas \u00faltimas aulas. Vimos que a a transforma\u00e7\u00e3o S pode ser constru\u00edda com os autovetores do operador (neste caso, a matriz M ). Neste caso, a matriz S ser\u00e1 exatamente a matriz autoV, calculada acima... S = autoV Si = la . inv ( S ) # calcula a matriz inversa dot ( Si , dot ( M , S )) matrix([[ 2.00000000e+00, 2.22044605e-16], [ 0.00000000e+00, -1.00000000e+00]]) # voc\u00ea pode \"limpar\" os erros num\u00e9ricos de arredondamento, se preferir dot ( Si , dot ( M , S )) . round () matrix([[ 2., 0.], [ 0., -1.]]) Onde podemos ver que a matriz est\u00e1 agora na forma sua forma diagonal. \ud83d\ude09","title":"Exemplo 1"},{"location":"Aula13_Lab1/#exemplo-2","text":"Para praticar um pouco, agora \u00e9 sua vez! Vamos usar as matrizes do Problema 3 da Lista 3 : A= \\begin{pmatrix} -1 & 2i & 0 \\\\ 0 & 4 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} A= \\begin{pmatrix} -1 & 2i & 0 \\\\ 0 & 4 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} B= \\begin{pmatrix} 0 & 2 & i \\\\ -i & 2i & 0 \\\\ 0 & 1 & 4 \\end{pmatrix} B= \\begin{pmatrix} 0 & 2 & i \\\\ -i & 2i & 0 \\\\ 0 & 1 & 4 \\end{pmatrix} Agora \u00e9 sua vez: comece escrevendo essas matriz num formato que a linguagem Python entenda. Depois calcule as duas inversas (na lista foi pedido apenas de A) e verifique se elas comutam . Percebe agora os \" super poderes \" que isso te d\u00e1?? Poderia ser uma matriz de 10x10 ou 1000x1000 que o \\underline{seu} \\underline{seu} trabalho seria essencialmente o mesmo! \ud83d\ude09 Numpy faria todo o trabalho pesado para voc\u00ea!! Parece-me motiva\u00e7\u00e3o suficiente para aprender usar, n\u00e3o?","title":"Exemplo 2"},{"location":"Aula13_Lab1/#exemplo-3","text":"Como \u00faltimo exemplo, vamos considerar agora o uma aplica\u00e7\u00e3o de distribui\u00e7\u00e3o estat\u00edstica. Por exemplo, considere que foram feitas v\u00e1rias medidas de um observ\u00e1vel o resultado \u00e9 dado por uma lista de valores, como a abaixo. m = [ 10 , 13 , 14 , 14 , 6 , 8 , 7 , 9 , 12 , 14 , 13 , 11 , 10 , 7 , 7 ] vetor = array ( m ) / 2 #importante converter para array primeiro! print ( m ) print ( vetor ) # usando a fun\u00e7\u00e3o print() para mostrar os valores das vari\u00e1veis m e vetor [10, 13, 14, 14, 6, 8, 7, 9, 12, 14, 13, 11, 10, 7, 7] [5. 6.5 7. 7. 3. 4. 3.5 4.5 6. 7. 6.5 5.5 5. 3.5 3.5] # podemos construi um histograma das medidas usando a fun\u00e7\u00e3o hist() n1 , bins1 , patches1 = hist ( m , bins = 5 , range = ( 5 , 14 )) # a frequ\u00eancia (# de ocorr\u00eancias) em cada bin \u00e9 dada por n1 array([1., 4., 3., 2., 5.]) # observe que m \u00e9 uma lista enquanto vetor \u00e9 um array, mas isso n\u00e3o faz diferen\u00e7a p/ hist() n2 , bins2 , patches2 = hist ( vetor , bins = 5 , range = ( 1 , 14 )) n2 array([4., 6., 5., 0., 0.]) Podemos agora facilmente calcular a probabilidade (a partir das medidas, n\u00e3o a te\u00f3rica) dos resultados dessas medidas. pvals1 = n1 / n1 . sum () pvals1 array([0.06666667, 0.26666667, 0.2 , 0.13333333, 0.33333333]) pvals2 = n2 / n2 . sum () pvals2 array([0.26666667, 0.4 , 0.33333333, 0. , 0. ]) # note que as somas est\u00e3o normalizadas... print ( \"A soma de todas as probabilidades pvals1 =\" , np . sum ( pvals1 )) print ( \"A soma de todas as probabilidades pvals2 =\" , np . sum ( pvals2 )) A soma de todas as probabilidades pvals1 = 1.0 A soma de todas as probabilidades pvals2 = 1.0 Vamos nos aprofundar um pouco mais nisso, pois vale a pena... # o valore m\u00e9dio de <m> m = 2 * vetor sm = m . sum () sm 155.0 np . sum ( m ) 155.0 p = 2 * vetor / sm m1 = dot ( p , m ) # <m> : valor m\u00e9dio de m m1 11.09032258064516 \\langle m \\rangle = \\sum_i p_i\\ m_i \\langle m \\rangle = \\sum_i p_i\\ m_i \\langle m \\rangle^2 = \\sum_i p_i\\ m_i^2 \\langle m \\rangle^2 = \\sum_i p_i\\ m_i^2 \\sigma^2 = \\langle m^2 \\rangle - \\langle m \\rangle^2 \\sigma^2 = \\langle m^2 \\rangle - \\langle m \\rangle^2 m2 = dot ( p , m ** 2 ) m2 130.13548387096773 sigma = sqrt ( m2 - m1 ** 2 ) sigma 2.6721206799468824 pm = np . random . normal ( m1 , sigma , 10000 ) hist ( pm , 100 ); p(x)=\\frac{1}{\\sigma \\sqrt{2\\pi} } \\exp \\left(\\frac{-(x - \\langle m \\rangle)^2}{2 \\sigma^2} \\right) p(x)=\\frac{1}{\\sigma \\sqrt{2\\pi} } \\exp \\left(\\frac{-(x - \\langle m \\rangle)^2}{2 \\sigma^2} \\right) import matplotlib.pyplot as plt count , bins , ignored = plt . hist ( pm , 100 , density = True ) plt . plot ( bins , 1 / ( sigma * np . sqrt ( 2 * np . pi )) * np . exp ( - ( bins - m1 ) ** 2 / ( 2 * sigma ** 2 ) ), linewidth = 2 , color = 'r' ); Sugest\u00e3o: tenta agora usar esses dados para calcular a probabilidade de se obter um determinado valor m_k m_k (ou, mais estritamente falando, um intervalo entre m_k m_k e m_k + dm m_k + dm .). Agora voc\u00ea teria, provavelmente, muito mais chances de fazer uma boa compara\u00e7\u00e3o com os valores te\u00f3ricos calculados (analiticamente) para um operador observ\u00e1vel f\u00edsico.","title":"Exemplo 3"},{"location":"Aula14_Lab2/","text":"Lab 2: Computa\u00e7\u00e3o Simb\u00f3lica em Python Neste notebook veremos como utilizar recursos de computa\u00e7\u00e3o alg\u00e9brica, com express\u00f5es anal\u00edticas, dentro do ambiente Jupyter notebook. Para isso, usaremos a biblioteca SymPy . A inten\u00e7\u00e3o aqui \u00e9 fornecer uma introdu\u00e7\u00e3o b\u00e1sica ao tema, concentrando-se em como usar esses recursos no contexto do nosso curso. Para mais informa\u00e7\u00f5es, voc\u00ea pode consultar a excelente documenta\u00e7\u00e3o do SymPy que est\u00e1 dispon\u00edvel online. Um bom ponto de partida \u00e9 o Tutorial do SymPy . Em particular, n\u00e3o deixe de ler Gotchas! , para evitar problemas comuns de sintaxe!! \ud83d\ude09 Computa\u00e7\u00e3o Simb\u00f3lica A computa\u00e7\u00e3o simb\u00f3lica envolve a computa\u00e7\u00e3o de objetos matem\u00e1ticos simbolicamente. Isso significa que os objetos matem\u00e1ticos s\u00e3o representados exatamente, n\u00e3o aproximadamente, e express\u00f5es matem\u00e1ticas com vari\u00e1veis n\u00e3o avaliadas s\u00e3o deixadas em forma simb\u00f3lica, ou alg\u00e9brica. Para entender a diferen\u00e7a veja os exemplos abaixo import math print ( 'A raiz de 9 \u00e9:' , math . sqrt ( 9 )) print ( 'A raiz de 8 \u00e9:' , math . sqrt ( 8 )) A raiz de 9 \u00e9: 3.0 A raiz de 8 \u00e9: 2.8284271247461903 import sympy print ( 'A raiz de 9 \u00e9:' , sympy . sqrt ( 9 )) print ( 'A raiz de 8 \u00e9:' , sympy . sqrt ( 8 )) A raiz de 9 \u00e9: 3 A raiz de 8 \u00e9: 2*sqrt(2) Na aula passada, algu\u00e9m perguntou porque o seno de \\pi \\pi n\u00e3o era exatamente zero num dos resultados. Expliquei que isso era devido a aproxima\u00e7\u00e3o finita que sempre existe ao representar um n\u00famero no computador. As ferramentas de computa\u00e7\u00e3o alg\u00e9brica (CAS - Computer Algebra System ), como o SymPy, usam recursos especiais para representar um n\u00famero sem aproxima\u00e7\u00f5es na mem\u00f3ria do computador. Isso vem com um custo computacional, mas \u00e9 bastante \u00fatil em v\u00e1rias situa\u00e7\u00f5es, como veremos. Veja abaixo como SymPy avalia o \\text{sen}(\\pi) \\text{sen}(\\pi) sympy . sin ( sympy . pi ) # resultado do seno de pi, com o SymPy \\displaystyle 0 \\displaystyle 0 math . sin ( math . pi ) # resultado do seno de pi, com a biblioteca padr\u00e3o do Python 1.2246467991473532e-16 Al\u00e9m de eliminar os erros de arredondamento , a computa\u00e7\u00e3o alg\u00e9brica permite manipular algebricamente express\u00f5es matem\u00e1tica e at\u00e9 mesmo aplicar ferramentas de C\u00e1lculo, como derivadas e integrais. Veremos exemplos mais adiante. \ud83e\udd14 Como usar? - vari\u00e1veis simb\u00f3licas De maneira bem resumida, al\u00e9m de carregar a biblioteca SymPy e aprender sua sintaxe, o primeiro passo principal \u00e9 definir uma vari\u00e1vel como simb\u00f3lica , como \u00e9 mostrado nos exemplos abaixo: import sympy x , y , z , t = sympy . symbols ( 'x y z t' ) Agora os identificadores (vari\u00e1veis) \"x, y, z, t\" s\u00e3o vari\u00e1veis simb\u00f3licas e ser\u00e3o tratadas como objetos sympy, e n\u00e3o mais como identificadores usuais do Python. Podemos, ent\u00e3o definir fun\u00e7\u00f5es com essas vari\u00e1veis e us\u00e1-las em express\u00f5es alg\u00e9bricas. O exemplo abaixo, calcula a derivada de e^{-2x^2} e^{-2x^2} : sympy . diff ( sympy . exp ( - 2 * x ** 2 )) \\displaystyle - 4 x e^{- 2 x^{2}} \\displaystyle - 4 x e^{- 2 x^{2}} Para simplificar a sintaxe, e evitar ficar digitando sympy. o tempo todo, podemos carregar as fun\u00e7\u00f5es diretamente (mas cuidado com isso!!), ou usar uma atalho (alias), com j\u00e1 mostramos na \u00faltima aula. Aqui, para simplificar, eu vou carregar todas as fun\u00e7\u00f5es que usaremos neste tutorial, na linha abaixo. from sympy import sin , cos , exp , diff , integrate , series , solve , pi , Matrix , simplify , oo diff ( exp ( - 2 * x ** 2 )) \\displaystyle - 4 x e^{- 2 x^{2}} \\displaystyle - 4 x e^{- 2 x^{2}} Podemos usar solve para resolver equa\u00e7\u00f5es, como, por exemplo x^2 - 8 = 0 x^2 - 8 = 0 solve ( x ** 2 - 8 , x ) [-2*sqrt(2), 2*sqrt(2)] para tornar a apresenta\u00e7\u00e3o das resposta mais agrad\u00e1vel, podemos usar o comando sympy . init_printing ( use_latex = True ) solve ( x ** 2 - 8 , x ) \\displaystyle \\left[ - 2 \\sqrt{2}, \\ 2 \\sqrt{2}\\right] \\displaystyle \\left[ - 2 \\sqrt{2}, \\ 2 \\sqrt{2}\\right] Ou simplificar express\u00f5es alg\u00e9bricas sympy . pprint (( x ** 3 + x ** 2 - x - 1 ) / ( x ** 2 + 2 * x + 1 )) simplify (( x ** 3 + x ** 2 - x - 1 ) / ( x ** 2 + 2 * x + 1 )) 3 2 x + x - x - 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 2 x + 2\u22c5x + 1 \\displaystyle x - 1 \\displaystyle x - 1 Fun\u00e7\u00f5es e gr\u00e1ficos A forma mais simples de definir uma fun\u00e7\u00e3o no SymPy \u00e9 simplesmente definir uma nova vari\u00e1vel atribuindo uma express\u00e3o com vari\u00e1veis simb\u00f3licas expr = x ** 3 - 5 * x ** 2 + 3 * x + 2 expr \\displaystyle x^{3} - 5 x^{2} + 3 x + 2 \\displaystyle x^{3} - 5 x^{2} + 3 x + 2 sympy . plot ( expr ,( x , - 1.5 , 5 )); # \u00e9 f\u00e1cil plotar o gr\u00e1fico de uma fun\u00e7\u00e3o Para mais informa\u00e7\u00f5es veja a documenta\u00e7\u00e3o online . Matrizes \u00c9 poss\u00edvel definir matrizes com o comando Matrix() M = Matrix ([[ 1 , 2 ],[ 1 , 0 ]]) # define M com a nossa velha conhecida matriz... M \\displaystyle \\left[\\begin{matrix}1 & 2\\\\1 & 0\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}1 & 2\\\\1 & 0\\end{matrix}\\right] M . det () # calcula o determinante da matriz M \\displaystyle -2 \\displaystyle -2 M . inv () # matriz inversa \\displaystyle \\left[\\begin{matrix}0 & 1\\\\\\frac{1}{2} & - \\frac{1}{2}\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}0 & 1\\\\\\frac{1}{2} & - \\frac{1}{2}\\end{matrix}\\right] M . eigenvals () # autovalores \\displaystyle \\left\\{ -1 : 1, \\ 2 : 1\\right\\} \\displaystyle \\left\\{ -1 : 1, \\ 2 : 1\\right\\} M . eigenvects () # autovetores \\displaystyle \\left[ \\left( -1, \\ 1, \\ \\left[ \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 2, \\ 1, \\ \\left[ \\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\right]\\right)\\right] \\displaystyle \\left[ \\left( -1, \\ 1, \\ \\left[ \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 2, \\ 1, \\ \\left[ \\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\right]\\right)\\right] sympy . eye ( 4 ) # matriz identidade de ordem 4 \\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right] Podemos tamb\u00e9m definir uma matriz de vari\u00e1veis e fun\u00e7\u00f5es simb\u00f3licas! \ud83d\ude09 G = Matrix ([[ x ** 2 , y + x ],[ 2 * x ** 3 * y , 5 * y ** 3 ]]) G \\displaystyle \\left[\\begin{matrix}x^{2} & x + y\\\\2 x^{3} y & 5 y^{3}\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}x^{2} & x + y\\\\2 x^{3} y & 5 y^{3}\\end{matrix}\\right] Para mais informa\u00e7\u00f5es e recursos, veja a documenta\u00e7\u00e3o online . Vale a pena ver a parte de diagonaliza\u00e7\u00e3o . C\u00e1lculo Vejamos alguns exemplos de como usar sympy em C\u00e1lculo... Derivadas diff ( sin ( x ) * exp ( x ), x ) \\displaystyle e^{x} \\sin{\\left(x \\right)} + e^{x} \\cos{\\left(x \\right)} \\displaystyle e^{x} \\sin{\\left(x \\right)} + e^{x} \\cos{\\left(x \\right)} diff ( exp ( 2 * x ), x , 3 ) # calcula a terceira derivada de exp(2*x) \\displaystyle 8 e^{2 x} \\displaystyle 8 e^{2 x} Podemos aplicar derivadas em objetos mais complexos, como matrizes, por exemplo. diff ( G , x ) # a matriz G foi definida acima \\displaystyle \\left[\\begin{matrix}2 x & 1\\\\6 x^{2} y & 0\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}2 x & 1\\\\6 x^{2} y & 0\\end{matrix}\\right] Para calcular derivadas parciais , como \\frac{\\partial^2}{\\partial x \\partial y} e^{(x^2+y^3)} \\frac{\\partial^2}{\\partial x \\partial y} e^{(x^2+y^3)} diff ( exp ( x ** 2 + y ** 3 ), x , y ) # derivadas parciais com rela\u00e7\u00e3o a x e y \\displaystyle 6 x y^{2} e^{x^{2} + y^{3}} \\displaystyle 6 x y^{2} e^{x^{2} + y^{3}} Integra\u00e7\u00e3o - integrais indefinidas: integrate(expr, vari\u00e1vel) - integrais definidas: integrate(expr, (var_int, lim_inf, lim_sup)) integrate ( cos ( x ), x ) \\displaystyle \\sin{\\left(x \\right)} \\displaystyle \\sin{\\left(x \\right)} # integral de uma gaussiana no intervalo {0,oo} integrate ( exp ( - x ** 2 ),( x , 0 , oo )) # O s\u00edmbolo \"oo\" representa infinito \\displaystyle \\frac{\\sqrt{\\pi}}{2} \\displaystyle \\frac{\\sqrt{\\pi}}{2} H\u00e1 ainda outra forma de definir de a integral, que pode ser conveniente integral = sympy . Integral ( sin ( x ** 2 ), x ) integral \\displaystyle \\int \\sin{\\left(x^{2} \\right)}\\, dx \\displaystyle \\int \\sin{\\left(x^{2} \\right)}\\, dx A vari\u00e1vel integral representa a integra\u00e7\u00e3o indicada acima. Para realizar o c\u00e1lculo, pode-se usar .doit() integral . doit () \\displaystyle \\frac{3 \\sqrt{2} \\sqrt{\\pi} S\\left(\\frac{\\sqrt{2} x}{\\sqrt{\\pi}}\\right) \\Gamma\\left(\\frac{3}{4}\\right)}{8 \\Gamma\\left(\\frac{7}{4}\\right)} \\displaystyle \\frac{3 \\sqrt{2} \\sqrt{\\pi} S\\left(\\frac{\\sqrt{2} x}{\\sqrt{\\pi}}\\right) \\Gamma\\left(\\frac{3}{4}\\right)}{8 \\Gamma\\left(\\frac{7}{4}\\right)} O m\u00e9todo .doit() pode ser usado em outros contextos tamb\u00e9m. S\u00e9ries de pot\u00eancias Podemos facilmente expandir em s\u00e9rie de Taylor , usando o comando series() series ( exp ( x )) \\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right) \\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right) Se definirmos uma express\u00e3o (ou fun\u00e7\u00e3o), podemos fazer a sua expans\u00e3o usando o m\u00e9todo .series() . Neste caso, a sintaxe geral tem a forma: f(x).series(x, x0, n) , onde x0 indica o ponto de expans\u00e3o e n indica a ordem da expans\u00e3o. expr = exp ( - 2 * x ** 2 ) expr . series ( x , 0 , 9 ) \\displaystyle 1 - 2 x^{2} + 2 x^{4} - \\frac{4 x^{6}}{3} + \\frac{2 x^{8}}{3} + O\\left(x^{9}\\right) \\displaystyle 1 - 2 x^{2} + 2 x^{4} - \\frac{4 x^{6}}{3} + \\frac{2 x^{8}}{3} + O\\left(x^{9}\\right) S\u00e9ries de Fourier Tamb\u00e9m \u00e9 poss\u00edvel expandir em S\u00e9rie de Fourier . Para isso, vamos carregar a fun\u00e7\u00e3o from sympy import fourier_series s = fourier_series ( x ** 2 ) s \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} + \\frac{\\pi^{2}}{3} + \\ldots \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} + \\frac{\\pi^{2}}{3} + \\ldots #sympy.init_printing(pretty_print=True,use_unicode=True) s1 = fourier_series ( x ** 2 ) s2 = fourier_series ( x ** 3 ) sympy . pprint ( s1 ) sympy . pprint ( s2 ) 2 \u03c0 -4\u22c5cos(x) + cos(2\u22c5x) + \u2500\u2500 + \u2026 3 \u239b 3\u239e \u239b 3 3\u22c5\u03c0\u239e \u239c 4\u22c5\u03c0 2\u22c5\u03c0 \u239f \u239b 3\u239e \u239c- \u03c0 + \u2500\u2500\u2500\u239f\u22c5sin(2\u22c5x) \u239c- \u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u239f\u22c5sin(3\u22c5x) \u239d-12\u22c5\u03c0 + 2\u22c5\u03c0 \u23a0\u22c5sin(x) \u239d 2 \u23a0 \u239d 9 3 \u23a0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2026 \u03c0 \u03c0 \u03c0 \u00c9 poss\u00edvel definir melhor o ponto de expans\u00e3o, o intervalo e a ordem da expans\u00e3o s = fourier_series ( x ** 2 , ( x , - pi , pi )) s . truncate ( 5 ) \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} - \\frac{4 \\cos{\\left(3 x \\right)}}{9} + \\frac{\\cos{\\left(4 x \\right)}}{4} + \\frac{\\pi^{2}}{3} \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} - \\frac{4 \\cos{\\left(3 x \\right)}}{9} + \\frac{\\cos{\\left(4 x \\right)}}{4} + \\frac{\\pi^{2}}{3} Transformada de Fourier from sympy import fourier_transform , exp from sympy.abc import x , k fourier_transform ( exp ( - 3 * x ** 2 ), x , k ) \\displaystyle \\frac{\\sqrt{3} \\sqrt{\\pi} e^{- \\frac{\\pi^{2} k^{2}}{3}}}{3} \\displaystyle \\frac{\\sqrt{3} \\sqrt{\\pi} e^{- \\frac{\\pi^{2} k^{2}}{3}}}{3} Para mais detalhes e informa\u00e7\u00f5es, consulte a documenta\u00e7\u00e3o online . Exemplos e exerc\u00edcios Exemplo 1 Vamos resolver aqui o Exemplo 2.6 do livro do Griffiths , para ilustrar alguns recursos interessantes. Para mais aprender mais sobre Fun\u00e7\u00f5es Especiais do sympy, veja a documenta\u00e7\u00e3o online . from sympy import Heaviside , DiracDelta , plot , symbols , oo , Eq , sqrt , pi , exp , sin psi , A , a = symbols ( 'psi, A, a' ) h = A * ( Heaviside ( x + a ) - Heaviside ( x - a )) plot ( h . subs ((( a , 1 ),( A , 1 ))) ,( x , - 3 , 3 )); plot ( h . subs ((( a , 0.5 ),( A , 2 ))) ,( x , - 3 , 3 )); psi = h / sqrt ( 2 * a ) psi #psi1 = psi.subs(((A,1),(a,1))) #psi1 \\displaystyle \\frac{\\sqrt{2} A \\left(- \\theta\\left(- a + x\\right) + \\theta\\left(a + x\\right)\\right)}{2 \\sqrt{a}} \\displaystyle \\frac{\\sqrt{2} A \\left(- \\theta\\left(- a + x\\right) + \\theta\\left(a + x\\right)\\right)}{2 \\sqrt{a}} psi1 = psi . subs ((( A , 1 ),( a , 1 ))) plot ( psi1 ,( x , - 2 , 2 )); phi = ( 1 / sqrt ( 2 * pi )) * ( 1 / sqrt ( 2 * a )) * integrate ( exp ( - 1 j * k * x ),( x , - a , a )) phi \\displaystyle \\frac{\\begin{cases} - \\frac{1.0 i e^{1.0 i a k}}{k} + \\frac{1.0 i e^{- 1.0 i a k}}{k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\2 a & \\text{otherwise} \\end{cases}}{2 \\sqrt{\\pi} \\sqrt{a}} \\displaystyle \\frac{\\begin{cases} - \\frac{1.0 i e^{1.0 i a k}}{k} + \\frac{1.0 i e^{- 1.0 i a k}}{k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\2 a & \\text{otherwise} \\end{cases}}{2 \\sqrt{\\pi} \\sqrt{a}} phi . simplify () \\displaystyle \\begin{cases} \\frac{1.0 \\sin{\\left(1.0 a k \\right)}}{\\sqrt{\\pi} \\sqrt{a} k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\\\frac{\\sqrt{a}}{\\sqrt{\\pi}} & \\text{otherwise} \\end{cases} \\displaystyle \\begin{cases} \\frac{1.0 \\sin{\\left(1.0 a k \\right)}}{\\sqrt{\\pi} \\sqrt{a} k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\\\frac{\\sqrt{a}}{\\sqrt{\\pi}} & \\text{otherwise} \\end{cases} phi1 = phi . subs ( a , 1 ) plot ( phi1 ,( k , - 10 , 10 )); Exemplo 2 from sympy import Heaviside , Function , symbols , plot , simplify , exp , sin , pi from sympy import integrate , dsolve , Eq , Derivative as D x , k , a , A , n = symbols ( 'x, k, a, A, n' , real = True ) psi = Function ( 'psi' )( x ) V = 1000 * ( 1 - ( Heaviside ( x ) - Heaviside ( x - 1 ))) plot ( V ,( x , - 2 , 2 )); eq = Eq ( D ( psi , x , x ) + k ** 2 * psi , 0 ) eq \\displaystyle k^{2} \\psi{\\left(x \\right)} + \\frac{d^{2}}{d x^{2}} \\psi{\\left(x \\right)} = 0 \\displaystyle k^{2} \\psi{\\left(x \\right)} + \\frac{d^{2}}{d x^{2}} \\psi{\\left(x \\right)} = 0 dsolve ( eq , psi ) \\displaystyle \\psi{\\left(x \\right)} = C_{1} \\sin{\\left(x \\left|{k}\\right| \\right)} + C_{2} \\cos{\\left(k x \\right)} \\displaystyle \\psi{\\left(x \\right)} = C_{1} \\sin{\\left(x \\left|{k}\\right| \\right)} + C_{2} \\cos{\\left(k x \\right)} Ao analisar a solu\u00e7\u00e3o acima, nos limites em que x=0 x=0 e x=a x=a , onde \\psi \\rightarrow 0 \\psi \\rightarrow 0 , temos que \\psi(0) = C_1 \\sin(0)+ C_2 \\cos(0) = C_2 =0 \\\\ A = C_1 \\psi(0) = C_1 \\sin(0)+ C_2 \\cos(0) = C_2 =0 \\\\ A = C_1 e \\psi(a) = A \\sin(a k) = 0 \\\\ \\sin(ak) = 0 \\rightarrow \\quad k = \\frac{n\\pi}{a}, \\quad n=1,2,3,\\dots \\psi(a) = A \\sin(a k) = 0 \\\\ \\sin(ak) = 0 \\rightarrow \\quad k = \\frac{n\\pi}{a}, \\quad n=1,2,3,\\dots Como pode-se verificar psi = A * sin ( n * pi * x / a ) norma = integrate ( psi * psi , ( x , 0 , a )) . simplify () norma \\displaystyle \\begin{cases} \\frac{A^{2} a \\left(\\pi n - \\frac{\\sin{\\left(2 \\pi n \\right)}}{2}\\right)}{2 \\pi n} & \\text{for}\\: \\frac{\\pi n}{a} \\neq 0 \\\\0 & \\text{otherwise} \\end{cases} \\displaystyle \\begin{cases} \\frac{A^{2} a \\left(\\pi n - \\frac{\\sin{\\left(2 \\pi n \\right)}}{2}\\right)}{2 \\pi n} & \\text{for}\\: \\frac{\\pi n}{a} \\neq 0 \\\\0 & \\text{otherwise} \\end{cases} solve ( Eq ( norma . subs ( n , 1 ) , 1 ), A ) \\displaystyle \\left[ - \\sqrt{2} \\sqrt{\\frac{1}{a}}, \\ \\sqrt{2} \\sqrt{\\frac{1}{a}}\\right] \\displaystyle \\left[ - \\sqrt{2} \\sqrt{\\frac{1}{a}}, \\ \\sqrt{2} \\sqrt{\\frac{1}{a}}\\right] Portanto, a condi\u00e7\u00e3o de normaliza\u00e7\u00e3o resulta em A=\\sqrt{\\frac{2}{a}} A=\\sqrt{\\frac{2}{a}} , e os estados estacion\u00e1rios s\u00e3o \\psi_n (x) = \\sqrt{\\frac{2}{a}} \\text{sen}\\left( \\frac{n \\pi}{a} x \\right) \\psi_n (x) = \\sqrt{\\frac{2}{a}} \\text{sen}\\left( \\frac{n \\pi}{a} x \\right) Para visualizar estes estados, podemos fazer a=1 a=1 e plotar para n=1,2,3 n=1,2,3 psi . subs ((( A , 1 ),( a , 1 ))) \\displaystyle \\sin{\\left(\\pi n x \\right)} \\displaystyle \\sin{\\left(\\pi n x \\right)} import numpy as np import matplotlib.pyplot as plt x = np . linspace ( 0 , 1 , 51 ) for k in range ( 1 , 5 ): plt . plot ( x , np . sin ( k * np . pi * x ) )","title":"Aula 14 - Lab 2"},{"location":"Aula14_Lab2/#lab-2-computacao-simbolica-em-python","text":"Neste notebook veremos como utilizar recursos de computa\u00e7\u00e3o alg\u00e9brica, com express\u00f5es anal\u00edticas, dentro do ambiente Jupyter notebook. Para isso, usaremos a biblioteca SymPy . A inten\u00e7\u00e3o aqui \u00e9 fornecer uma introdu\u00e7\u00e3o b\u00e1sica ao tema, concentrando-se em como usar esses recursos no contexto do nosso curso. Para mais informa\u00e7\u00f5es, voc\u00ea pode consultar a excelente documenta\u00e7\u00e3o do SymPy que est\u00e1 dispon\u00edvel online. Um bom ponto de partida \u00e9 o Tutorial do SymPy . Em particular, n\u00e3o deixe de ler Gotchas! , para evitar problemas comuns de sintaxe!! \ud83d\ude09","title":"Lab 2: Computa\u00e7\u00e3o Simb\u00f3lica em Python"},{"location":"Aula14_Lab2/#computacao-simbolica","text":"A computa\u00e7\u00e3o simb\u00f3lica envolve a computa\u00e7\u00e3o de objetos matem\u00e1ticos simbolicamente. Isso significa que os objetos matem\u00e1ticos s\u00e3o representados exatamente, n\u00e3o aproximadamente, e express\u00f5es matem\u00e1ticas com vari\u00e1veis n\u00e3o avaliadas s\u00e3o deixadas em forma simb\u00f3lica, ou alg\u00e9brica. Para entender a diferen\u00e7a veja os exemplos abaixo import math print ( 'A raiz de 9 \u00e9:' , math . sqrt ( 9 )) print ( 'A raiz de 8 \u00e9:' , math . sqrt ( 8 )) A raiz de 9 \u00e9: 3.0 A raiz de 8 \u00e9: 2.8284271247461903 import sympy print ( 'A raiz de 9 \u00e9:' , sympy . sqrt ( 9 )) print ( 'A raiz de 8 \u00e9:' , sympy . sqrt ( 8 )) A raiz de 9 \u00e9: 3 A raiz de 8 \u00e9: 2*sqrt(2) Na aula passada, algu\u00e9m perguntou porque o seno de \\pi \\pi n\u00e3o era exatamente zero num dos resultados. Expliquei que isso era devido a aproxima\u00e7\u00e3o finita que sempre existe ao representar um n\u00famero no computador. As ferramentas de computa\u00e7\u00e3o alg\u00e9brica (CAS - Computer Algebra System ), como o SymPy, usam recursos especiais para representar um n\u00famero sem aproxima\u00e7\u00f5es na mem\u00f3ria do computador. Isso vem com um custo computacional, mas \u00e9 bastante \u00fatil em v\u00e1rias situa\u00e7\u00f5es, como veremos. Veja abaixo como SymPy avalia o \\text{sen}(\\pi) \\text{sen}(\\pi) sympy . sin ( sympy . pi ) # resultado do seno de pi, com o SymPy \\displaystyle 0 \\displaystyle 0 math . sin ( math . pi ) # resultado do seno de pi, com a biblioteca padr\u00e3o do Python 1.2246467991473532e-16 Al\u00e9m de eliminar os erros de arredondamento , a computa\u00e7\u00e3o alg\u00e9brica permite manipular algebricamente express\u00f5es matem\u00e1tica e at\u00e9 mesmo aplicar ferramentas de C\u00e1lculo, como derivadas e integrais. Veremos exemplos mais adiante.","title":"Computa\u00e7\u00e3o Simb\u00f3lica"},{"location":"Aula14_Lab2/#como-usar-variaveis-simbolicas","text":"De maneira bem resumida, al\u00e9m de carregar a biblioteca SymPy e aprender sua sintaxe, o primeiro passo principal \u00e9 definir uma vari\u00e1vel como simb\u00f3lica , como \u00e9 mostrado nos exemplos abaixo: import sympy x , y , z , t = sympy . symbols ( 'x y z t' ) Agora os identificadores (vari\u00e1veis) \"x, y, z, t\" s\u00e3o vari\u00e1veis simb\u00f3licas e ser\u00e3o tratadas como objetos sympy, e n\u00e3o mais como identificadores usuais do Python. Podemos, ent\u00e3o definir fun\u00e7\u00f5es com essas vari\u00e1veis e us\u00e1-las em express\u00f5es alg\u00e9bricas. O exemplo abaixo, calcula a derivada de e^{-2x^2} e^{-2x^2} : sympy . diff ( sympy . exp ( - 2 * x ** 2 )) \\displaystyle - 4 x e^{- 2 x^{2}} \\displaystyle - 4 x e^{- 2 x^{2}} Para simplificar a sintaxe, e evitar ficar digitando sympy. o tempo todo, podemos carregar as fun\u00e7\u00f5es diretamente (mas cuidado com isso!!), ou usar uma atalho (alias), com j\u00e1 mostramos na \u00faltima aula. Aqui, para simplificar, eu vou carregar todas as fun\u00e7\u00f5es que usaremos neste tutorial, na linha abaixo. from sympy import sin , cos , exp , diff , integrate , series , solve , pi , Matrix , simplify , oo diff ( exp ( - 2 * x ** 2 )) \\displaystyle - 4 x e^{- 2 x^{2}} \\displaystyle - 4 x e^{- 2 x^{2}} Podemos usar solve para resolver equa\u00e7\u00f5es, como, por exemplo x^2 - 8 = 0 x^2 - 8 = 0 solve ( x ** 2 - 8 , x ) [-2*sqrt(2), 2*sqrt(2)] para tornar a apresenta\u00e7\u00e3o das resposta mais agrad\u00e1vel, podemos usar o comando sympy . init_printing ( use_latex = True ) solve ( x ** 2 - 8 , x ) \\displaystyle \\left[ - 2 \\sqrt{2}, \\ 2 \\sqrt{2}\\right] \\displaystyle \\left[ - 2 \\sqrt{2}, \\ 2 \\sqrt{2}\\right] Ou simplificar express\u00f5es alg\u00e9bricas sympy . pprint (( x ** 3 + x ** 2 - x - 1 ) / ( x ** 2 + 2 * x + 1 )) simplify (( x ** 3 + x ** 2 - x - 1 ) / ( x ** 2 + 2 * x + 1 )) 3 2 x + x - x - 1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 2 x + 2\u22c5x + 1 \\displaystyle x - 1 \\displaystyle x - 1","title":"\ud83e\udd14 Como usar?  - vari\u00e1veis simb\u00f3licas"},{"location":"Aula14_Lab2/#funcoes-e-graficos","text":"A forma mais simples de definir uma fun\u00e7\u00e3o no SymPy \u00e9 simplesmente definir uma nova vari\u00e1vel atribuindo uma express\u00e3o com vari\u00e1veis simb\u00f3licas expr = x ** 3 - 5 * x ** 2 + 3 * x + 2 expr \\displaystyle x^{3} - 5 x^{2} + 3 x + 2 \\displaystyle x^{3} - 5 x^{2} + 3 x + 2 sympy . plot ( expr ,( x , - 1.5 , 5 )); # \u00e9 f\u00e1cil plotar o gr\u00e1fico de uma fun\u00e7\u00e3o Para mais informa\u00e7\u00f5es veja a documenta\u00e7\u00e3o online .","title":"Fun\u00e7\u00f5es e gr\u00e1ficos"},{"location":"Aula14_Lab2/#matrizes","text":"\u00c9 poss\u00edvel definir matrizes com o comando Matrix() M = Matrix ([[ 1 , 2 ],[ 1 , 0 ]]) # define M com a nossa velha conhecida matriz... M \\displaystyle \\left[\\begin{matrix}1 & 2\\\\1 & 0\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}1 & 2\\\\1 & 0\\end{matrix}\\right] M . det () # calcula o determinante da matriz M \\displaystyle -2 \\displaystyle -2 M . inv () # matriz inversa \\displaystyle \\left[\\begin{matrix}0 & 1\\\\\\frac{1}{2} & - \\frac{1}{2}\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}0 & 1\\\\\\frac{1}{2} & - \\frac{1}{2}\\end{matrix}\\right] M . eigenvals () # autovalores \\displaystyle \\left\\{ -1 : 1, \\ 2 : 1\\right\\} \\displaystyle \\left\\{ -1 : 1, \\ 2 : 1\\right\\} M . eigenvects () # autovetores \\displaystyle \\left[ \\left( -1, \\ 1, \\ \\left[ \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 2, \\ 1, \\ \\left[ \\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\right]\\right)\\right] \\displaystyle \\left[ \\left( -1, \\ 1, \\ \\left[ \\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right]\\right]\\right), \\ \\left( 2, \\ 1, \\ \\left[ \\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\right]\\right)\\right] sympy . eye ( 4 ) # matriz identidade de ordem 4 \\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right] Podemos tamb\u00e9m definir uma matriz de vari\u00e1veis e fun\u00e7\u00f5es simb\u00f3licas! \ud83d\ude09 G = Matrix ([[ x ** 2 , y + x ],[ 2 * x ** 3 * y , 5 * y ** 3 ]]) G \\displaystyle \\left[\\begin{matrix}x^{2} & x + y\\\\2 x^{3} y & 5 y^{3}\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}x^{2} & x + y\\\\2 x^{3} y & 5 y^{3}\\end{matrix}\\right] Para mais informa\u00e7\u00f5es e recursos, veja a documenta\u00e7\u00e3o online . Vale a pena ver a parte de diagonaliza\u00e7\u00e3o .","title":"Matrizes"},{"location":"Aula14_Lab2/#calculo","text":"Vejamos alguns exemplos de como usar sympy em C\u00e1lculo... Derivadas diff ( sin ( x ) * exp ( x ), x ) \\displaystyle e^{x} \\sin{\\left(x \\right)} + e^{x} \\cos{\\left(x \\right)} \\displaystyle e^{x} \\sin{\\left(x \\right)} + e^{x} \\cos{\\left(x \\right)} diff ( exp ( 2 * x ), x , 3 ) # calcula a terceira derivada de exp(2*x) \\displaystyle 8 e^{2 x} \\displaystyle 8 e^{2 x} Podemos aplicar derivadas em objetos mais complexos, como matrizes, por exemplo. diff ( G , x ) # a matriz G foi definida acima \\displaystyle \\left[\\begin{matrix}2 x & 1\\\\6 x^{2} y & 0\\end{matrix}\\right] \\displaystyle \\left[\\begin{matrix}2 x & 1\\\\6 x^{2} y & 0\\end{matrix}\\right] Para calcular derivadas parciais , como \\frac{\\partial^2}{\\partial x \\partial y} e^{(x^2+y^3)} \\frac{\\partial^2}{\\partial x \\partial y} e^{(x^2+y^3)} diff ( exp ( x ** 2 + y ** 3 ), x , y ) # derivadas parciais com rela\u00e7\u00e3o a x e y \\displaystyle 6 x y^{2} e^{x^{2} + y^{3}} \\displaystyle 6 x y^{2} e^{x^{2} + y^{3}} Integra\u00e7\u00e3o - integrais indefinidas: integrate(expr, vari\u00e1vel) - integrais definidas: integrate(expr, (var_int, lim_inf, lim_sup)) integrate ( cos ( x ), x ) \\displaystyle \\sin{\\left(x \\right)} \\displaystyle \\sin{\\left(x \\right)} # integral de uma gaussiana no intervalo {0,oo} integrate ( exp ( - x ** 2 ),( x , 0 , oo )) # O s\u00edmbolo \"oo\" representa infinito \\displaystyle \\frac{\\sqrt{\\pi}}{2} \\displaystyle \\frac{\\sqrt{\\pi}}{2} H\u00e1 ainda outra forma de definir de a integral, que pode ser conveniente integral = sympy . Integral ( sin ( x ** 2 ), x ) integral \\displaystyle \\int \\sin{\\left(x^{2} \\right)}\\, dx \\displaystyle \\int \\sin{\\left(x^{2} \\right)}\\, dx A vari\u00e1vel integral representa a integra\u00e7\u00e3o indicada acima. Para realizar o c\u00e1lculo, pode-se usar .doit() integral . doit () \\displaystyle \\frac{3 \\sqrt{2} \\sqrt{\\pi} S\\left(\\frac{\\sqrt{2} x}{\\sqrt{\\pi}}\\right) \\Gamma\\left(\\frac{3}{4}\\right)}{8 \\Gamma\\left(\\frac{7}{4}\\right)} \\displaystyle \\frac{3 \\sqrt{2} \\sqrt{\\pi} S\\left(\\frac{\\sqrt{2} x}{\\sqrt{\\pi}}\\right) \\Gamma\\left(\\frac{3}{4}\\right)}{8 \\Gamma\\left(\\frac{7}{4}\\right)} O m\u00e9todo .doit() pode ser usado em outros contextos tamb\u00e9m.","title":"C\u00e1lculo"},{"location":"Aula14_Lab2/#series-de-potencias","text":"Podemos facilmente expandir em s\u00e9rie de Taylor , usando o comando series() series ( exp ( x )) \\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right) \\displaystyle 1 + x + \\frac{x^{2}}{2} + \\frac{x^{3}}{6} + \\frac{x^{4}}{24} + \\frac{x^{5}}{120} + O\\left(x^{6}\\right) Se definirmos uma express\u00e3o (ou fun\u00e7\u00e3o), podemos fazer a sua expans\u00e3o usando o m\u00e9todo .series() . Neste caso, a sintaxe geral tem a forma: f(x).series(x, x0, n) , onde x0 indica o ponto de expans\u00e3o e n indica a ordem da expans\u00e3o. expr = exp ( - 2 * x ** 2 ) expr . series ( x , 0 , 9 ) \\displaystyle 1 - 2 x^{2} + 2 x^{4} - \\frac{4 x^{6}}{3} + \\frac{2 x^{8}}{3} + O\\left(x^{9}\\right) \\displaystyle 1 - 2 x^{2} + 2 x^{4} - \\frac{4 x^{6}}{3} + \\frac{2 x^{8}}{3} + O\\left(x^{9}\\right)","title":"S\u00e9ries de pot\u00eancias"},{"location":"Aula14_Lab2/#series-de-fourier","text":"Tamb\u00e9m \u00e9 poss\u00edvel expandir em S\u00e9rie de Fourier . Para isso, vamos carregar a fun\u00e7\u00e3o from sympy import fourier_series s = fourier_series ( x ** 2 ) s \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} + \\frac{\\pi^{2}}{3} + \\ldots \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} + \\frac{\\pi^{2}}{3} + \\ldots #sympy.init_printing(pretty_print=True,use_unicode=True) s1 = fourier_series ( x ** 2 ) s2 = fourier_series ( x ** 3 ) sympy . pprint ( s1 ) sympy . pprint ( s2 ) 2 \u03c0 -4\u22c5cos(x) + cos(2\u22c5x) + \u2500\u2500 + \u2026 3 \u239b 3\u239e \u239b 3 3\u22c5\u03c0\u239e \u239c 4\u22c5\u03c0 2\u22c5\u03c0 \u239f \u239b 3\u239e \u239c- \u03c0 + \u2500\u2500\u2500\u239f\u22c5sin(2\u22c5x) \u239c- \u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u239f\u22c5sin(3\u22c5x) \u239d-12\u22c5\u03c0 + 2\u22c5\u03c0 \u23a0\u22c5sin(x) \u239d 2 \u23a0 \u239d 9 3 \u23a0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 + \u2026 \u03c0 \u03c0 \u03c0 \u00c9 poss\u00edvel definir melhor o ponto de expans\u00e3o, o intervalo e a ordem da expans\u00e3o s = fourier_series ( x ** 2 , ( x , - pi , pi )) s . truncate ( 5 ) \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} - \\frac{4 \\cos{\\left(3 x \\right)}}{9} + \\frac{\\cos{\\left(4 x \\right)}}{4} + \\frac{\\pi^{2}}{3} \\displaystyle - 4 \\cos{\\left(x \\right)} + \\cos{\\left(2 x \\right)} - \\frac{4 \\cos{\\left(3 x \\right)}}{9} + \\frac{\\cos{\\left(4 x \\right)}}{4} + \\frac{\\pi^{2}}{3}","title":"S\u00e9ries de Fourier"},{"location":"Aula14_Lab2/#transformada-de-fourier","text":"from sympy import fourier_transform , exp from sympy.abc import x , k fourier_transform ( exp ( - 3 * x ** 2 ), x , k ) \\displaystyle \\frac{\\sqrt{3} \\sqrt{\\pi} e^{- \\frac{\\pi^{2} k^{2}}{3}}}{3} \\displaystyle \\frac{\\sqrt{3} \\sqrt{\\pi} e^{- \\frac{\\pi^{2} k^{2}}{3}}}{3} Para mais detalhes e informa\u00e7\u00f5es, consulte a documenta\u00e7\u00e3o online .","title":"Transformada de Fourier"},{"location":"Aula14_Lab2/#exemplos-e-exercicios","text":"","title":"Exemplos e exerc\u00edcios"},{"location":"Aula14_Lab2/#exemplo-1","text":"Vamos resolver aqui o Exemplo 2.6 do livro do Griffiths , para ilustrar alguns recursos interessantes. Para mais aprender mais sobre Fun\u00e7\u00f5es Especiais do sympy, veja a documenta\u00e7\u00e3o online . from sympy import Heaviside , DiracDelta , plot , symbols , oo , Eq , sqrt , pi , exp , sin psi , A , a = symbols ( 'psi, A, a' ) h = A * ( Heaviside ( x + a ) - Heaviside ( x - a )) plot ( h . subs ((( a , 1 ),( A , 1 ))) ,( x , - 3 , 3 )); plot ( h . subs ((( a , 0.5 ),( A , 2 ))) ,( x , - 3 , 3 )); psi = h / sqrt ( 2 * a ) psi #psi1 = psi.subs(((A,1),(a,1))) #psi1 \\displaystyle \\frac{\\sqrt{2} A \\left(- \\theta\\left(- a + x\\right) + \\theta\\left(a + x\\right)\\right)}{2 \\sqrt{a}} \\displaystyle \\frac{\\sqrt{2} A \\left(- \\theta\\left(- a + x\\right) + \\theta\\left(a + x\\right)\\right)}{2 \\sqrt{a}} psi1 = psi . subs ((( A , 1 ),( a , 1 ))) plot ( psi1 ,( x , - 2 , 2 )); phi = ( 1 / sqrt ( 2 * pi )) * ( 1 / sqrt ( 2 * a )) * integrate ( exp ( - 1 j * k * x ),( x , - a , a )) phi \\displaystyle \\frac{\\begin{cases} - \\frac{1.0 i e^{1.0 i a k}}{k} + \\frac{1.0 i e^{- 1.0 i a k}}{k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\2 a & \\text{otherwise} \\end{cases}}{2 \\sqrt{\\pi} \\sqrt{a}} \\displaystyle \\frac{\\begin{cases} - \\frac{1.0 i e^{1.0 i a k}}{k} + \\frac{1.0 i e^{- 1.0 i a k}}{k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\2 a & \\text{otherwise} \\end{cases}}{2 \\sqrt{\\pi} \\sqrt{a}} phi . simplify () \\displaystyle \\begin{cases} \\frac{1.0 \\sin{\\left(1.0 a k \\right)}}{\\sqrt{\\pi} \\sqrt{a} k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\\\frac{\\sqrt{a}}{\\sqrt{\\pi}} & \\text{otherwise} \\end{cases} \\displaystyle \\begin{cases} \\frac{1.0 \\sin{\\left(1.0 a k \\right)}}{\\sqrt{\\pi} \\sqrt{a} k} & \\text{for}\\: k > -\\infty \\wedge k < \\infty \\wedge k \\neq 0 \\\\\\frac{\\sqrt{a}}{\\sqrt{\\pi}} & \\text{otherwise} \\end{cases} phi1 = phi . subs ( a , 1 ) plot ( phi1 ,( k , - 10 , 10 ));","title":"Exemplo 1"},{"location":"Aula14_Lab2/#exemplo-2","text":"from sympy import Heaviside , Function , symbols , plot , simplify , exp , sin , pi from sympy import integrate , dsolve , Eq , Derivative as D x , k , a , A , n = symbols ( 'x, k, a, A, n' , real = True ) psi = Function ( 'psi' )( x ) V = 1000 * ( 1 - ( Heaviside ( x ) - Heaviside ( x - 1 ))) plot ( V ,( x , - 2 , 2 )); eq = Eq ( D ( psi , x , x ) + k ** 2 * psi , 0 ) eq \\displaystyle k^{2} \\psi{\\left(x \\right)} + \\frac{d^{2}}{d x^{2}} \\psi{\\left(x \\right)} = 0 \\displaystyle k^{2} \\psi{\\left(x \\right)} + \\frac{d^{2}}{d x^{2}} \\psi{\\left(x \\right)} = 0 dsolve ( eq , psi ) \\displaystyle \\psi{\\left(x \\right)} = C_{1} \\sin{\\left(x \\left|{k}\\right| \\right)} + C_{2} \\cos{\\left(k x \\right)} \\displaystyle \\psi{\\left(x \\right)} = C_{1} \\sin{\\left(x \\left|{k}\\right| \\right)} + C_{2} \\cos{\\left(k x \\right)} Ao analisar a solu\u00e7\u00e3o acima, nos limites em que x=0 x=0 e x=a x=a , onde \\psi \\rightarrow 0 \\psi \\rightarrow 0 , temos que \\psi(0) = C_1 \\sin(0)+ C_2 \\cos(0) = C_2 =0 \\\\ A = C_1 \\psi(0) = C_1 \\sin(0)+ C_2 \\cos(0) = C_2 =0 \\\\ A = C_1 e \\psi(a) = A \\sin(a k) = 0 \\\\ \\sin(ak) = 0 \\rightarrow \\quad k = \\frac{n\\pi}{a}, \\quad n=1,2,3,\\dots \\psi(a) = A \\sin(a k) = 0 \\\\ \\sin(ak) = 0 \\rightarrow \\quad k = \\frac{n\\pi}{a}, \\quad n=1,2,3,\\dots Como pode-se verificar psi = A * sin ( n * pi * x / a ) norma = integrate ( psi * psi , ( x , 0 , a )) . simplify () norma \\displaystyle \\begin{cases} \\frac{A^{2} a \\left(\\pi n - \\frac{\\sin{\\left(2 \\pi n \\right)}}{2}\\right)}{2 \\pi n} & \\text{for}\\: \\frac{\\pi n}{a} \\neq 0 \\\\0 & \\text{otherwise} \\end{cases} \\displaystyle \\begin{cases} \\frac{A^{2} a \\left(\\pi n - \\frac{\\sin{\\left(2 \\pi n \\right)}}{2}\\right)}{2 \\pi n} & \\text{for}\\: \\frac{\\pi n}{a} \\neq 0 \\\\0 & \\text{otherwise} \\end{cases} solve ( Eq ( norma . subs ( n , 1 ) , 1 ), A ) \\displaystyle \\left[ - \\sqrt{2} \\sqrt{\\frac{1}{a}}, \\ \\sqrt{2} \\sqrt{\\frac{1}{a}}\\right] \\displaystyle \\left[ - \\sqrt{2} \\sqrt{\\frac{1}{a}}, \\ \\sqrt{2} \\sqrt{\\frac{1}{a}}\\right] Portanto, a condi\u00e7\u00e3o de normaliza\u00e7\u00e3o resulta em A=\\sqrt{\\frac{2}{a}} A=\\sqrt{\\frac{2}{a}} , e os estados estacion\u00e1rios s\u00e3o \\psi_n (x) = \\sqrt{\\frac{2}{a}} \\text{sen}\\left( \\frac{n \\pi}{a} x \\right) \\psi_n (x) = \\sqrt{\\frac{2}{a}} \\text{sen}\\left( \\frac{n \\pi}{a} x \\right) Para visualizar estes estados, podemos fazer a=1 a=1 e plotar para n=1,2,3 n=1,2,3 psi . subs ((( A , 1 ),( a , 1 ))) \\displaystyle \\sin{\\left(\\pi n x \\right)} \\displaystyle \\sin{\\left(\\pi n x \\right)} import numpy as np import matplotlib.pyplot as plt x = np . linspace ( 0 , 1 , 51 ) for k in range ( 1 , 5 ): plt . plot ( x , np . sin ( k * np . pi * x ) )","title":"Exemplo 2"},{"location":"Aula8/","text":"5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a ``F\u00edsica'' do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica. 5.1 Espa\u00e7o de estados Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia. 5.2 Espa\u00e7o vetorial linear Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real). Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o. 5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem. Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que como o produto interno resulta num escalar (n\u00famero) complexo, ele n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = (\\phi,\\psi) = n\u00famero complexo (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0 Usando o produto interno, podemos definir tamb\u00e9m a norma (ou comprimento) do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo. ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||v|| = \\sum_{i=1}^{d} v^*_i v_i ||v|| = \\sum_{i=1}^{d} v^*_i v_i Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 Normas dos vetores ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||v|| = \\sum_{i=1}^{d} v^*_i v_i ||v|| = \\sum_{i=1}^{d} v^*_i v_i No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} , temos que \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n e onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} .","title":"5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica"},{"location":"Aula8/#5-estrutura-matematica-da-mecanica-quantica","text":"At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a ``F\u00edsica'' do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica.","title":"5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica"},{"location":"Aula8/#51-espaco-de-estados","text":"Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia.","title":"5.1  Espa\u00e7o de estados"},{"location":"Aula8/#52-espaco-vetorial-linear","text":"Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real). Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o.","title":"5.2 Espa\u00e7o vetorial linear"},{"location":"Aula8/#53-espacos-de-hilbert-espacos-vetoriais-da-mq","text":"Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem. Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que como o produto interno resulta num escalar (n\u00famero) complexo, ele n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = (\\phi,\\psi) = n\u00famero complexo (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0 Usando o produto interno, podemos definir tamb\u00e9m a norma (ou comprimento) do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo. ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||v|| = \\sum_{i=1}^{d} v^*_i v_i ||v|| = \\sum_{i=1}^{d} v^*_i v_i Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 Normas dos vetores ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||\\phi|| = \\int \\phi^*(x)\\phi(x)\\,dx ||v|| = \\sum_{i=1}^{d} v^*_i v_i ||v|| = \\sum_{i=1}^{d} v^*_i v_i No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} , temos que \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n e onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} .","title":"5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ"},{"location":"Aula9/","text":"\\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} 5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem. Produto interno Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = (\\phi,\\psi) = n\u00famero complexo (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0 Comprimentos e \u00e2ngulos O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Expans\u00e3o de vetores No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo. 5.4 Nota\u00e7\u00e3o de Dirac Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{c} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{c} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{c} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{c} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. Propriedades do produto interno Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{c} 1.\\, &&\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &&\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &&(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &&\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{c} 1.\\, &&\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &&\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &&(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &&\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. 5.5 Vetores de base O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de elmentos (vetores) da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 Procedimento de Gram-Schmidt Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar este procedimento para construir uma base ortonormal Para simplificar o entendimento do processo, iremos considerar um exemplo com 3 vetores de base (espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes, subtraindo as componentes de |w_1\\rangle |w_1\\rangle em |u_1\\rangle |u_1\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |k_i\\rangle \\} \\{ |k_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |k_1\\rangle = \\frac{ |w_1 \\rangle }{\\langle w_1 | w_1 \\rangle}; \\, |k_2\\rangle = \\frac{ |w_2 \\rangle }{\\langle w_2 | w_2 \\rangle}; \\, |k_3\\rangle = \\frac{ |w_3 \\rangle }{\\langle w_3 | w_3 \\rangle} |k_1\\rangle = \\frac{ |w_1 \\rangle }{\\langle w_1 | w_1 \\rangle}; \\, |k_2\\rangle = \\frac{ |w_2 \\rangle }{\\langle w_2 | w_2 \\rangle}; \\, |k_3\\rangle = \\frac{ |w_3 \\rangle }{\\langle w_3 | w_3 \\rangle} Algebra de Dirac Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Example Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: $$ | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle $$ $$ | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle $$ a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| .","title":"Aula9"},{"location":"Aula9/#53-espacos-de-hilbert-espacos-vetoriais-da-mq","text":"Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem.","title":"5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ"},{"location":"Aula9/#produto-interno","text":"Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = (\\phi,\\psi) = n\u00famero complexo (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0","title":"Produto interno"},{"location":"Aula9/#comprimentos-e-angulos","text":"O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} .","title":"Comprimentos e \u00e2ngulos"},{"location":"Aula9/#expansao-de-vetores","text":"No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo.","title":"Expans\u00e3o de vetores"},{"location":"Aula9/#54-notacao-de-dirac","text":"Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{c} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{c} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{c} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{c} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k.","title":"5.4 Nota\u00e7\u00e3o de Dirac"},{"location":"Aula9/#propriedades-do-produto-interno","text":"Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{c} 1.\\, &&\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &&\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &&(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &&\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{c} 1.\\, &&\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &&\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &&(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &&\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right.","title":"Propriedades do produto interno"},{"location":"Aula9/#55-vetores-de-base","text":"O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de elmentos (vetores) da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1","title":"5.5 Vetores de base"},{"location":"Aula9/#procedimento-de-gram-schmidt","text":"Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar este procedimento para construir uma base ortonormal Para simplificar o entendimento do processo, iremos considerar um exemplo com 3 vetores de base (espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes, subtraindo as componentes de |w_1\\rangle |w_1\\rangle em |u_1\\rangle |u_1\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |k_i\\rangle \\} \\{ |k_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |k_1\\rangle = \\frac{ |w_1 \\rangle }{\\langle w_1 | w_1 \\rangle}; \\, |k_2\\rangle = \\frac{ |w_2 \\rangle }{\\langle w_2 | w_2 \\rangle}; \\, |k_3\\rangle = \\frac{ |w_3 \\rangle }{\\langle w_3 | w_3 \\rangle} |k_1\\rangle = \\frac{ |w_1 \\rangle }{\\langle w_1 | w_1 \\rangle}; \\, |k_2\\rangle = \\frac{ |w_2 \\rangle }{\\langle w_2 | w_2 \\rangle}; \\, |k_3\\rangle = \\frac{ |w_3 \\rangle }{\\langle w_3 | w_3 \\rangle}","title":"Procedimento de Gram-Schmidt"},{"location":"Aula9/#algebra-de-dirac","text":"Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Example Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: $$ | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle $$ $$ | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle $$ a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| .","title":"Algebra de Dirac"},{"location":"Aulas-S5/","text":"\\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} 5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a \"F\u00edsica\" do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica. 5.1 Espa\u00e7o de estados Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia. 5.2 Espa\u00e7o vetorial linear Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real). Dimens\u00e3o do espa\u00e7o Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o. Como veremos adiante, os espa\u00e7cos de Hilbert da MQ podem ser infinitos. 5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem. Produto interno Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (n\u00famero complexo) (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0 Comprimentos e \u00e2ngulos O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Expans\u00e3o de vetores No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo. 5.4 Nota\u00e7\u00e3o de Dirac Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. Propriedades do produto interno Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. 5.5 Vetores de base O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de vetores da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 Procedimento de Gram-Schmidt Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar o procedimento de Gram-Schmidt para construir uma base ortonormal a partir desse conjunto inicial. Para simplificar o entendimento do processo, consideramos um exemplo com 3 vetores de base (num espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes da base subtraindo deles as componentes nas dire\u00e7\u00f5es ortonais \u00e0quelas j\u00e1 constru\u00eddas. Neste caso, por exemplo, as dire\u00e7\u00f5es |w_2\\rangle |w_2\\rangle e |w_3\\rangle |w_3\\rangle s\u00e3o constru\u00eddas subtraindo as componente na dire\u00e7\u00e3o de |w_1\\rangle |w_1\\rangle e |w_2\\rangle |w_2\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |v_i\\rangle \\} \\{ |v_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} De forma geral, para um cojunto finito de vetores \\{u_k\\} \\{u_k\\} , de um espa\u00e7o vetorial \\mathcal{U} \\mathcal{U} de dimens\u00e3o d d , pode-se escrever os vetores ortonormais \\{v_k\\} \\{v_k\\} atrav\u00e9s da construindo: \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. Algebra de Dirac Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor |\\psi\\rangle |\\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). 5.6 Operadores lineares Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)! Exemplos importantes Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} Representa\u00e7\u00e3o de operadores A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) Valores esperados O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado. Autovalores e autovetores Quando um operador age sobre um dado vetor (estado) e o resultado \u00e9 o mesmo vetor (estado) multiplicado por um escalar, o vetor \u00e9 chamado de autovetor (autoestado) e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico e s\u00e3o muito importantes, pois esses autovetores formam uma base do espa\u00e7o e permitem represetar qualquer estado do sistema. A seguir temos uma breve revis\u00e3o de como calcular autovalores e autovetores, a partir de conceitos e m\u00e9todos de Algebra Linear. C\u00e1lculo dos autovalores Dado um operador linear \\hat{T} \\hat{T} , como j\u00e1 vimos, pode-se sempre represent\u00e1-lo por uma matriz T T . O conjunto de autovalores \\lambda \\lambda dessa matriz podem ser determinados atrav\u00e9s da equa\u00e7\u00e3o caracter\u00edstica (tamb\u00e9m chamada de equa\u00e7\u00e3o secular ), para o determinante abaixo: $$ \\operatorname{det}(T-\\lambda I)=0$$ onde I=\\mathbb{1} I=\\mathbb{1} \u00e9 a matriz identidade. A solu\u00e7\u00e3o da equa\u00e7\u00e3o caracter\u00edstica fornece os autovalores \\lambda \\lambda , que s\u00e3o as raizes do polin\u00f4nimo ( caracter\u00edstico ), indicado acima. Exerc\u00edcio sugerido Escreva o equa\u00e7\u00e3o caracter\u00edstica e ache os autovalores da matriz A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} C\u00e1lculo dos autovetores A partir dos autovalores pode-se determinar os autovetores da matriz T T , que pode ser ent\u00e3o escrita na forma diagonal. Para ilustrar melhor isso, usaremos um exemplo, a partir do problema proposto a seguir. Exerc\u00edcio sugerido Considere o operador \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} , expresso numa base ortonormal. Ache a matriz T T , que representa o operador nesta base, e determine os autovetores normalizados do operador, com seus autovalores. Considere que o espa\u00e7o \u00e9 bidimensional. Antes de seguir, voc\u00ea deve resolver o problema proposto acima, em detalhe, pelo menos at\u00e9 onde puder, para ter certeza de que est\u00e1 entendo todos os passos necess\u00e1rios \u00e0 resolu\u00e7\u00e3o do problema. Ao fazer isso ir\u00e1 encontrar os valores que usaremos na resolu\u00e7\u00e3o que exemplificada o c\u00e1lculo de um dos autovetores, a seguir Exemplo: resolu\u00e7\u00e3o dos autovetores Os autovalores do problema anterio s\u00e3o \\lambda_1=2 \\lambda_1=2 e \\lambda_2=-1 \\lambda_2=-1 . Substitui-se, ent\u00e3o, esses valores, um de cada vez, na equa\u00e7\u00e3o de autovalores \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} para determinar os autovetores \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} , como \u00e9 mostrado abaixo para \\ket{u_2} \\ket{u_2} . \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. portanto, \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. Normalizando o vetor temos: \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, portanto, finalmente, temos: \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. Verifique agora que \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. Conjuga\u00e7\u00e3o Hermitiana At\u00e9 agora vimos que um operador age num ket para produzir um novo ket , de acordo com \\hat{T} \\ket{u} = \\ket{v}. \\hat{T} \\ket{u} = \\ket{v}. Vejamos agora, mais atentamente, sua a\u00e7\u00e3o dentro de um produto interno \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} . Sabemos que isso resulta num escalar (n\u00famero) complexo. Podemos tomar complexo conjugado desse n\u00famero, usando a rela\u00e7\u00e3o \\bra{w}v\\rangle = \\bra{v}w\\rangle^* \\bra{w}v\\rangle = \\bra{v}w\\rangle^* . Observe atentamente o que ocorre com o operador \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} onde \\hat{T^{\\dagger}} \\hat{T^{\\dagger}} (pronuncia-se \"T dagger \") \u00e9 chamado de Hermitiado conjugaddo ou adjunto do operador \\hat{T} \\hat{T} . Como formar o Adjunto de uma express\u00e3o geral? Substitua qualquer constante por seu complexo conjugado. Substitua kets pelos bras associados, e vice-versa. Substitua cada operador por seu Adjunto. Inverta a ordem de todos os fatores na express\u00e3o. O Hermitiado Conjugado de uma matriz J\u00e1 sabemos como encontrar a matriz M M de um operador \\hat{M} \\hat{M} qualquer. Para encontrar a matriz do Adjunto desse, simbolizada por M^{dagger} M^{dagger} , basta seguir os seguintes passos: Matriz Adjunta Calcule a matriz transposta M^T M^T , trocando as linhas pelas colunas. Tome o complexo conjugado de cada elemento de M^T. M^T. De forma resumida: M^{\\dagger}= \\left( M^T \\right)^*. M^{\\dagger}= \\left( M^T \\right)^*. Propriedade da opera\u00e7\u00e3o de transposi\u00e7\u00e3o (A+B)^T = A^T + B^T. (A+B)^T = A^T + B^T. (A^T)^T = A. (A^T)^T = A. (aA)^T= a A^T. (aA)^T= a A^T. (AB)^T = B^T A^T. (AB)^T = B^T A^T. Operadores Hermitianos Um operador \u00e9 dito Hermitiano quando \\hat{T}^{\\dagger}=\\hat{T} \\hat{T}^{\\dagger}=\\hat{T} . Para um operador Hemitiano, temos que \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* Veremos que os operadores de observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica devem ser sempre operadores Hermitianos. Como esse operadores podem ser representado por matrizes, \u00e9 interessante ver com determinar se uma matriz \u00e9 Hemitiana. Matriz Hermitiana Uma matriz M M \u00e9 Hermitiana se satisfaz: M = M^{\\dagger}. M = M^{\\dagger}. Como vimos, M^{\\dagger} M^{\\dagger} corresponde ao complexo conjugado da matriz transposta. Portanto, para satisfazer essa condi\u00e7\u00e3o, os elementos da diagonal principal da matriz devem ser todos n\u00fameros reais (n\u00e3o complexos). Como consequ\u00eancia, o tra\u00e7o do operador (matriz) ser\u00e1, necessariamente um n\u00famero real. Autovalores de um operador Hermitiano Pode-se demonstrar que operadores Hermitianos tem autovalores reais (verifique!). Por conta dessa propriedade, requer-se que todos os observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica, sejam representados por operadores Hermitianos. Portanto, tanto o tra\u00e7o como os autovalores de um operador Hermitiano s\u00e3o n\u00fameros reais. Operador anti-Hermitiano Um operador \\hat{A} \\hat{A} \u00e9 dito anti-Hermitiano se: A^{\\dagger}=-A A^{\\dagger}=-A Verifique que, neste caso, os elementos da diagonal principal do operador (matriz) anti-Hermitiano(a) s\u00e3o todos n\u00fameros imagin\u00e1rios puros. Operadores Unit\u00e1rios Um operador \\hat{U} \\hat{U} (de matriz U U ) \u00e9 unit\u00e1rio se: UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} Isso significa que U^{\\dagger} = U^{-1} U^{\\dagger} = U^{-1} ou seja, que a matriz adjunta \u00e9 igual a matriz inversa. Outra importante caracter\u00edstica das matrizes unit\u00e1rias \u00e9 que as linhas e colunas dessa matrizes formam um conjunto de vetores ortonormais. Comutadores Seja \\hat{A} \\hat{A} e \\hat{B} \\hat{B} dois operadores lineares do espa\u00e7o. Em geral, temos que \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. Assim, define-se o comutador [\\hat{A},\\hat{B}] [\\hat{A},\\hat{B}] como sendo [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. Se [\\hat{A},\\hat{B}]=0 [\\hat{A},\\hat{B}]=0 , dizemos que os operadores comutam. Dois operadores comutam se, e apenas se, eles compartilham uma mesma base de autovetores comuns. Propriedades do comutador [A,B]=-[B,A] [A,B]=-[B,A] [A+B,C]=[A,C]+[B,C] [A+B,C]=[A,C]+[B,C] [A,BC]=[A,B]C+B[A,C] [A,BC]=[A,B]C+B[A,C] Se \\hat{X} \\hat{X} e \\hat{P} \\hat{P} representam os operadores posi\u00e7\u00e3o e momento linear, ent\u00e3o [\\hat{X} [\\hat{X} , \\hat{P}]=i\\hbar \\hat{P}]=i\\hbar , enquanto [\\hat{X} [\\hat{X} , \\hat{X}]= [\\hat{P} \\hat{X}]= [\\hat{P} , \\hat{P}]=0. \\hat{P}]=0. Conjunto Completo de Observ\u00e1veis que Comutam (CCOC) Um conjunto de operadores \\hat{A} \\hat{A} , \\hat{B} \\hat{B} , \\hat{C}, \\dots \\hat{C}, \\dots forma um CCCO se todos os subpares desses operadores comutam entre si. [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 Isso implica que existe uma base comum de autovetores que \u00e9 \u00fanica para todos eles, exceto por um fator mutiplicativo.","title":"Aulas S5"},{"location":"Aulas-S5/#5-estrutura-matematica-da-mecanica-quantica","text":"At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a \"F\u00edsica\" do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica.","title":"5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica"},{"location":"Aulas-S5/#51-espaco-de-estados","text":"Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia.","title":"5.1  Espa\u00e7o de estados"},{"location":"Aulas-S5/#52-espaco-vetorial-linear","text":"Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real).","title":"5.2 Espa\u00e7o vetorial linear"},{"location":"Aulas-S5/#dimensao-do-espaco","text":"Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o. Como veremos adiante, os espa\u00e7cos de Hilbert da MQ podem ser infinitos.","title":"Dimens\u00e3o do espa\u00e7o"},{"location":"Aulas-S5/#53-espacos-de-hilbert-espacos-vetoriais-da-mq","text":"Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem.","title":"5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ"},{"location":"Aulas-S5/#produto-interno","text":"Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (n\u00famero complexo) (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0","title":"Produto interno"},{"location":"Aulas-S5/#comprimentos-e-angulos","text":"O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} .","title":"Comprimentos e \u00e2ngulos"},{"location":"Aulas-S5/#expansao-de-vetores","text":"No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo.","title":"Expans\u00e3o de vetores"},{"location":"Aulas-S5/#54-notacao-de-dirac","text":"Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k.","title":"5.4 Nota\u00e7\u00e3o de Dirac"},{"location":"Aulas-S5/#propriedades-do-produto-interno","text":"Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right.","title":"Propriedades do produto interno"},{"location":"Aulas-S5/#55-vetores-de-base","text":"O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de vetores da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1","title":"5.5 Vetores de base"},{"location":"Aulas-S5/#procedimento-de-gram-schmidt","text":"Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar o procedimento de Gram-Schmidt para construir uma base ortonormal a partir desse conjunto inicial. Para simplificar o entendimento do processo, consideramos um exemplo com 3 vetores de base (num espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes da base subtraindo deles as componentes nas dire\u00e7\u00f5es ortonais \u00e0quelas j\u00e1 constru\u00eddas. Neste caso, por exemplo, as dire\u00e7\u00f5es |w_2\\rangle |w_2\\rangle e |w_3\\rangle |w_3\\rangle s\u00e3o constru\u00eddas subtraindo as componente na dire\u00e7\u00e3o de |w_1\\rangle |w_1\\rangle e |w_2\\rangle |w_2\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |v_i\\rangle \\} \\{ |v_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} De forma geral, para um cojunto finito de vetores \\{u_k\\} \\{u_k\\} , de um espa\u00e7o vetorial \\mathcal{U} \\mathcal{U} de dimens\u00e3o d d , pode-se escrever os vetores ortonormais \\{v_k\\} \\{v_k\\} atrav\u00e9s da construindo: \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}.","title":"Procedimento de Gram-Schmidt"},{"location":"Aulas-S5/#algebra-de-dirac","text":"Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor |\\psi\\rangle |\\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i).","title":"Algebra de Dirac"},{"location":"Aulas-S5/#56-operadores-lineares","text":"Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)!","title":"5.6 Operadores lineares"},{"location":"Aulas-S5/#exemplos-importantes","text":"Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1}","title":"Exemplos importantes"},{"location":"Aulas-S5/#representacao-de-operadores","text":"A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)","title":"Representa\u00e7\u00e3o de operadores"},{"location":"Aulas-S5/#valores-esperados","text":"O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado.","title":"Valores esperados"},{"location":"Aulas-S5/#autovalores-e-autovetores","text":"Quando um operador age sobre um dado vetor (estado) e o resultado \u00e9 o mesmo vetor (estado) multiplicado por um escalar, o vetor \u00e9 chamado de autovetor (autoestado) e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico e s\u00e3o muito importantes, pois esses autovetores formam uma base do espa\u00e7o e permitem represetar qualquer estado do sistema. A seguir temos uma breve revis\u00e3o de como calcular autovalores e autovetores, a partir de conceitos e m\u00e9todos de Algebra Linear. C\u00e1lculo dos autovalores Dado um operador linear \\hat{T} \\hat{T} , como j\u00e1 vimos, pode-se sempre represent\u00e1-lo por uma matriz T T . O conjunto de autovalores \\lambda \\lambda dessa matriz podem ser determinados atrav\u00e9s da equa\u00e7\u00e3o caracter\u00edstica (tamb\u00e9m chamada de equa\u00e7\u00e3o secular ), para o determinante abaixo: $$ \\operatorname{det}(T-\\lambda I)=0$$ onde I=\\mathbb{1} I=\\mathbb{1} \u00e9 a matriz identidade. A solu\u00e7\u00e3o da equa\u00e7\u00e3o caracter\u00edstica fornece os autovalores \\lambda \\lambda , que s\u00e3o as raizes do polin\u00f4nimo ( caracter\u00edstico ), indicado acima. Exerc\u00edcio sugerido Escreva o equa\u00e7\u00e3o caracter\u00edstica e ache os autovalores da matriz A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} C\u00e1lculo dos autovetores A partir dos autovalores pode-se determinar os autovetores da matriz T T , que pode ser ent\u00e3o escrita na forma diagonal. Para ilustrar melhor isso, usaremos um exemplo, a partir do problema proposto a seguir. Exerc\u00edcio sugerido Considere o operador \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} , expresso numa base ortonormal. Ache a matriz T T , que representa o operador nesta base, e determine os autovetores normalizados do operador, com seus autovalores. Considere que o espa\u00e7o \u00e9 bidimensional. Antes de seguir, voc\u00ea deve resolver o problema proposto acima, em detalhe, pelo menos at\u00e9 onde puder, para ter certeza de que est\u00e1 entendo todos os passos necess\u00e1rios \u00e0 resolu\u00e7\u00e3o do problema. Ao fazer isso ir\u00e1 encontrar os valores que usaremos na resolu\u00e7\u00e3o que exemplificada o c\u00e1lculo de um dos autovetores, a seguir Exemplo: resolu\u00e7\u00e3o dos autovetores Os autovalores do problema anterio s\u00e3o \\lambda_1=2 \\lambda_1=2 e \\lambda_2=-1 \\lambda_2=-1 . Substitui-se, ent\u00e3o, esses valores, um de cada vez, na equa\u00e7\u00e3o de autovalores \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} para determinar os autovetores \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} , como \u00e9 mostrado abaixo para \\ket{u_2} \\ket{u_2} . \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. portanto, \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. Normalizando o vetor temos: \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, portanto, finalmente, temos: \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. Verifique agora que \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.","title":"Autovalores e autovetores"},{"location":"Aulas-S5/#conjugacao-hermitiana","text":"At\u00e9 agora vimos que um operador age num ket para produzir um novo ket , de acordo com \\hat{T} \\ket{u} = \\ket{v}. \\hat{T} \\ket{u} = \\ket{v}. Vejamos agora, mais atentamente, sua a\u00e7\u00e3o dentro de um produto interno \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} . Sabemos que isso resulta num escalar (n\u00famero) complexo. Podemos tomar complexo conjugado desse n\u00famero, usando a rela\u00e7\u00e3o \\bra{w}v\\rangle = \\bra{v}w\\rangle^* \\bra{w}v\\rangle = \\bra{v}w\\rangle^* . Observe atentamente o que ocorre com o operador \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} onde \\hat{T^{\\dagger}} \\hat{T^{\\dagger}} (pronuncia-se \"T dagger \") \u00e9 chamado de Hermitiado conjugaddo ou adjunto do operador \\hat{T} \\hat{T} . Como formar o Adjunto de uma express\u00e3o geral? Substitua qualquer constante por seu complexo conjugado. Substitua kets pelos bras associados, e vice-versa. Substitua cada operador por seu Adjunto. Inverta a ordem de todos os fatores na express\u00e3o. O Hermitiado Conjugado de uma matriz J\u00e1 sabemos como encontrar a matriz M M de um operador \\hat{M} \\hat{M} qualquer. Para encontrar a matriz do Adjunto desse, simbolizada por M^{dagger} M^{dagger} , basta seguir os seguintes passos: Matriz Adjunta Calcule a matriz transposta M^T M^T , trocando as linhas pelas colunas. Tome o complexo conjugado de cada elemento de M^T. M^T. De forma resumida: M^{\\dagger}= \\left( M^T \\right)^*. M^{\\dagger}= \\left( M^T \\right)^*. Propriedade da opera\u00e7\u00e3o de transposi\u00e7\u00e3o (A+B)^T = A^T + B^T. (A+B)^T = A^T + B^T. (A^T)^T = A. (A^T)^T = A. (aA)^T= a A^T. (aA)^T= a A^T. (AB)^T = B^T A^T. (AB)^T = B^T A^T.","title":"Conjuga\u00e7\u00e3o Hermitiana"},{"location":"Aulas-S5/#operadores-hermitianos","text":"Um operador \u00e9 dito Hermitiano quando \\hat{T}^{\\dagger}=\\hat{T} \\hat{T}^{\\dagger}=\\hat{T} . Para um operador Hemitiano, temos que \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* Veremos que os operadores de observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica devem ser sempre operadores Hermitianos. Como esse operadores podem ser representado por matrizes, \u00e9 interessante ver com determinar se uma matriz \u00e9 Hemitiana. Matriz Hermitiana Uma matriz M M \u00e9 Hermitiana se satisfaz: M = M^{\\dagger}. M = M^{\\dagger}. Como vimos, M^{\\dagger} M^{\\dagger} corresponde ao complexo conjugado da matriz transposta. Portanto, para satisfazer essa condi\u00e7\u00e3o, os elementos da diagonal principal da matriz devem ser todos n\u00fameros reais (n\u00e3o complexos). Como consequ\u00eancia, o tra\u00e7o do operador (matriz) ser\u00e1, necessariamente um n\u00famero real. Autovalores de um operador Hermitiano Pode-se demonstrar que operadores Hermitianos tem autovalores reais (verifique!). Por conta dessa propriedade, requer-se que todos os observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica, sejam representados por operadores Hermitianos. Portanto, tanto o tra\u00e7o como os autovalores de um operador Hermitiano s\u00e3o n\u00fameros reais. Operador anti-Hermitiano Um operador \\hat{A} \\hat{A} \u00e9 dito anti-Hermitiano se: A^{\\dagger}=-A A^{\\dagger}=-A Verifique que, neste caso, os elementos da diagonal principal do operador (matriz) anti-Hermitiano(a) s\u00e3o todos n\u00fameros imagin\u00e1rios puros.","title":"Operadores Hermitianos"},{"location":"Aulas-S5/#operadores-unitarios","text":"Um operador \\hat{U} \\hat{U} (de matriz U U ) \u00e9 unit\u00e1rio se: UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} Isso significa que U^{\\dagger} = U^{-1} U^{\\dagger} = U^{-1} ou seja, que a matriz adjunta \u00e9 igual a matriz inversa. Outra importante caracter\u00edstica das matrizes unit\u00e1rias \u00e9 que as linhas e colunas dessa matrizes formam um conjunto de vetores ortonormais.","title":"Operadores Unit\u00e1rios"},{"location":"Aulas-S5/#comutadores","text":"Seja \\hat{A} \\hat{A} e \\hat{B} \\hat{B} dois operadores lineares do espa\u00e7o. Em geral, temos que \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. Assim, define-se o comutador [\\hat{A},\\hat{B}] [\\hat{A},\\hat{B}] como sendo [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. Se [\\hat{A},\\hat{B}]=0 [\\hat{A},\\hat{B}]=0 , dizemos que os operadores comutam. Dois operadores comutam se, e apenas se, eles compartilham uma mesma base de autovetores comuns. Propriedades do comutador [A,B]=-[B,A] [A,B]=-[B,A] [A+B,C]=[A,C]+[B,C] [A+B,C]=[A,C]+[B,C] [A,BC]=[A,B]C+B[A,C] [A,BC]=[A,B]C+B[A,C] Se \\hat{X} \\hat{X} e \\hat{P} \\hat{P} representam os operadores posi\u00e7\u00e3o e momento linear, ent\u00e3o [\\hat{X} [\\hat{X} , \\hat{P}]=i\\hbar \\hat{P}]=i\\hbar , enquanto [\\hat{X} [\\hat{X} , \\hat{X}]= [\\hat{P} \\hat{X}]= [\\hat{P} , \\hat{P}]=0. \\hat{P}]=0.","title":"Comutadores"},{"location":"Aulas-S5/#conjunto-completo-de-observaveis-que-comutam-ccoc","text":"Um conjunto de operadores \\hat{A} \\hat{A} , \\hat{B} \\hat{B} , \\hat{C}, \\dots \\hat{C}, \\dots forma um CCCO se todos os subpares desses operadores comutam entre si. [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 Isso implica que existe uma base comum de autovetores que \u00e9 \u00fanica para todos eles, exceto por um fator mutiplicativo.","title":"Conjunto Completo de Observ\u00e1veis que Comutam (CCOC)"},{"location":"Aulas_S5-S6/","text":"\\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} 5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a \"F\u00edsica\" do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica. 5.1 Espa\u00e7o de estados Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia. 5.2 Espa\u00e7o vetorial linear Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real). Dimens\u00e3o do espa\u00e7o Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o. Como veremos adiante, os espa\u00e7cos de Hilbert da MQ podem ser infinitos. 5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem. Produto interno Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (n\u00famero complexo) (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0 Comprimentos e \u00e2ngulos O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Expans\u00e3o de vetores No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo. 5.4 Nota\u00e7\u00e3o de Dirac Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. Propriedades do produto interno Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. 5.5 Vetores de base O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de vetores da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 Procedimento de Gram-Schmidt Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar o procedimento de Gram-Schmidt para construir uma base ortonormal a partir desse conjunto inicial. Para simplificar o entendimento do processo, consideramos um exemplo com 3 vetores de base (num espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes da base subtraindo deles as componentes nas dire\u00e7\u00f5es ortonais \u00e0quelas j\u00e1 constru\u00eddas. Neste caso, por exemplo, as dire\u00e7\u00f5es |w_2\\rangle |w_2\\rangle e |w_3\\rangle |w_3\\rangle s\u00e3o constru\u00eddas subtraindo as componente na dire\u00e7\u00e3o de |w_1\\rangle |w_1\\rangle e |w_2\\rangle |w_2\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |v_i\\rangle \\} \\{ |v_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} De forma geral, para um cojunto finito de vetores \\{u_k\\} \\{u_k\\} , de um espa\u00e7o vetorial \\mathcal{U} \\mathcal{U} de dimens\u00e3o d d , pode-se escrever os vetores ortonormais \\{v_k\\} \\{v_k\\} atrav\u00e9s da construindo: \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. Algebra de Dirac Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor |\\psi\\rangle |\\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). 5.6 Operadores lineares Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)! Exemplos importantes Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa certa dire\u00e7\u00e3o \\ket{u_i} \\ket{u_i} , ou numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} Representa\u00e7\u00e3o de operadores A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) Valores esperados O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado. Autovalores e autovetores Quando um operador age sobre um dado vetor (estado) e o resultado \u00e9 o mesmo vetor (estado) multiplicado por um escalar, o vetor \u00e9 chamado de autovetor (autoestado) e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico e s\u00e3o muito importantes, pois esses autovetores formam uma base do espa\u00e7o e permitem represetar qualquer estado do sistema. A seguir temos uma breve revis\u00e3o de como calcular autovalores e autovetores, a partir de conceitos e m\u00e9todos de Algebra Linear. C\u00e1lculo dos autovalores Dado um operador linear \\hat{T} \\hat{T} , como j\u00e1 vimos, pode-se sempre represent\u00e1-lo por uma matriz T T . O conjunto de autovalores \\lambda \\lambda dessa matriz podem ser determinados atrav\u00e9s da equa\u00e7\u00e3o caracter\u00edstica (tamb\u00e9m chamada de equa\u00e7\u00e3o secular ), para o determinante abaixo: $$ \\operatorname{det}(T-\\lambda I)=0$$ onde I=\\mathbb{1} I=\\mathbb{1} \u00e9 a matriz identidade. A solu\u00e7\u00e3o da equa\u00e7\u00e3o caracter\u00edstica fornece os autovalores \\lambda \\lambda , que s\u00e3o as raizes do polin\u00f4nimo ( caracter\u00edstico ), indicado acima. Exerc\u00edcio sugerido Escreva o equa\u00e7\u00e3o caracter\u00edstica e ache os autovalores da matriz A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} C\u00e1lculo dos autovetores A partir dos autovalores pode-se determinar os autovetores da matriz T T , que pode ser ent\u00e3o escrita na forma diagonal. Para ilustrar melhor isso, usaremos um exemplo, a partir do problema proposto a seguir. Exerc\u00edcio sugerido Considere o operador \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} , expresso numa base ortonormal. Ache a matriz T T , que representa o operador nesta base, e determine os autovetores normalizados do operador, com seus autovalores. Considere que o espa\u00e7o \u00e9 bidimensional. Antes de seguir, voc\u00ea deve resolver o problema proposto acima, em detalhe, pelo menos at\u00e9 onde puder, para ter certeza de que est\u00e1 entendo todos os passos necess\u00e1rios \u00e0 resolu\u00e7\u00e3o do problema. Ao fazer isso ir\u00e1 encontrar os valores que usaremos na resolu\u00e7\u00e3o que exemplificada o c\u00e1lculo de um dos autovetores, a seguir Exemplo: resolu\u00e7\u00e3o dos autovetores Os autovalores do problema anterio s\u00e3o \\lambda_1=2 \\lambda_1=2 e \\lambda_2=-1 \\lambda_2=-1 . Substitui-se, ent\u00e3o, esses valores, um de cada vez, na equa\u00e7\u00e3o de autovalores \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} para determinar os autovetores \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} , como \u00e9 mostrado abaixo para \\ket{u_2} \\ket{u_2} . \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. portanto, \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. Normalizando o vetor temos: \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, portanto, finalmente, temos: \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. Verifique agora que \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. Conjuga\u00e7\u00e3o Hermitiana At\u00e9 agora vimos que um operador age num ket para produzir um novo ket , de acordo com \\hat{T} \\ket{u} = \\ket{v}. \\hat{T} \\ket{u} = \\ket{v}. Vejamos agora, mais atentamente, sua a\u00e7\u00e3o dentro de um produto interno \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} . Sabemos que isso resulta num escalar (n\u00famero) complexo. Podemos tomar complexo conjugado desse n\u00famero, usando a rela\u00e7\u00e3o \\bra{w}v\\rangle = \\bra{v}w\\rangle^* \\bra{w}v\\rangle = \\bra{v}w\\rangle^* . Observe atentamente o que ocorre com o operador \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} onde \\hat{T^{\\dagger}} \\hat{T^{\\dagger}} (pronuncia-se \"T dagger \") \u00e9 chamado de conjugado Hermitiano, ou Hermitiado conjugado, ou ainda adjunto do operador \\hat{T} \\hat{T} . Como formar o Adjunto de uma express\u00e3o geral? Substitua qualquer constante por seu complexo conjugado. Substitua kets pelos bras associados, e vice-versa. Substitua cada operador por seu Adjunto. Inverta a ordem de todos os fatores na express\u00e3o. O conjugado Hermitiado de uma matriz J\u00e1 sabemos como encontrar a matriz M M de um operador \\hat{M} \\hat{M} qualquer. Para encontrar a matriz do Adjunto desse, simbolizada por M^{\\dagger} M^{\\dagger} , basta seguir os seguintes passos: Matriz Adjunta Calcule a matriz transposta M^T M^T , trocando as linhas pelas colunas. Tome o complexo conjugado de cada elemento de M^T. M^T. De forma resumida: M^{\\dagger}= \\left( M^T \\right)^*. M^{\\dagger}= \\left( M^T \\right)^*. Propriedade da opera\u00e7\u00e3o de transposi\u00e7\u00e3o (A+B)^T = A^T + B^T. (A+B)^T = A^T + B^T. (A^T)^T = A. (A^T)^T = A. (aA)^T= a A^T. (aA)^T= a A^T. (AB)^T = B^T A^T. (AB)^T = B^T A^T. Operadores Hermitianos Um operador \u00e9 dito Hermitiano quando \u00e9 auto-adjunto: \\hat{T}^{\\dagger}=\\hat{T} \\hat{T}^{\\dagger}=\\hat{T} . Isto \u00e9, quando o seu adjunto \u00e9 ele pr\u00f3prio. Para um operador Hemitiano, temos que \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* Veremos que os operadores de observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica devem ser sempre operadores Hermitianos. Como esse operadores podem ser representado por matrizes, \u00e9 interessante ver com determinar se uma matriz \u00e9 Hemitiana. Matriz Hermitiana Uma matriz M M \u00e9 Hermitiana se satisfaz: M = M^{\\dagger}. M = M^{\\dagger}. Como vimos, M^{\\dagger} M^{\\dagger} corresponde ao complexo conjugado da matriz transposta. Portanto, para satisfazer essa condi\u00e7\u00e3o, os elementos da diagonal principal da matriz devem ser todos n\u00fameros reais (n\u00e3o complexos). Como consequ\u00eancia, o tra\u00e7o do operador (matriz) ser\u00e1, necessariamente um n\u00famero real. Autovalores de um operador Hermitiano Pode-se demonstrar que operadores Hermitianos tem autovalores reais (verifique!). Por conta dessa propriedade, requer-se que todos os observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica, sejam representados por operadores Hermitianos. Portanto, tanto o tra\u00e7o como os autovalores de um operador Hermitiano s\u00e3o n\u00fameros reais. Autovetores de um operador Hermitiano Outra propriedade importante dos operadores (matrizes) hermetianos \u00e9 que os autovalores correspondentes a autovalores diferentes s\u00e3o ortogonais. Tamb\u00e9m n\u00e3o \u00e9 dif\u00edcil demonstrar essa propriedade ( verifique! ), usando a propriedade anterior, que os autovalores s\u00e3o n\u00fameros reais. Como os autovetores s\u00e3o ortogonais, \u00e9 poss\u00edvel, portanto, construir uma base (ortonormal) que gere o espa\u00e7o. Propriedades de operadores hermitianos Autovalores reais Autovetores ortogonais, para autovalores diferentes Autovetores geram o espa\u00e7o (podem formar uma base) Operador anti-Hermitiano Um operador \\hat{A} \\hat{A} \u00e9 dito anti-Hermitiano se: A^{\\dagger}=-A A^{\\dagger}=-A Verifique que, neste caso, os elementos da diagonal principal do operador (matriz) anti-Hermitiano(a) s\u00e3o todos n\u00fameros imagin\u00e1rios puros. Operadores Normais Um operador A A \u00e9 dito ser normal se AA^{\\dagger} = A^{\\dagger}A. AA^{\\dagger} = A^{\\dagger}A. Claramente, um operador Hermitiano tamb\u00e9m \u00e9 um operador normal. Operadores normais s\u00e3o interessantes pois h\u00e1 um teorema importante relacionado \u00e0 sua representa\u00e7\u00e3o que garante que eles podem ser escritos numa forma chamada de decomposi\u00e7\u00e3o espectral , e que ser\u00e1 bastante \u00fatil mais adiante. Voltaremos a falar deles quando discutirmos o processo de diagonaliza\u00e7\u00e3o de um operador. Operadores Unit\u00e1rios Um operador \\hat{U} \\hat{U} (de matriz U U ) \u00e9 unit\u00e1rio se: UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} Isso significa que U^{\\dagger} = U^{-1} U^{\\dagger} = U^{-1} ou seja, que a matriz adjunta \u00e9 igual a matriz inversa. Outra importante caracter\u00edstica das matrizes unit\u00e1rias \u00e9 que as linhas e colunas dessa matrizes formam um conjunto de vetores ortonormais. Pode-se perceber ainda que operadores unit\u00e1rios tamb\u00e9m s\u00e3o operadores normais e, portanto, podem tem uma decomposi\u00e7\u00e3o spectral, como veremos depois. Finalmente, outra caracter\u00edstica importante desses operadores \u00e9 que geometricamente eles preservam o produto interno entre vetores, com pode ser facilmente verificado ( U\\ket{v},U\\ket{w}) = \\bra{v}U^{\\dagger}U\\ket{w}=\\braket{v}{\\mathbb{1}}{w}=\\bra{v}w\\rangle. ( U\\ket{v},U\\ket{w}) = \\bra{v}U^{\\dagger}U\\ket{w}=\\braket{v}{\\mathbb{1}}{w}=\\bra{v}w\\rangle. Comutadores e anticomutadores Seja \\hat{A} \\hat{A} e \\hat{B} \\hat{B} dois operadores lineares do espa\u00e7o. Em geral, temos que \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. Assim, define-se o comutador [\\hat{A},\\hat{B}] [\\hat{A},\\hat{B}] como sendo [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. Se [\\hat{A},\\hat{B}]=0 [\\hat{A},\\hat{B}]=0 , dizemos que os operadores comutam. Dois operadores comutam se, e apenas se, eles compartilham uma mesma base de autovetores comuns. Propriedades do comutador [A,B]=-[B,A] [A,B]=-[B,A] [A+B,C]=[A,C]+[B,C] [A+B,C]=[A,C]+[B,C] [A,BC]=[A,B]C+B[A,C] [A,BC]=[A,B]C+B[A,C] Se \\hat{X} \\hat{X} e \\hat{P} \\hat{P} representam os operadores posi\u00e7\u00e3o e momento linear, ent\u00e3o [\\hat{X} [\\hat{X} , \\hat{P}]=i\\hbar \\hat{P}]=i\\hbar , enquanto [\\hat{X} [\\hat{X} , \\hat{X}]= [\\hat{P} \\hat{X}]= [\\hat{P} , \\hat{P}]=0. \\hat{P}]=0. Anticomutador Define-se o anticomutador \\{\\hat{A},\\hat{B}\\} \\{\\hat{A},\\hat{B}\\} como sendo \\{ \\hat{A},\\hat{B} \\} = \\hat{A}\\hat{B} + \\hat{B}\\hat{A}. \\{ \\hat{A},\\hat{B} \\} = \\hat{A}\\hat{B} + \\hat{B}\\hat{A}. Conjunto Completo de Observ\u00e1veis que Comutam (CCOC) Um conjunto de operadores \\hat{A} \\hat{A} , \\hat{B} \\hat{B} , \\hat{C}, \\dots \\hat{C}, \\dots forma um CCCO se todos os subpares desses operadores comutam entre si. [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 Isso implica que existe uma base comum de autovetores que \u00e9 \u00fanica para todos eles, exceto por um fator mutiplicativo. Para avan\u00e7ar para os t\u00f3picos das pr\u00f3ximas aulas, use o menu de navega\u00e7\u00e3o ou clique aqui .","title":"Aulas S5 & S6"},{"location":"Aulas_S5-S6/#5-estrutura-matematica-da-mecanica-quantica","text":"At\u00e9 este ponto, discutimos, em linhas gerais, como expressar e resolver problemas f\u00edsicos na mec\u00e2nica qu\u00e2ntica, em termos da Equa\u00e7\u00e3o de Schr\u00f6dinger (EqS). Discutimos, de uma maneira ampla, as estrat\u00e9gias para resolver a EqS no caso geral e, em particular, discutimos a resolu\u00e7\u00e3o da equa\u00e7\u00e3o independente do tempo, resolvendo alguns exemplos emblem\u00e1ticos de potencias unidimencionais simples. Visto sob essa perspectiva, pode-se ter a impress\u00e3o que mec\u00e2nica qu\u00e2ntica se resume \u00e0 solu\u00e7\u00e3o da EqS, usando m\u00e9todos matem\u00e1ticos mais ou menos familiares (solu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais parciais). Embora essa seja uma estrat\u00e9gia v\u00e1lida e efetiva em alguns casos, ela \u00e9 bastante limitada e seria um grande equ\u00edvoco pensar que as estrat\u00e9gias da mec\u00e2nica qu\u00e2ntica se limitam simplesmente a solu\u00e7\u00f5es da Eq. de Schr\u00f6dinger. O roteiro seguido at\u00e9 aqui teve uma motiva\u00e7\u00e3o did\u00e1tica e, deliberamente, procurou enfatizar os aspectos f\u00edsicos do problema. Apresentando apenas a matem\u00e1tica necess\u00e1ria para formular e resolver o problema. Por essa raz\u00e3o, n\u00e3o temos sido muito rigorosos com o formalismo. Trocando rigor matem\u00e1tico por intui\u00e7\u00e3o f\u00edsica, sempre que poss\u00edvel, para n\u00e3o obscurecer desnecessariamente a \"F\u00edsica\" do problema. Essa estrat\u00e9gia \u00e9 bastante razo\u00e1vel para uma introdu\u00e7\u00e3o ao assunto. Apesar disso, o dom\u00ednio do formalismo matem\u00e1tico tamb\u00e9m \u00e9 importante e necess\u00e1rio para ser bem sucedido na resolu\u00e7\u00e3o de problemas gerais da MQ, ou mesmo para entender muitos temas de pesquisa contempor\u00e2nea. A situa\u00e7\u00e3o ideal \u00e9 aquela onde consegue-se combinar ambas habilidades, que \u00e9 um dos objetivos secund\u00e1rios deste curso. Neste capitulo, portanto, seguiremos uma estrat\u00e9gia diferente e complementar \u00e0quela seguida at\u00e9 agora. O foco agora ser\u00e1 ampliar a linguagem e abstra\u00e7\u00e3o do problema, apresentadno de modo mais formal a estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica moderna. A prioridade ainda permanecer\u00e1 com a F\u00edsica e n\u00e3o a Matem\u00e1tica. Portanto, n\u00e3o se almeja mero rigor matem\u00e1tico, mas, sim, introduzir novos conceitos e representa\u00e7\u00f5es que ser\u00e3o muito \u00fateis para expandir os horizontes dentro da teoria e, como iremos explorar nos pr\u00f3ximos cap\u00edtulos, ser\u00e3o fundamentais para entender a linguagem contempor\u00e2nea dessa importante disciplina cient\u00edfica.","title":"5. Estrutura matem\u00e1tica da Mec\u00e2nica Qu\u00e2ntica"},{"location":"Aulas_S5-S6/#51-espaco-de-estados","text":"Resumindo o que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, motivada pela forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: Dentro do que vimos at\u00e9 aqui, podemos, ainda de uma maneira informal, dizer que as solu\u00e7\u00f5es estacion\u00e1rias \\psi_n(x) \\psi_n(x) da EqS s\u00e3o fun\u00e7\u00f5es de ondas que representam os poss\u00edveis estados do sistema, com energia E_n E_n . Outra forma de dizer isso, observando a forma da equa\u00e7\u00e3o H\\psi_n(x)=E_n\\psi_n(x) H\\psi_n(x)=E_n\\psi_n(x) , \u00e9 dizer que \\{\\psi_n(x)\\} \\{\\psi_n(x)\\} \u00e9 o conjunto de autofun\u00e7\u00f5es do operador H H , representando os autoestados do sistema com autovalores E_n E_n . Vimos nos exemplos discutidos, como no caso da caixa infinita, que \\psi_n(x) \\psi_n(x) possui uma s\u00e9rie de propriedades interessantes e \u00fateis. Entre elas: \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\int \\psi^*_n(x)\\psi_m(x)dx=\\delta_{nm} \\Psi(x)=\\sum_n c_n \\psi_n(x) \\Psi(x)=\\sum_n c_n \\psi_n(x) c_n = \\int \\psi^*_n(x) \\Psi(x)dx c_n = \\int \\psi^*_n(x) \\Psi(x)dx ; onde \\sum_n |c_n|^2 = 1 \\sum_n |c_n|^2 = 1 <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx <A_{_{\\Psi}}> = \\int \\Psi^*(x) A \\Psi(x) dx De fato, pode-se extender e generalizar essas ideias para expressar esses objetos em termos mais abstratos e gerais, atrav\u00e9s do conceito de espa\u00e7o vetorial linear. Como os estados \\psi_n(x) \\psi_n(x) e os operadores (que nesse contexto ser\u00e3o transforma\u00e7\u00f5es lineares) nesses estados devem satisfazer um certas propriedades para representar um sistema f\u00edsico, esses espa\u00e7os vetoriais devem ter conjunto de estruturas e propriedades especiais que veremos logo mais. Por simplicidade, iremos nos referir a esses espa\u00e7os como espa\u00e7os de Hilbert . Para deixar esse ponto mais claro, vamos relembrar/introduzir algumas defini\u00e7\u00f5es e conceitos, para formalizar e definir melhor essa ideia.","title":"5.1  Espa\u00e7o de estados"},{"location":"Aulas_S5-S6/#52-espaco-vetorial-linear","text":"Partido da defini\u00e7\u00e3o mais geral e abstrata: Defini\u00e7\u00e3o 1 Grupo comutativo sob adi\u00e7\u00e3o, \\mathcal{V} \\mathcal{V} , com multiplica\u00e7\u00e3o por escalar definida sobre um campo complexo \\mathcal{F} \\mathcal{F} , satisfazendo propriedades associativa e distributiva. Os elementos do espa\u00e7o \\mathcal{V} \\mathcal{V} s\u00e3o chamados de vetores e os elementos do campo \\mathcal{F} \\mathcal{F} s\u00e3o escalares . As propriedades associativa e distributiva da multiplica\u00e7\u00e3o por escalar implica: Se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} \\lambda(\\mu\\vec{v})=(\\lambda\\mu)\\vec{v} , \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} \\lambda(\\vec{v}+\\vec{u})=\\lambda\\vec{v}+\\lambda\\vec{u} e (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} (\\lambda+\\mu)\\vec{u}=\\lambda\\vec{u}+\\mu\\vec{u} . Vale lembrar algumas outras defini\u00e7\u00f5es ( Grupo e Campo ), da Algebra: Grupo: Conjunto de elementos, que inclui inversos e identidade, com uma opera\u00e7\u00e3o ( * * ) fechada que satisfaz associatividade. Grupos n\u00e3o precisam ser comutativos, mas quando apresentam essa propriedade s\u00e3o chamados de grupos comutativos ou Abelianos. Fechado : \\forall\\, x,y \\in G \\rightarrow x*y \\in G \\forall\\, x,y \\in G \\rightarrow x*y \\in G Associativo : \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) \\forall\\, x,y,z \\in G \\rightarrow (x*y)*z=x*(y*z) Identidade : \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G \\exists\\, e\\in G \\rightarrow e*x=x*e=x; \\,\\, \\forall\\, x \\in G Inverso : \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e \\forall\\, x \\in G, \\exists\\, x^{-1} \\rightarrow (x^{-1})*x=x*(x^{-1})=e Campo: De maneira simples, s\u00e3o conjuntos de elementos onde s\u00e3o definidas as quatro opera\u00e7\u00f5es aritm\u00e9ticas ( + + , - - , \\times \\times , \\div \\div ) de forma comutativa. Como as opera\u00e7\u00f5es ( - - , \\div \\div ) s\u00e3o, na verdade, opera\u00e7\u00f5es inversas de ( + + , \\times \\times ), s\u00e3o definidos em termos dessas duas opera\u00e7\u00f5es. Formalmente, campos s\u00e3o conjuntos de elementos com opera\u00e7\u00f5es de adi\u00e7\u00e3o e multiplica\u00e7\u00e3o ( + + , \\times \\times ) definida; sendo comutativo para ( + + ) e comutativo para ( \\times \\times ) omitindo o elemento nulo (zero). Satisfaz ainda a propriedade distributiva a\\times(b+c)=a\\times b + a\\times c a\\times(b+c)=a\\times b + a\\times c . Campos s\u00e3o, portanto, dois grupos comutativos com duas opera\u00e7\u00f5es ( + + , \\times \\times ). Exemplos importantes s\u00e3o os campos dos n\u00fameros reais, complexos e racionais. Alternativamente, uma defini\u00e7\u00e3o um pouco mais familiar de espa\u00e7o vetorial \u00e9: Defini\u00e7\u00e3o 2: Conjunto \\mathcal{V}\\ne\\emptyset \\mathcal{V}\\ne\\emptyset (n\u00e3o vazio) de elementos, chamados vetores, que \u00e9 fechado sob adi\u00e7\u00e3o e multiplica\u00e7\u00e3o por um escalar de um campo complexo \\mathcal{F} \\mathcal{F} . Ou seja, se \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} \\mathcal{V}=\\{\\vec{u},\\vec{v},\\vec{w},...\\} e \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} \\mathcal{F}=\\{\\lambda,\\mu,\\kappa,...\\} , temos que: \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} \\forall\\, \\vec{u},\\vec{w}\\in \\mathcal{V} e \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} \\forall\\, \\lambda,\\mu \\in \\mathcal{F} \\rightarrow \\lambda\\vec{u}+\\mu\\vec{w} \\in \\mathcal{V} Se o campo \\mathcal{F} \\mathcal{F} \u00e9 complexo (real) o espa\u00e7o \u00e9 dito ser um espa\u00e7o vetorial linear complexo (real).","title":"5.2 Espa\u00e7o vetorial linear"},{"location":"Aulas_S5-S6/#dimensao-do-espaco","text":"Um conjunto de vetores \\{\\phi_n \\} \\{\\phi_n \\} \u00e9 dito linearmente independente (LI) se n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o linear n\u00e3o-trivial que leve ao vetor nulo, isto \u00e9: \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n \\sum_n c_n \\phi_n = 0 \\rightarrow c_n = 0\\, \\forall\\, n . A dimens\u00e3o d d do espa\u00e7o vetorial \u00e9 dada pelo n\u00famero m\u00e1ximo de vetores LI desse espa\u00e7o. Qualquer vetor do espa\u00e7o pode ser escrito como uma combina\u00e7\u00e3o linear dos vetores da base desse espa\u00e7o, formado por vetores LI do espa\u00e7o. Como veremos adiante, os espa\u00e7cos de Hilbert da MQ podem ser infinitos.","title":"Dimens\u00e3o do espa\u00e7o"},{"location":"Aulas_S5-S6/#53-espacos-de-hilbert-espacos-vetoriais-da-mq","text":"Na mec\u00e2nica qu\u00e2ntica s\u00e3o usados espa\u00e7os vetoriais com algumas propriedades e estruturas adicionais, para garantir certas propriedades f\u00edsicas desej\u00e1veis da teoria. \u00c9 comum, principalmente entre os f\u00edsicos, chamar esses estados de estados de Hilbert. Os espa\u00e7os de Hilbert podem ser finitos (com dimens\u00e3o d d ) ou infinitos, por exemplo, quando os vetores s\u00e3o fun\u00e7\u00f5es cont\u00ednuas. Embora essa terminologia n\u00e3o seja muito precisa, dado que os espa\u00e7os vetoriais usados na MQ s\u00e3o apenas um tipo particular de espa\u00e7o de Hilbert (neste contexto: os espa\u00e7os cujos vetores s\u00e3o fun\u00e7\u00f5es quadrado-integr\u00e1veis , tamb\u00e9m chamados de espa\u00e7os de Lebesgue do tipo L_2 L_2 ), n\u00f3s usaremos essa \"conven\u00e7\u00e3o\", para simplificar a linguagem.","title":"5.3 Espa\u00e7os de Hilbert: espa\u00e7os vetoriais da MQ"},{"location":"Aulas_S5-S6/#produto-interno","text":"Uma das estruturas adicionais dos espa\u00e7os de Hilbert \u00e9 o produto interno que leva dois vetores do espa\u00e7o num n\u00famero complexo, segundo a defini\u00e7\u00e3o: \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx \\forall\\, \\phi, \\psi \\in \\mathcal{H} \\rightarrow (\\phi,\\psi) = \\int \\phi^*(x)\\psi(x)\\,dx No caso de um espa\u00e7o discreto de dimens\u00e3o d d , o produto interno \u00e9 definido como (w,v)=\\sum_{i=1}^{d} w_i^* v_i (w,v)=\\sum_{i=1}^{d} w_i^* v_i Note que o produto interno resulta num escalar complexo, que n\u00e3o \u00e9 um elemento do espa\u00e7o de Hilbert. O produto interno tem as seguintes propriedades: (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (\\phi,\\psi) = \\lambda \\in \\mathbb{C} (n\u00famero complexo) (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi,\\psi) = (\\psi,\\phi)^* (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (\\phi, c_1 \\psi_1 + c_2 \\psi_2 ) = c_1(\\phi, \\psi_1) + c_2(\\phi,\\psi_2 ) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (c_1 \\psi_1 + c_2 \\psi_2, \\phi ) = c_1^* (\\psi_1, \\phi) + c_2^*(\\psi_2, \\phi) (\\phi,\\phi) \\ge 0 (\\phi,\\phi) \\ge 0 , sendo nulo apenas quando \\phi=0 \\phi=0","title":"Produto interno"},{"location":"Aulas_S5-S6/#comprimentos-e-angulos","text":"O conceito de produto interno nos permite generalizar os conceitos de comprimento (norma) e medidas de \u00e2ngulos entre vetores em espa\u00e7os de dimens\u00f5es e elementos arbitr\u00e1rios. Embora os vetores agora n\u00e3o sejam mais \"setas\" no espa\u00e7o tridimensional Euclidiano, pode-se explorar a analogia com o conceito de produto escalar (o produto interno) daquele espa\u00e7o, para definir a norma do vetor, atrav\u00e9s do produto interno de um vetor por ele mesmo: (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (\\phi,\\phi) = \\int \\phi^*(x)\\phi(x)\\,dx = |\\phi|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 (v,v) = \\sum_{i=1}^{d} v^*_i v_i = |v|^2 ||\\phi|| = \\sqrt{|\\phi|^2} ||\\phi|| = \\sqrt{|\\phi|^2} ||v|| = \\sqrt{|v|^2} ||v|| = \\sqrt{|v|^2} Observe que a norma \u00e9 sempre um n\u00famero real, tal que ||\\phi|| \\ge 0 ||\\phi|| \\ge 0 e ||v|| \\ge 0 ||v|| \\ge 0 , conforme nos assegura a desigualdade de Schwartz: |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). |(\\psi,\\phi)|^2 \\le (\\psi,\\psi)(\\phi,\\phi). Tamb\u00e9m \u00e9 satisfeito o teorema de desigualdade triangular: ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . ||(\\psi + \\phi)|| \\le ||\\psi|| + ||\\phi|| . Para ambos os casos, a desigualdade s\u00f3 \u00e9 v\u00e1lida quando um dos vetores \u00e9 m\u00faltiplo do outro. Dois veltores s\u00e3o tido ortogonais quando seu produto interno \u00e9 nulo. Da mesma forma, um conjunto de vetores \\{\\phi_n\\} \\{\\phi_n\\} \u00e9 dito ortonormal quando o produto interno entre pares de seus elementos obedece a rela\u00e7\u00e3o (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} .","title":"Comprimentos e \u00e2ngulos"},{"location":"Aulas_S5-S6/#expansao-de-vetores","text":"No caso em que \\mathcal{H} \\mathcal{H} \u00e9 finito, com dimens\u00e3o d d , dado um vetor arbitr\u00e1rio \\psi \\psi e uma base \\{ \\phi_n \\} \\{ \\phi_n \\} de vetores linearmente independentes, podemos expressar o vetor \\psi = \\sum_n c_n \\phi_n \\psi = \\sum_n c_n \\phi_n , onde c_n=(\\phi_n,\\psi) c_n=(\\phi_n,\\psi) e (\\phi_n,\\phi_m)=\\delta_{nm} (\\phi_n,\\phi_m)=\\delta_{nm} . Podemos pensar nos coeficientes c_n c_n como sendo as componentes do vetor no espa\u00e7o de Hilbert, an\u00e1logos \u00e0s componentes de um vetor no espa\u00e7o Euclidiano. Por\u00e9m, \u00e9 importante lembrar que essas componentes s\u00e3o expressas por n\u00fameros complexos. As componente do vetor de estado t\u00eam toda a informa\u00e7\u00e3o relativa ao estado, determinando completamente o vetor (estado) do sistema. Tamb\u00e9m de modo an\u00e1logo, podemos expressar as soma de dois vetore em termos dessas componentes \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\Psi_a + \\Psi_b = \\sum_n (a_i + b_n) \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. \\lambda \\Psi_a= \\sum_n \\lambda a_i \\psi_n. Pare, Pense & Contemple! Antes de prosseguir, pare e reflita por um momento no significado e amplitude esses resultados. Lembre-se que o espa\u00e7o \\mathcal{H} \\mathcal{H} pode ter dimens\u00f5e infinitas, tanto no n\u00famero de elemento (vetores), como nas dimens\u00f5es (n\u00famero de componentes) desses vetores. Esses resultados, nada \u00f3bvios, s\u00e3o extremamente poderosos e \u00fateis, justificando plenamente o tempo investido em generalizar e abstrair a descri\u00e7\u00e3o dos nossos problemas usando esse formalismo.","title":"Expans\u00e3o de vetores"},{"location":"Aulas_S5-S6/#54-notacao-de-dirac","text":"Introduzimos agora a nota\u00e7\u00e3o de Dirac, bastante popular na mec\u00e2nica qu\u00e2ntica, onde o vetor de estado \u00e9 chamado de \" ket \" e representado pelo s\u00edmbolo |\\psi\\rangle |\\psi\\rangle . O vetor correspondente do espa\u00e7o dual \u00e9 chamado de \" bra \" \u00e9 representado por \\langle\\psi| \\langle\\psi| , de tal forma que o produto interno pode ser representado por (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle (\\psi,\\psi)=\\langle\\psi|\\psi\\rangle . Note que \\langle\\psi|=|\\psi\\rangle^* \\langle\\psi|=|\\psi\\rangle^* , corresponde ao complexo conjugado transposto do vetor de estado |\\psi\\rangle |\\psi\\rangle . Isso fica claro, quando observamos a representa\u00e7\u00e3o matricial desse vetores. Considere, por exemplo, que o vetor de estado tenha n n componentes ( c_1,c_2,...,c_n c_1,c_2,...,c_n ). Neste caso, o \" ket \" |\\psi\\rangle |\\psi\\rangle \u00e9 escrito como um vetor coluna, enquanto o seu vetor dual \" bra \" \u00e9 um vetor linha, conforme indicado abaixo: |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. |\\psi\\rangle = \\left[ \\begin{array}{c} c_1\\\\ c_2\\\\ ...\\\\ c_n \\end{array} \\right] \\, \\Rightarrow \\,\\,\\, \\langle\\psi| = \\left[ \\begin{array}{c} c_{1}^{*} & c^*_2 & ...& c^*_n \\end{array} \\right]. Nesta representa\u00e7\u00e3o, todas as propriedades anteriores s\u00e3o equivalentes a opera\u00e7\u00f5es sobre matrizes (ou vetores linha/coluna), como, por exemplo, soma (subtra\u00e7\u00e3o), multiplica\u00e7\u00e3o por escalares e combina\u00e7\u00f5es lineares dessas opera\u00e7\u00f5es. O produto interno ( \"bracket\" ), como \u00e9 f\u00e1cil perceber, corresponde a uma multiplica\u00e7\u00e3o de matrizes, resultando num escalar: \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k. \\langle\\phi|\\psi\\rangle = \\left[ \\begin{array}{l} b^*_1 & b^*_2 & ... & b^*_n \\end{array} \\right] \\left[ \\begin{array}{c} c_1 \\\\ c_2 \\\\ ... \\\\ c_n \\end{array} \\right] = \\begin{array}{l} b^*_1\\,c_1 & b^*_2\\,c_2 & ...& b^*_n\\,c_n \\end{array} = \\sum_{k=1}^n b^*_k\\,c_k.","title":"5.4 Nota\u00e7\u00e3o de Dirac"},{"location":"Aulas_S5-S6/#propriedades-do-produto-interno","text":"Reescrevemos aqui as propriedade dos produto interno, na nota\u00e7\u00e3o de Dirac. Para os vetores |\\psi\\rangle |\\psi\\rangle e |\\phi\\rangle |\\phi\\rangle , pertencentes ao espa\u00e7o \\mathcal{H} \\mathcal{H} , e os escalares \\alpha \\alpha e \\beta \\beta do campo complexo \\mathcal{F} \\mathcal{F} , as seguintes propriedades s\u00e3o satisfeitas: \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} \\begin{array}{ll} 1.\\, &\\langle\\psi|\\phi\\rangle = \\langle\\phi|\\psi\\rangle ^* \\\\ 2.\\, &\\langle\\psi|(\\alpha|\\phi\\rangle+\\beta|\\eta\\rangle) = \\alpha\\langle\\psi|\\phi\\rangle + \\beta\\langle\\psi|\\eta\\rangle \\\\ 3.\\, &(\\alpha\\langle\\phi| +\\beta\\langle\\eta|)|\\psi\\rangle = \\alpha^*\\langle\\phi|\\psi\\rangle + \\beta^*\\langle\\eta|\\psi\\rangle \\\\ 4.\\, &\\langle\\psi|\\psi\\rangle \\ge 0 \\textrm{ sendo igual s\u00f3 se } |\\psi \\rangle = 0 \\end{array} Se \\langle\\psi|\\Phi\\rangle=0 \\langle\\psi|\\Phi\\rangle=0 , os vetores s\u00e3o ortogonais. Os comprimentos (normas) dos vetores s\u00e3o expressos por: Norma do vetor : ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}. Vetor normalizado quando: ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. ||\\psi|| = \\sqrt{\\langle\\psi|\\psi\\rangle}=1. Vetores ortonormais : \\langle u_j | u_k \\rangle = \\delta_{jk} \\langle u_j | u_k \\rangle = \\delta_{jk} \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right. \\delta_{jk} = \\left\\{ \\begin{array}{c} 1 & \\textrm{ se }j=k \\\\ 0 & \\textrm{ caso contr\u00e1rio}\\end{array} \\right.","title":"Propriedades do produto interno"},{"location":"Aulas_S5-S6/#55-vetores-de-base","text":"O conjunto de vetore \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} formam uma base do espa\u00e7o se eles satisfazem os seguinte crit\u00e9rios: \u00c9 poss\u00edvel escrever qualquer vetor do espa\u00e7o como uma combina\u00e7\u00e3o linear \u00fanica dos vetores \\{ \\phi_i \\} \\{ \\phi_i \\} . O conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 linearmente indenpendente. Satisfaz a rela\u00e7\u00e3o de completeza. Condi\u00e7\u00e3o 1: Se o conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} estende todo o espa\u00e7o \\mathcal{H} \\mathcal{H} , \u00e9 poss\u00edvel escrever um vetor |\\Psi\\rangle |\\Psi\\rangle arbitr\u00e1rio como uma combin\u00e7\u00e3o linear dos vetores da base |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle |\\Psi\\rangle = c_1 |\\phi_1\\rangle + c_2 |\\phi_2\\rangle + \\dots + c_n |\\phi_n\\rangle = \\sum_{i=1}^n c_i |\\phi_i\\rangle onde os coeficientes da expans\u00e3o s\u00e3o n\u00fameros complexos dados por c_i = \\langle \\phi_i | \\Psi \\rangle. c_i = \\langle \\phi_i | \\Psi \\rangle. Condi\u00e7\u00e3o 2: A conjunto \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \u00e9 dito linearmente independente quando a equa\u00e7\u00e3o a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 a_1 |\\phi_1\\rangle + a_2 |\\phi_2\\rangle + \\dots + a_n |\\phi_n\\rangle = 0 implica que todos os coeficientes s\u00e3o nulos, c_1=c_2=...=c_n=0 c_1=c_2=...=c_n=0 . Em outras palavras, n\u00e3o h\u00e1 nenhuma combina\u00e7\u00e3o (n\u00e3o trivial) que produza o vetor nulo. Dimens\u00e3o do espa\u00e7o O n\u00famero de vetores da base fornece a dimens\u00e3o do espa\u00e7o vetorial. Condi\u00e7\u00e3o 3: Um conjunto ortonormal \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} \\{ |\\phi_1\\rangle, |\\phi_2\\rangle, \\dots,|\\phi_n\\rangle \\} constitue uma base se e somente se satisfaz a rela\u00e7\u00e3o de completeza \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1 \\sum_{i=1}^n |\\phi_i\\rangle \\langle \\phi_i| = 1","title":"5.5 Vetores de base"},{"location":"Aulas_S5-S6/#procedimento-de-gram-schmidt","text":"Se tivermos um conjunto de vetores \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} que n\u00e3o \u00e9 ortonormal, \u00e9 poss\u00edvel usar o procedimento de Gram-Schmidt para construir uma base ortonormal a partir desse conjunto inicial. Para simplificar o entendimento do processo, consideramos um exemplo com 3 vetores de base (num espa\u00e7o de dimens\u00e7\u00e3o 3). Come\u00e7amos selecionando um dos vetores do conjunto \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} e definindo o vetor: |w_1\\rangle = |u_1\\rangle |w_1\\rangle = |u_1\\rangle A partir disso, constroi-se sucessivamente os vetores seguintes da base subtraindo deles as componentes nas dire\u00e7\u00f5es ortonais \u00e0quelas j\u00e1 constru\u00eddas. Neste caso, por exemplo, as dire\u00e7\u00f5es |w_2\\rangle |w_2\\rangle e |w_3\\rangle |w_3\\rangle s\u00e3o constru\u00eddas subtraindo as componente na dire\u00e7\u00e3o de |w_1\\rangle |w_1\\rangle e |w_2\\rangle |w_2\\rangle , conforme: \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} \\begin{array}{c} &&|w_2 \\rangle =& |u_2\\rangle - \\frac{\\langle w_1 | u_2 \\rangle}{\\langle w_1 | w_1 \\rangle} |w_1\\rangle \\\\ \\\\ &&|w_3\\rangle =& |u_3\\rangle - \\frac{\\langle w_1 | u_3 \\rangle }{\\langle w_1 | w_1 \\rangle} |w_1\\rangle - \\frac{\\langle w_2 | u_3 \\rangle }{\\langle w_2 | w_2 \\rangle} |w_2\\rangle \\end{array} Finalmente, para obter um conjunto ortonormal \\{ |v_i\\rangle \\} \\{ |v_i\\rangle \\} , n\u00f3s podemos normalizar cada um dos vetores |w_i\\rangle |w_i\\rangle : |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} |v_1\\rangle = \\frac{ |w_1 \\rangle }{||\\langle w_1 | w_1 \\rangle||}; \\, |v_2\\rangle = \\frac{ |w_2 \\rangle }{||\\langle w_2 | w_2 \\rangle}||; \\, |v_3\\rangle = \\frac{ |w_3 \\rangle }{||\\langle w_3 | w_3 \\rangle||} De forma geral, para um cojunto finito de vetores \\{u_k\\} \\{u_k\\} , de um espa\u00e7o vetorial \\mathcal{U} \\mathcal{U} de dimens\u00e3o d d , pode-se escrever os vetores ortonormais \\{v_k\\} \\{v_k\\} atrav\u00e9s da construindo: \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}. \\left|v_{k+1}\\right\\rangle \\equiv \\frac{\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle}{\\|\\left|w_{k+1}\\right\\rangle-\\sum_{i=1}^{k}\\left\\langle v_{i} | w_{k+1}\\right\\rangle\\left|v_{i}\\right\\rangle \\|}.","title":"Procedimento de Gram-Schmidt"},{"location":"Aulas_S5-S6/#algebra-de-dirac","text":"Vejamos como expressar vetores inteiramente em termos do kets da base e manipular bras e kets de forma alg\u00e9brica. Representando um ket como bra Para obter o bra correspondente a um dado ket , | \\phi\\rangle = \\alpha |\\psi\\rangle | \\phi\\rangle = \\alpha |\\psi\\rangle , basta tomar o complexo conjugado: \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| \\langle \\phi| = (\\alpha |\\psi\\rangle)^* = \\alpha \\langle \\psi| podemos tamb\u00e9m escrever |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle |\\alpha \\psi\\rangle = \\alpha |\\psi\\rangle . O mesmo pode ser feito para o bra , mas deve-se tomar um cuidado extra, neste caso: \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| \\langle \\alpha \\psi| = \\alpha^* \\langle \\psi| Exerc\u00edcio sugerido Suponha que \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} \\{ |u_1 \\rangle, |u_2 \\rangle, |u_3 \\rangle \\} seja uma base ortonormal. Nesta base temos: | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\psi \\rangle = 2i |u_1 \\rangle - 3|u_2 \\rangle + i|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle | \\phi\\rangle = 3 |u_1 \\rangle - 2|u_2 \\rangle + 4|u_3 \\rangle a) Ache \\langle\\psi| \\langle\\psi| e \\langle \\phi| \\langle \\phi| . b) Calcule o produto interno \\langle \\phi|\\psi\\rangle \\langle \\phi|\\psi\\rangle e mostre que igual seu conjugado. c) Sendo a = 3 + 3i a = 3 + 3i , calcule |a\\psi\\rangle |a\\psi\\rangle . d) Ache as express\u00f5es de |\\psi+\\phi\\rangle |\\psi+\\phi\\rangle e |\\psi-\\phi\\rangle |\\psi-\\phi\\rangle e) Calcule \\langle a \\psi | \\langle a \\psi | e compare com a^* \\langle \\psi| a^* \\langle \\psi| . f) Normalize o vetor | \\psi \\rangle | \\psi \\rangle . Encontrando os coeficientes da expans\u00e3o Da mesma forma que fazemos os vetores do espa\u00e7o Euclidiano, para encontrar as componentes de um vetor no espa\u00e7o de Hilber basta fazer o produto escalar (interno) do vetor com o correspondente verto da base. Em nota\u00e7\u00e3o de Dirac, se o vetor \u00e9 dado por $$ |\\psi\\rangle=c_{1}\\left|u_{1}\\right\\rangle+c_{2}\\left|u_{2}\\right\\rangle+\\cdots+c_{n}\\left|u_{n}\\right\\rangle=\\sum_{i=1}^{n} c_{i}\\left|u_{i}\\right\\rangle $$ os coeficientes s\u00e3o dados por $$ c_i = \\left\\langle u_i | \\psi \\right\\rangle $$ que podem ser convenientemente escritos na forma | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) | \\psi \\rangle \\rightarrow\\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\vdots \\\\ \\left\\langle u_{n} | \\psi\\right\\rangle \\end{array} \\right) = \\left(\\begin{array}{c} c_{1} \\\\ c_{2} \\\\ \\vdots \\\\ c_{n} \\end{array}\\right) Note, por\u00e9m, que um vetor pode ser escrito em termos de diversas bases diferentes (o vetor tem exist\u00eancia indepentende da base) e em cada uma delas os valores das componentes ser\u00e3o diferentes. Exemplo Considere o vetor abaixo, expresso em termos de uma base ortonormal: $$ |\\psi\\rangle=2 i\\left|u_{1}\\right\\rangle-3\\left|u_{2}\\right\\rangle+i\\left|u_{3}\\right\\rangle$$ Neste caso, o velor coluna dos coeficientes representando |\\psi\\rangle |\\psi\\rangle \u00e9 dado por |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). |\\psi\\rangle = \\left( \\begin{array}{c} \\left\\langle u_{1} | \\psi\\right\\rangle \\\\ \\left\\langle u_{2} | \\psi\\right\\rangle \\\\ \\left\\langle u_{3} |\\psi\\right\\rangle \\end{array}\\right) = \\left( \\begin{array}{c} 2 i \\\\ -3 \\\\ i \\end{array} \\right). Da mesma forma, o vetor dual (\" bra \") correspondente ao vetor |\\psi\\rangle |\\psi\\rangle pode ser representado na forma de um vetor linha \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. \\left\\langle\\psi\\left|=\\left(\\left\\langle\\psi | u_{1}\\right\\rangle\\left\\langle\\psi | u_{2}\\right\\rangle\\left\\langle\\psi | u_{3}\\right\\rangle\\right)=\\left(\\left\\langle u_{1} | \\psi\\right\\rangle^{*}\\left\\langle u_{2} | \\psi\\right\\rangle^{*}\\left\\langle u_{3} | \\psi\\right\\rangle^{*}\\right)\\right.\\right. e portanto \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i). \\langle\\psi|=\\left((2 i)^{*}(-3)^{*}(i)^{*}\\right)=(-2 i-3-i).","title":"Algebra de Dirac"},{"location":"Aulas_S5-S6/#56-operadores-lineares","text":"Grandezas f\u00edsicas observ\u00e1veis, que podem ser medidas no laborat\u00f3rio, como posi\u00e7\u00e3o e momento, s\u00e3o representandos dentro da estrutura matem\u00e1tica da mec\u00e2nica qu\u00e2ntica por operadores lineares num espa\u00e7o vetorial de Hilbert. Matematicamente, esses operadores s\u00e3o mapas que levam (transformam) um vetor em outro vetor. Isto \u00e9, s\u00e3o receitas ou regras de transforma\u00e7\u00e3o de um dado vetor num novo vetor, geralmente diferente do primeiro. Frequentemente usa-se como s\u00edmbolo uma letra ma\u00edscula com \"chapel\" (sinal circunflexo) sobre a letra para indicar um operador. Assim, na nota\u00e7\u00e3o de Dirac, escreve-se, por exemplo: $$ \\hat{T}|\\psi\\rangle=|\\phi \\rangle. $$ Os operadores que mais nos interessam na MQ s\u00e3o os operadores lineares. Um operador \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \\hat{T}:\\mathcal{H}\\rightarrow\\mathcal{H} \u00e9 linear no espa\u00e7o \\mathcal{H} \\mathcal{H} se, dados escalares \\alpha, \\beta \\in \\mathbb{C} \\alpha, \\beta \\in \\mathbb{C} e vetores |u\\rangle, |v\\rangle \\in \\mathcal{H} |u\\rangle, |v\\rangle \\in \\mathcal{H} , ele satisfaz a rela\u00e7\u00e3o: $$ \\hat{T}(\\alpha|u\\rangle+\\beta|v\\rangle)=\\alpha\\, \\hat{T}|u\\rangle+\\beta\\, \\hat{T}|v\\rangle. $$ Al\u00e9m disso, os operadores lineare tamb\u00e9m satisfazem as seguintes rela\u00e7\u00f5es: (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}+\\hat{S})\\ket{u}=\\hat{T}\\ket{u} + \\hat{S}\\ket{u} (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) (\\hat{T}\\,\\hat{S})\\ket{u}=\\hat{T}(\\hat{S}\\ket{u}) Operadores atuam tanto nos vetores kets como nos vetores duais bras , seguindo a seguinte nota\u00e7\u00e3o (aten\u00e7\u00e3o para a ordem!): $$ \\hat{T}\\ket{u} \\quad \\text{ ou } \\quad \\bra{u} \\hat{T} $$ mas nunca (\\,\\ket{u} \\hat{T}\\,) (\\,\\ket{u} \\hat{T}\\,) ou (\\,\\hat{T} \\bra{u}\\,) (\\,\\hat{T} \\bra{u}\\,) , que s\u00e3o formas incorretas (inv\u00e1lidas)!","title":"5.6 Operadores lineares"},{"location":"Aulas_S5-S6/#exemplos-importantes","text":"Operador Identidade: o operador mais simples $$ \\mathbb{1}\\ket{u}=\\ket{u} $$ Produto externo (defini\u00e7\u00e3o): o produto externo entre kets e bras \u00e9 dado por $$ \\ket{\\psi}\\bra{\\phi} = \\hat{P} $$ note que o produto externo resulta num operador e n\u00e3o num escalar! Essa constru\u00e7\u00e3o ser\u00e1 muito \u00fatil, como veremos adiante. Operador projetor: usando o produto externo, podemos calcular as proje\u00e7\u00f5es de um dado vetor numa certa dire\u00e7\u00e3o \\ket{u_i} \\ket{u_i} , ou numa base \\{ u_i \\} \\{ u_i \\} , fazendo \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} \\begin{array}{ll} \\hat{P}_{u_i} = | u_i \\rangle \\langle u_i | &\\rightarrow \\quad \\hat{P}_{u_i} | \\chi\\rangle = | u_i \\rangle (\\langle u_i |\\chi\\rangle) = \\beta | u_i \\rangle \\\\ \\hat{P}_{u} = \\sum_i | u_i \\rangle \\langle u_i | &\\rightarrow \\quad {P}_{u} | \\chi\\rangle = \\sum_i \\,c_i | u_i \\rangle = | \\chi\\rangle \\end{array} Rela\u00e7\u00e3o de completeza: usando os resultados anteriores podemos observar que |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle |\\psi\\rangle=\\sum_{i=1}^{n}c_i\\left|u_{i}\\right\\rangle = \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i} | \\psi\\right\\rangle=\\left(\\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right|\\right)|\\psi\\rangle \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1} \\sum_{i=1}^{n}\\left|u_{i}\\right\\rangle\\left\\langle u_{i}\\right| = \\mathbb{1}","title":"Exemplos importantes"},{"location":"Aulas_S5-S6/#representacao-de-operadores","text":"A opera\u00e7\u00e3o matem\u00e1tica de transformar um vetor de um espa\u00e7o vetorial linear num outro vetor, atrav\u00e9s da a\u00e7\u00e3o de um operador linear, pode ser representada de v\u00e1rias formas. Uma delas \u00e9 a representa\u00e7\u00e3o matricial, onde os operadores s\u00e3o representados por matrizes quadradas e os vetores por matrizes linhas e colunas. Neste caso, a transforma\u00e7\u00e3o linear torna-se uma mera multiplica\u00e7\u00e3o dessas matrizes. \u00c9 importante lembrar que, da mesma forma que os vetores do espa\u00e7o, os operadores t\u00eam exist\u00eancia e significado pr\u00f3prios no espa\u00e7o vetorial e sua a\u00e7\u00e3o independe da representa\u00e7\u00e3o ou da base escolhida. Por outro lado, sua representa\u00e7\u00e3o matricial, em geral, depende da base escolhida. Devemos lembrar, por\u00e9m, que a forma matricial \u00e9 apenas uma das representa\u00e7\u00f5es poss\u00edveis de um operador linear. Representa\u00e7\u00e3o matricial A matriz de um operador numa dada base pode ser obtida a partir da a\u00e7\u00e3o do operador em cada vetor da base. Assim, se \\{ u_i \\} \\{ u_i \\} representa o conjunto de vetores da base, as componentes do operador \\hat{T} \\hat{T} podem ser obtidas atrav\u00e9s da opera\u00e7\u00e3o T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. T_{i j}=\\left\\langle u_{i}|\\hat{T}| u_{j}\\right\\rangle. Em um espa\u00e7o vetorial de dimens\u00e3o n, as componentes do operador podem ser arranjadas na forma de uma matriz quadrada n \\times n n \\times n , onde T_{i j} T_{i j} representa o elemento na linha i i e coluna j j , conforme: \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} \\begin{aligned} \\hat{T} \\rightarrow\\left(T_{i j}\\right) &=\\left(\\begin{array}{cccc} T_{11} & T_{12} & \\dots & T_{1 n} \\\\ T_{21} & T_{22} & \\dots & T_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ T_{n 1} & T_{n 2} & \\dots & T_{n n} \\end{array}\\right) \\\\ &=\\left(\\begin{array}{cccc} \\left\\langle u_{1}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{1}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\left\\langle u_{2} \\hat{T} | u_{1}\\right\\rangle & \\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{2}|\\hat{T}| u_{n}\\right\\rangle \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\left\\langle u_{n}|\\hat{T}| u_{1}\\right\\rangle & \\left\\langle u_{n}|\\hat{T}| u_{2}\\right\\rangle & \\dots & \\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle \\end{array}\\right) \\end{aligned} Exerc\u00edcio sugerido Suponha uma base ortonormal \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} \\left\\{\\left|u_{1}\\right\\rangle,\\left|u_{2}\\right\\rangle,\\left|u_{3}\\right\\rangle\\right\\} , um operador \\hat{A} \\hat{A} cuja a a\u00e7\u00e3o \u00e9 dada por: $$ \\begin{array}{l} \\hat{A}\\left|u_{1}\\right\\rangle=2\\left|u_{1}\\right\\rangle; \\ \\hat{A}\\left|u_{2}\\right\\rangle=3\\left|u_{1}\\right\\rangle-i\\left|u_{3}\\right\\rangle; \\ \\hat{A}\\left|u_{3}\\right\\rangle=-\\left|u_{2}\\right\\rangle \\end{array} $$ Escreve a matriz que representa o operador nesta base. Defini\u00e7\u00e3o : Tra\u00e7o de um operador O tra\u00e7o de um operador \\hat{T} \\hat{T} , denotado por \\text{Tr}(\\hat{T}) \\text{Tr}(\\hat{T}) , \u00e9 definido como sendo a soma dos elementos na diagonal principal da matriz que o representa \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. \\text{Tr}(\\hat{T})=T_{11}+T_{22}+\\ldots+T_{n n}=\\sum_{i=1}^{n} T_{i i}. Alternativamente, o tra\u00e7o tamb\u00e9m pode ser escrito como: \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle \\text{Tr}(\\hat{T})=\\left\\langle u_{1}|\\hat{T}| u_{2}\\right\\rangle+\\left\\langle u_{2}|\\hat{T}| u_{2}\\right\\rangle+\\ldots+\\left\\langle u_{n}|\\hat{T}| u_{n}\\right\\rangle=\\sum_{i=1}^{n}\\left\\langle u_{i}|\\hat{T}| u_{i}\\right\\rangle Exerc\u00edcio sugerido O tra\u00e7o de um operador obedece uma rela\u00e7\u00e3o c\u00edclica, como indicado $$ \\operatorname{Tr}(A B C)=\\operatorname{Tr}(B C A)=\\operatorname{Tr}(C A B) $$ Prove isso para o caso de dois operadores A A e B B , i.e. prove que \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A) \\operatorname{Tr}(A B)=\\operatorname{Tr}(B A)","title":"Representa\u00e7\u00e3o de operadores"},{"location":"Aulas_S5-S6/#valores-esperados","text":"O valore esperado de um operador com rela\u00e7\u00e3o a um estado \\Psi \\Psi \u00e9 dado por \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle \\langle\\hat{A}\\rangle=\\langle\\Psi|\\hat{A}| \\Psi\\rangle Exerc\u00edcio sugerido Considere uma part\u00edcula no estado $$ |\\Psi\\rangle=2 i\\left|u_{1}\\right\\rangle-\\left|u_{2}\\right\\rangle+4 i\\left|u_{3}\\right\\rangle $$ e um operador $$ \\hat{A}=\\left|u_{1}\\right\\rangle\\left\\langle u_{1}| -2 i| u_{1}\\right\\rangle\\left\\langle u_{2}|+| u_{3}\\right\\rangle\\left\\langle u_{3}\\right| $$ Considerando que \\{ |u_i\\rangle \\} \\{ |u_i\\rangle \\} \u00e9 uma base ortonormal, calcule \\langle \\hat{A} \\rangle \\langle \\hat{A} \\rangle nesse estado.","title":"Valores esperados"},{"location":"Aulas_S5-S6/#autovalores-e-autovetores","text":"Quando um operador age sobre um dado vetor (estado) e o resultado \u00e9 o mesmo vetor (estado) multiplicado por um escalar, o vetor \u00e9 chamado de autovetor (autoestado) e o escalar de autovalor. Assim, por exemplo, no caso da energia total $$ \\hat{H}|\\psi_n\\rangle = E_n |\\psi_n \\rangle $$ No contexto da mec\u00e2nica qu\u00e2ntica, operadores de observ\u00e1veis f\u00edsicos t\u00eam como autovalores o conjunto de todas as poss\u00edveis medidas daquela grandeza f\u00edsica, num dado sistema qu\u00e2ntico. Os autovetores de um operador s\u00e3o autoestados do sistema qu\u00e2ntico e s\u00e3o muito importantes, pois esses autovetores formam uma base do espa\u00e7o e permitem represetar qualquer estado do sistema. A seguir temos uma breve revis\u00e3o de como calcular autovalores e autovetores, a partir de conceitos e m\u00e9todos de Algebra Linear. C\u00e1lculo dos autovalores Dado um operador linear \\hat{T} \\hat{T} , como j\u00e1 vimos, pode-se sempre represent\u00e1-lo por uma matriz T T . O conjunto de autovalores \\lambda \\lambda dessa matriz podem ser determinados atrav\u00e9s da equa\u00e7\u00e3o caracter\u00edstica (tamb\u00e9m chamada de equa\u00e7\u00e3o secular ), para o determinante abaixo: $$ \\operatorname{det}(T-\\lambda I)=0$$ onde I=\\mathbb{1} I=\\mathbb{1} \u00e9 a matriz identidade. A solu\u00e7\u00e3o da equa\u00e7\u00e3o caracter\u00edstica fornece os autovalores \\lambda \\lambda , que s\u00e3o as raizes do polin\u00f4nimo ( caracter\u00edstico ), indicado acima. Exerc\u00edcio sugerido Escreva o equa\u00e7\u00e3o caracter\u00edstica e ache os autovalores da matriz A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} A = \\begin{pmatrix} 7i & -1 \\\\ 2 & -6i \\end{pmatrix} C\u00e1lculo dos autovetores A partir dos autovalores pode-se determinar os autovetores da matriz T T , que pode ser ent\u00e3o escrita na forma diagonal. Para ilustrar melhor isso, usaremos um exemplo, a partir do problema proposto a seguir. Exerc\u00edcio sugerido Considere o operador \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} \\hat{T}=\\ket{\\phi_1}\\bra{\\phi_1}+2\\ket{\\phi_1}\\bra{\\phi_2}+\\ket{\\phi_2}\\bra{\\phi_1} , expresso numa base ortonormal. Ache a matriz T T , que representa o operador nesta base, e determine os autovetores normalizados do operador, com seus autovalores. Considere que o espa\u00e7o \u00e9 bidimensional. Antes de seguir, voc\u00ea deve resolver o problema proposto acima, em detalhe, pelo menos at\u00e9 onde puder, para ter certeza de que est\u00e1 entendo todos os passos necess\u00e1rios \u00e0 resolu\u00e7\u00e3o do problema. Ao fazer isso ir\u00e1 encontrar os valores que usaremos na resolu\u00e7\u00e3o que exemplificada o c\u00e1lculo de um dos autovetores, a seguir Exemplo: resolu\u00e7\u00e3o dos autovetores Os autovalores do problema anterio s\u00e3o \\lambda_1=2 \\lambda_1=2 e \\lambda_2=-1 \\lambda_2=-1 . Substitui-se, ent\u00e3o, esses valores, um de cada vez, na equa\u00e7\u00e3o de autovalores \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} \\hat{T}\\ket{u_i}=\\lambda_i \\ket{u_i} para determinar os autovetores \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} , como \u00e9 mostrado abaixo para \\ket{u_2} \\ket{u_2} . \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = - \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. \\Rightarrow a + 2b = -a, \\text{ ou } b = -a. portanto, \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. \\ket{u_2}= \\begin{pmatrix} a \\\\ -a \\end{pmatrix}. Normalizando o vetor temos: \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, \\bra{u_2} u_2 \\rangle = 1 \\rightarrow 2a^2 = 1 \\Rightarrow a=\\frac{1}{\\sqrt{2}}, portanto, finalmente, temos: \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}. Verifique agora que \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.","title":"Autovalores e autovetores"},{"location":"Aulas_S5-S6/#conjugacao-hermitiana","text":"At\u00e9 agora vimos que um operador age num ket para produzir um novo ket , de acordo com \\hat{T} \\ket{u} = \\ket{v}. \\hat{T} \\ket{u} = \\ket{v}. Vejamos agora, mais atentamente, sua a\u00e7\u00e3o dentro de um produto interno \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} \\bra{w}v\\rangle = \\braket{w}{\\hat{T}}{u} . Sabemos que isso resulta num escalar (n\u00famero) complexo. Podemos tomar complexo conjugado desse n\u00famero, usando a rela\u00e7\u00e3o \\bra{w}v\\rangle = \\bra{v}w\\rangle^* \\bra{w}v\\rangle = \\bra{v}w\\rangle^* . Observe atentamente o que ocorre com o operador \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} \\braket{w}{\\hat{T}}{v} = \\braket{v}{\\hat{T}}{w}^* =\\braket{w}{\\hat{T^{\\dagger}}}{v} onde \\hat{T^{\\dagger}} \\hat{T^{\\dagger}} (pronuncia-se \"T dagger \") \u00e9 chamado de conjugado Hermitiano, ou Hermitiado conjugado, ou ainda adjunto do operador \\hat{T} \\hat{T} . Como formar o Adjunto de uma express\u00e3o geral? Substitua qualquer constante por seu complexo conjugado. Substitua kets pelos bras associados, e vice-versa. Substitua cada operador por seu Adjunto. Inverta a ordem de todos os fatores na express\u00e3o. O conjugado Hermitiado de uma matriz J\u00e1 sabemos como encontrar a matriz M M de um operador \\hat{M} \\hat{M} qualquer. Para encontrar a matriz do Adjunto desse, simbolizada por M^{\\dagger} M^{\\dagger} , basta seguir os seguintes passos: Matriz Adjunta Calcule a matriz transposta M^T M^T , trocando as linhas pelas colunas. Tome o complexo conjugado de cada elemento de M^T. M^T. De forma resumida: M^{\\dagger}= \\left( M^T \\right)^*. M^{\\dagger}= \\left( M^T \\right)^*. Propriedade da opera\u00e7\u00e3o de transposi\u00e7\u00e3o (A+B)^T = A^T + B^T. (A+B)^T = A^T + B^T. (A^T)^T = A. (A^T)^T = A. (aA)^T= a A^T. (aA)^T= a A^T. (AB)^T = B^T A^T. (AB)^T = B^T A^T.","title":"Conjuga\u00e7\u00e3o Hermitiana"},{"location":"Aulas_S5-S6/#operadores-hermitianos","text":"Um operador \u00e9 dito Hermitiano quando \u00e9 auto-adjunto: \\hat{T}^{\\dagger}=\\hat{T} \\hat{T}^{\\dagger}=\\hat{T} . Isto \u00e9, quando o seu adjunto \u00e9 ele pr\u00f3prio. Para um operador Hemitiano, temos que \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* \\braket{w}{\\hat{T}}{v}=\\braket{v}{\\hat{T}}{w}^* Veremos que os operadores de observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica devem ser sempre operadores Hermitianos. Como esse operadores podem ser representado por matrizes, \u00e9 interessante ver com determinar se uma matriz \u00e9 Hemitiana. Matriz Hermitiana Uma matriz M M \u00e9 Hermitiana se satisfaz: M = M^{\\dagger}. M = M^{\\dagger}. Como vimos, M^{\\dagger} M^{\\dagger} corresponde ao complexo conjugado da matriz transposta. Portanto, para satisfazer essa condi\u00e7\u00e3o, os elementos da diagonal principal da matriz devem ser todos n\u00fameros reais (n\u00e3o complexos). Como consequ\u00eancia, o tra\u00e7o do operador (matriz) ser\u00e1, necessariamente um n\u00famero real. Autovalores de um operador Hermitiano Pode-se demonstrar que operadores Hermitianos tem autovalores reais (verifique!). Por conta dessa propriedade, requer-se que todos os observ\u00e1veis f\u00edsicos na mec\u00e2nica qu\u00e2ntica, sejam representados por operadores Hermitianos. Portanto, tanto o tra\u00e7o como os autovalores de um operador Hermitiano s\u00e3o n\u00fameros reais. Autovetores de um operador Hermitiano Outra propriedade importante dos operadores (matrizes) hermetianos \u00e9 que os autovalores correspondentes a autovalores diferentes s\u00e3o ortogonais. Tamb\u00e9m n\u00e3o \u00e9 dif\u00edcil demonstrar essa propriedade ( verifique! ), usando a propriedade anterior, que os autovalores s\u00e3o n\u00fameros reais. Como os autovetores s\u00e3o ortogonais, \u00e9 poss\u00edvel, portanto, construir uma base (ortonormal) que gere o espa\u00e7o. Propriedades de operadores hermitianos Autovalores reais Autovetores ortogonais, para autovalores diferentes Autovetores geram o espa\u00e7o (podem formar uma base) Operador anti-Hermitiano Um operador \\hat{A} \\hat{A} \u00e9 dito anti-Hermitiano se: A^{\\dagger}=-A A^{\\dagger}=-A Verifique que, neste caso, os elementos da diagonal principal do operador (matriz) anti-Hermitiano(a) s\u00e3o todos n\u00fameros imagin\u00e1rios puros.","title":"Operadores Hermitianos"},{"location":"Aulas_S5-S6/#operadores-normais","text":"Um operador A A \u00e9 dito ser normal se AA^{\\dagger} = A^{\\dagger}A. AA^{\\dagger} = A^{\\dagger}A. Claramente, um operador Hermitiano tamb\u00e9m \u00e9 um operador normal. Operadores normais s\u00e3o interessantes pois h\u00e1 um teorema importante relacionado \u00e0 sua representa\u00e7\u00e3o que garante que eles podem ser escritos numa forma chamada de decomposi\u00e7\u00e3o espectral , e que ser\u00e1 bastante \u00fatil mais adiante. Voltaremos a falar deles quando discutirmos o processo de diagonaliza\u00e7\u00e3o de um operador.","title":"Operadores Normais"},{"location":"Aulas_S5-S6/#operadores-unitarios","text":"Um operador \\hat{U} \\hat{U} (de matriz U U ) \u00e9 unit\u00e1rio se: UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} UU^{\\dagger} = U^{\\dagger} U = \\mathbb{1} Isso significa que U^{\\dagger} = U^{-1} U^{\\dagger} = U^{-1} ou seja, que a matriz adjunta \u00e9 igual a matriz inversa. Outra importante caracter\u00edstica das matrizes unit\u00e1rias \u00e9 que as linhas e colunas dessa matrizes formam um conjunto de vetores ortonormais. Pode-se perceber ainda que operadores unit\u00e1rios tamb\u00e9m s\u00e3o operadores normais e, portanto, podem tem uma decomposi\u00e7\u00e3o spectral, como veremos depois. Finalmente, outra caracter\u00edstica importante desses operadores \u00e9 que geometricamente eles preservam o produto interno entre vetores, com pode ser facilmente verificado ( U\\ket{v},U\\ket{w}) = \\bra{v}U^{\\dagger}U\\ket{w}=\\braket{v}{\\mathbb{1}}{w}=\\bra{v}w\\rangle. ( U\\ket{v},U\\ket{w}) = \\bra{v}U^{\\dagger}U\\ket{w}=\\braket{v}{\\mathbb{1}}{w}=\\bra{v}w\\rangle.","title":"Operadores Unit\u00e1rios"},{"location":"Aulas_S5-S6/#comutadores-e-anticomutadores","text":"Seja \\hat{A} \\hat{A} e \\hat{B} \\hat{B} dois operadores lineares do espa\u00e7o. Em geral, temos que \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. \\hat{A}\\hat{B} \\ne \\hat{B}\\hat{A}. Assim, define-se o comutador [\\hat{A},\\hat{B}] [\\hat{A},\\hat{B}] como sendo [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. [\\hat{A},\\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. Se [\\hat{A},\\hat{B}]=0 [\\hat{A},\\hat{B}]=0 , dizemos que os operadores comutam. Dois operadores comutam se, e apenas se, eles compartilham uma mesma base de autovetores comuns. Propriedades do comutador [A,B]=-[B,A] [A,B]=-[B,A] [A+B,C]=[A,C]+[B,C] [A+B,C]=[A,C]+[B,C] [A,BC]=[A,B]C+B[A,C] [A,BC]=[A,B]C+B[A,C] Se \\hat{X} \\hat{X} e \\hat{P} \\hat{P} representam os operadores posi\u00e7\u00e3o e momento linear, ent\u00e3o [\\hat{X} [\\hat{X} , \\hat{P}]=i\\hbar \\hat{P}]=i\\hbar , enquanto [\\hat{X} [\\hat{X} , \\hat{X}]= [\\hat{P} \\hat{X}]= [\\hat{P} , \\hat{P}]=0. \\hat{P}]=0. Anticomutador Define-se o anticomutador \\{\\hat{A},\\hat{B}\\} \\{\\hat{A},\\hat{B}\\} como sendo \\{ \\hat{A},\\hat{B} \\} = \\hat{A}\\hat{B} + \\hat{B}\\hat{A}. \\{ \\hat{A},\\hat{B} \\} = \\hat{A}\\hat{B} + \\hat{B}\\hat{A}.","title":"Comutadores e anticomutadores"},{"location":"Aulas_S5-S6/#conjunto-completo-de-observaveis-que-comutam-ccoc","text":"Um conjunto de operadores \\hat{A} \\hat{A} , \\hat{B} \\hat{B} , \\hat{C}, \\dots \\hat{C}, \\dots forma um CCCO se todos os subpares desses operadores comutam entre si. [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 [\\hat{A},\\hat{B}] = [\\hat{B},\\hat{C}] = [\\hat{A},\\hat{C}] = \\dots= 0 Isso implica que existe uma base comum de autovetores que \u00e9 \u00fanica para todos eles, exceto por um fator mutiplicativo. Para avan\u00e7ar para os t\u00f3picos das pr\u00f3ximas aulas, use o menu de navega\u00e7\u00e3o ou clique aqui .","title":"Conjunto Completo de Observ\u00e1veis que Comutam (CCOC)"},{"location":"Aulas_S7/","text":"\\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\bra}[1]{\\left\\langle #1 \\right|} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\ket}[1]{\\left| #1 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} \\newcommand{\\braket}[3]{\\left\\langle #1 \\middle| #2 \\middle| #3 \\right\\rangle} 5. Estrutura matem\u00e1tica da MQ (parte 2) Nesta semana continuaremos a discutir a estrutura matem\u00e1tica da teoria qu\u00e2ntica, complementando a discuss\u00e3o a respeito de transforma\u00e7\u00f5es lineares e, em particular, mudan\u00e7as de bases e o processo de diagonaliza\u00e7\u00e3o de um operador (matriz). Tamb\u00e9m iremos apresentar alguns casos importantes como os operadores projetores em subespa\u00e7os e o produto tensorial que permite expandir espa\u00e7os de Hilbert. Para retornar aos t\u00f3picos das aulas anteriores, use o menu de navega\u00e7\u00e3o ou clique aqui . 5.7 Transforma\u00e7\u00f5es lineares e mudan\u00e7as de base Na aula anterior n\u00f3s discutimos como encontrar os autovetores de um operador. Surgiu, ent\u00e3o, a quest\u00e3o se os autovetores do operador deveriam ser sempre ortogonais e poderiam formar uma base. Aqui, vamos iniciar esta discuss\u00e3o com a quest\u00e3o, geral, se os autovetores de um operador podem sempre formam uma base do espa\u00e7o vetorial. Nos casos onde isso \u00e9 poss\u00edvel, como constru\u00ed-la? Primeiro, vamos relembrar que condi\u00e7\u00f5es um conjunto deve satisfazer para formar uma base. De maneira simples, para formar uma base do espa\u00e7o (ou subespa\u00e7o) um conjunto de autovetores devem satisfazer duas condi\u00e7\u00f5es: Condi\u00e7\u00f5es para formar uma base Ser ortonormais. Satisfazer a rela\u00e7\u00e3o de completeza. Para ajudar a fixar esses conceitos, considere o seguinte exemplo. Exemplo Considere o operador dado pela matriz Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} Observe que seus autovalores s\u00e3o \\lambda_{1,2}=\\pm 1 \\lambda_{1,2}=\\pm 1 . Tendo como autovetores: \\ket{u_1}= \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\ket{u_2}= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \\ket{u_1}= \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\ket{u_2}= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \u00c9 f\u00e1cil verificar que esse autovetores s\u00e3o ortonomais, portanto satisfazendo a primeira condi\u00e7\u00e3o. Agora, precisamos verificar se a segunda condi\u00e7\u00e3o tamb\u00e9m \u00e9 satisfeita. Ou seja, se o conjunto \u00e9 completo. Para isso, fazemos: \\ket{u_1} \\bra{u_1} + \\ket{u_2} \\bra{u_2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\\\ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\mathbb{1}. \\ket{u_1} \\bra{u_1} + \\ket{u_2} \\bra{u_2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\\\ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\mathbb{1}. Portanto, verificamos que a rela\u00e7\u00e3o de completeza tamb\u00e9m \u00e9 satisfeita. A conclus\u00e3o final \u00e9 que, neste caso, os autovetores formam uma base. De faot, \u00e9 f\u00e1cil verificar que qualquer vetor de dimens\u00e3o dois pode ser escrito numa expans\u00e3o em termos do conjunto \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} . Seja, por exemplo, \\ket{\\psi} \\ket{\\psi} um vertor arbitr\u00e1rio \\ket{\\psi} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha \\ket{u_1} + \\beta \\ket{u_2}. \\ket{\\psi} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha \\ket{u_1} + \\beta \\ket{u_2}. Forma diagonal O operador \\hat{Z} \\hat{Z} \u00e9 um exemplo de operador na forma diagonal. Representa\u00e7\u00e3o diagonal Um operador \\hat{A} \\hat{A} est\u00e1 na forma diagonal quando escrito \\hat{A} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} \\hat{A} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} onde os vetores \\ket{u_i} \\ket{u_i} formam uma base ortonormal. Neste caso, a matriz A A tem a forma A = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} A = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} Um operador \u00e9 dito diagonaliz\u00e1vel quando pode ser escrito na forma diagonal. A representa\u00e7\u00e3o diagonal tamb\u00e9m \u00e9 chamada de decomposi\u00e7\u00e3o ortogonal . Nem todos os operadores de um espa\u00e7o vetorial tem forma diagonal. Na \u00faltima aula, vimos tamb\u00e9m um exemplo em que M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} tem autovetores que n\u00e3o s\u00e3o ortonormais \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\Rightarrow \\quad \\bra{u_1} u_2\\rangle = \\frac{1}{\\sqrt{10}}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\Rightarrow \\quad \\bra{u_1} u_2\\rangle = \\frac{1}{\\sqrt{10}}. Ser\u00e1 que \u00e9 poss\u00edvel reescrever o operador M M numa forma diagonal? Ali\u00e1s, qual seria o significado disso e quais propriedades tal opera\u00e7\u00e3o deveria satisfazer, se existisse? Mais importante ainda, qual seria sua utilidade? Mais adiante veremos quais s\u00e3o as condi\u00e7\u00f5es necess\u00e1rias para um operador ser diagonaliz\u00e1vel. Veremos tamb\u00e9m como fazer para diagonaliz\u00e1-lo. Antes, vamos introduzir mais dois conceitos importantes. Os conceitos de mudan\u00e7a de base e transforma\u00e7\u00f5es de similaridade . Mudan\u00e7a de base Para efeito de compara\u00e7\u00e3o e analogia, podemos pensar na mudan\u00e7a de base como algo parecido \u00e0 mudan\u00e7a de sistema de coordenadas, com o qual j\u00e1 estamos acostumados na F\u00edsica. Como sabemos, a representa\u00e7\u00e3o de um vetor (ou operador), por exemplo, na forma matricial ir\u00e1 depender da base usada para represent\u00e1-lo, pois em geral os valores das compenentes ser\u00e3o diferentes, mas isso n\u00e3o altera o significado de cada um desses objetos. Dado um vetor qualquer \\ket{v} = \\sum_i b_i \\ket{b_i} = \\sum_i c_i \\ket{c_i}, \\ket{v} = \\sum_i b_i \\ket{b_i} = \\sum_i c_i \\ket{c_i}, expresso nas bases \\{ \\ket{b_i} \\} \\{ \\ket{b_i} \\} e \\{ \\ket{c_i} \\} \\{ \\ket{c_i} \\} , \u00e9 sempre poss\u00edvel achar uma transforma\u00e7\u00e3o de coordenadas que permita expressar as coordenadas c_i c_i desse vetor a partir das coordenadas b_i b_i . Para isso, basta encontrar a matriz (operador linear), S S , que leva cada vetor \\ket{b_i} \\ket{b_i} no correspondente vetor \\ket{c_i} \\ket{c_i} : \\ket{c_i} = S \\ket{b_i} \\rightarrow \\ket{b_i} = S^{-1} \\ket{c_i} \\Rightarrow c_i = b_i (S^{-1}). \\ket{c_i} = S \\ket{b_i} \\rightarrow \\ket{b_i} = S^{-1} \\ket{c_i} \\Rightarrow c_i = b_i (S^{-1}). Uma forma de pensar nisso \u00e9 que a transforma\u00e7\u00e3o S S leva os vetores da base \\ket{b_i} \\ket{b_i} nos vetores da base \\ket{c_i} \\ket{c_i} . Neste caso, S S \u00e9 efetivamente um mapa de como fazer essa transforma\u00e7\u00e3o dos vetores da base, e \u00e9 uma receita (mapa) de como \"levar\" cada ponto de um sistema de coordenadas no ponto correspondente no outro sistema de coordenadas. Na verdade, \u00e9 importante lembrar que aqui estamos interessados nas coordenadas (representa\u00e7\u00e3o) do vetor, que est\u00e1 sendo representado nas diferentes bases (sistemas de coordenadas). Figura 1: Efeito de uma tranforma\u00e7\u00e3o linear (S) num grid de pontos e nos versores da base Note, por\u00e9m, que nessa interpreta\u00e7\u00e3o todo o sistema de coordenadas \u00e9 transformado. Ou seja, o grid de pontos do espa\u00e7o, onde cada ponto \u00e9 espa\u00e7ado pelos versores (vetores unit\u00e1rios) da base \u00e9 transformado. Isso, em geral, representa \"deforma\u00e7\u00e3o\" do espa\u00e7o (mais estritamente, do grid de prontos representando o espa\u00e7o). Se for linear, essa tranforma\u00e7\u00e3o far\u00e1 com que um conjunto de pontos igualmente espa\u00e7ados continue igualmente espa\u00e7ados, mas como eles podem sofrer uma mudan\u00e7a de escala, os comprimentos e \u00e1reas do grid n\u00e3o s\u00e3o necessariamente conservados. Na verdade, pode-se demonstar que as \u00e1reas ser\u00e3o escaladas por uma fator exatamente igual ao determinante da matriz de transforma\u00e7\u00e3o. Como determinar a matriz de transforma\u00e7\u00e3o? Na verdade, \u00e9 bem simples. Basta considerar o efeito nos vetores da base. Usaremos a Fig. 1, acima, para ilustrar com um exemplo que nos ajudar\u00e1 a entender o processo. Imaginando que estamos indo da base \\ket{b_i} \\ket{b_i} , com os vetores indicados pelas setas potilhadas, e cujas coordenadas s\u00e3o \\ket{b_1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\end{pmatrix}; \\quad \\ket{b_2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\end{pmatrix} \\ket{b_1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\end{pmatrix}; \\quad \\ket{b_2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\end{pmatrix} os novos vetores da base \\ket{c_i} \\ket{c_i} s\u00e3o dados por \\ket{c_1}= \\left( \\begin{array}{c} \\frac{1}{2} \\\\ \\frac{3}{4} \\\\ \\end{array} \\right); \\quad \\ket{c_2}= \\left( \\begin{array}{c} -\\frac{1}{3} \\\\ \\frac{1}{2} \\\\ \\end{array} \\right) \\Rightarrow \\quad S = \\left( \\begin{array}{cc} \\frac{1}{2} & -\\frac{1}{3} \\\\ \\frac{3}{4} & \\frac{1}{2} \\\\ \\end{array} \\right) \\ket{c_1}= \\left( \\begin{array}{c} \\frac{1}{2} \\\\ \\frac{3}{4} \\\\ \\end{array} \\right); \\quad \\ket{c_2}= \\left( \\begin{array}{c} -\\frac{1}{3} \\\\ \\frac{1}{2} \\\\ \\end{array} \\right) \\Rightarrow \\quad S = \\left( \\begin{array}{cc} \\frac{1}{2} & -\\frac{1}{3} \\\\ \\frac{3}{4} & \\frac{1}{2} \\\\ \\end{array} \\right) Portanto, as colunas da matriz de transforma\u00e7\u00e3o correspondem aos vetores transformados da nova \"base\" (agora n\u00e3o mais ortonormal). Dada a matriz de transforma\u00e7\u00e3o S S , sabemos como todos os vetores do espa\u00e7o se transformam, pois sabemos como os vetores da base se transformam. Assim, podemos facilmente traduzir as coordenadas de uma base na outra, com facilidade. Mas como s\u00e3o transformados (ou representado) os operadores lineares de uma base para outra? A resposta simples \u00e9, novamente, ver o efeito da a\u00e7\u00e3o sobre os vetores expressos nas duas representa\u00e7\u00f5es (bases) e usar as rela\u00e7\u00f5es de transforma\u00e7\u00e3o entre os vetores das buas bases para expressar os elementos de matriz do operador na nova base. Pode-se demonstrar ( verifique! ) que, se L L for a representa\u00e7\u00e3o matricial de um operador \\hat{L} \\hat{L} na primeira base \\ket{b_i} \\ket{b_i} , a sua representa\u00e7\u00e3o matricial na segunda base ser\u00e1 dada por \\tilde{L} = S^{-1}L\\,S. \\tilde{L} = S^{-1}L\\,S. Transforma\u00e7\u00f5es de similaridade Para simplificar a nota\u00e7\u00e3o, vamos expressar os operadores por matrizes, mas o resultado discutido aqui \u00e9 geral. Matrizes (transforma\u00e7\u00f5es) similares Uma matriz B B \u00e9 dita similar a uma matriz A A se B = S^{-1}A\\,S B = S^{-1}A\\,S para qualquer matriz invers\u00edvel S S . Se B B \u00e9 similar a A A , ent\u00e3o B B tem os mesmo autovalores de A A . Isso pode ser facilmente demonstrado ( verifique! ). Como consequ\u00eancia de ter os mesmos autovalores, as duas matrizes ter\u00e3o tamb\u00e9m o mesmo tra\u00e7o (soma dos autovalores) e determinante (produto dos autovalores). Se a matriz S S for unit\u00e1ria ela preservar\u00e1 normas e \u00e2ngulos, de modo que se A A for uma representa\u00e7\u00e3o do operador \\hat{A} \\hat{A} numa base ortonormal, ent\u00e3o B B tamb\u00e9m o ser\u00e1. Como sempre trabalhamos com bases ortogonais, estaremos interessados geralmente em transforma\u00e7\u00f5es unit\u00e1rias de similaridade. Uma quest\u00e3o interessante agora \u00e9 se seria poss\u00edvel, em geral, encontrar uma transforma\u00e7\u00e3o S S que transforme os autovetores de uma matriz M M , geral, de tal modo que a eles passem a formar uma base do espa\u00e7o. Algo, por exemplo, inverso ao mostrado na Figura 1. Em outras palavras, ser\u00e1 que \u00e9 poss\u00edvel encontrar uma transforma\u00e7\u00e3o de similiaridade S S que coloque a matriz M M numa forma diagonal (i.e., que diagonalize M M )? Como vimos, nesta formula\u00e7\u00e3o da MQ usamos operadores lineares para representar grandezas f\u00edsicas observ\u00e1veis. Nesse contexto, vimos que os autovalores do operador est\u00e3o associados aos valores das medidas daquele observ\u00e1vel. Portanto, \u00e9 muito desej\u00e1vel preservar os autovalores de um operador que represente grandezas f\u00edsicas, se quiseremos buscar tranforma\u00e7\u00e3o que o diagonalize. Para esse efeito, portanto, usaremos transforma\u00e7\u00f5es de similidares. Mas como encontrar tais transforma\u00e7\u00f5es? Como diagonalizar M M ? 5.8 Diagonaliza\u00e7\u00e3o de operadores Como estamos interessados e na diagonaliza\u00e7\u00e3o de operadores observ\u00e1veis, vamos coniderar aqui uma operador Hermitiano C C , qualquer, que representa uma dada grandeza f\u00edsica. Estamos interessados em escrever esse operador numa forma diagonal D D , seguindo uma transforma\u00e7\u00e3o de similiaridade S S : D = S^{-1}C\\,S. D = S^{-1}C\\,S. Como fazemos para encontrar a transforma\u00e7\u00e3o S S ? Procedimento para encontrar a transforma\u00e7\u00e3o S Encontre os autovalore e autovetores da matriz C C Normalize os autovetores de C C Forme a matriz S S de modo que as colunas dessa matriz sejam os autovetores (colunas) normalizados de C C A matriz S^{-1} S^{-1} \u00e9 a matriz inversa 1 de S S , tal que S^{-1}S=SS^{-1}=\\mathbb{1} S^{-1}S=SS^{-1}=\\mathbb{1} . Fica como um exerc\u00edcio sugerido demonstrar que este procedimento resulta na forma diagonal D = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} D = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} onde \\lambda_i \\lambda_i s\u00e3o os autovalores de C C e D D . Ser\u00e1 que esse procedimento funciona sempre? N\u00e3o, nem sempre! Nem todas as matrizes s\u00e3o diagnaliz\u00e1veis. Para isso algumas condi\u00e7\u00f5es devem ser satisfeitas pela matriz C C . A primeira condi\u00e7\u00e3o \u00e9 que para ser diagonaliz\u00e1vel uma matriz deve ter autovetores que geram o espa\u00e7o. Assim, se todos os autovetores forem distintos, h\u00e1 uma boa chance dela ser diagonaliz\u00e1vel. Mesmo quando h\u00e1 ra\u00edzes m\u00faltiplias (degeneresc\u00eancia), em alguns casos, a matriz ainda pode ser diagonalizada, mas n\u00e3o sempre. Pode-se mostrar, por\u00e9m, que toda matriz normal \u00e9 diagonaliz\u00e1vel. Embora, essa n\u00e3o \u00e9 uma condi\u00e7\u00e3o necess\u00e1ria. Al\u00e9m disso, todas as matrizes hermitianas e todas as matrizes unit\u00e1rias tamb\u00e9m s\u00e3o diagonaliz\u00e1veis. 5.9 Subespa\u00e7os e projetores Considere um espa\u00e7o n-dimensional \\mathcal{H} \\mathcal{H} com um base ortonormal \\ket{1},\\ket{2},\\dots,\\ket{n} \\ket{1},\\ket{2},\\dots,\\ket{n} . Um subespa\u00e7o \\mathcal{S} \\mathcal{S} de \\mathcal{H} \\mathcal{H} \u00e9 um subconjunto de \\mathcal{H} \\mathcal{H} tal que \\mathcal{S} \\mathcal{S} \u00e9 ele pr\u00f3prio um espa\u00e7o vetorial, com respeito as opera\u00e7\u00f5es de soma e multplica\u00e7\u00e3o por um escalar. Podemos verificar que \\mathcal{S} \\mathcal{S} \u00e9 um espa\u00e7o vetorial usando os mesmos crit\u00e9rios usuais, mas se \\mathcal{S} \\mathcal{S} for um subconjunto de \\mathcal{H} \\mathcal{H} , basta verificar dois crit\u00e9rios O vetor zero (nulo) pertence a \\mathcal{S} \\mathcal{S} Para todos os vetores \\ket{u} \\ket{u} , \\ket{v} \\ket{v} em \\mathcal{S} \\mathcal{S} e escalar \\alpha \\alpha , temos que \\ket{u}+\\ket{v} \\ket{u}+\\ket{v} e \\alpha \\ket{u} \\alpha \\ket{u} pertencem a \\mathcal{S} \\mathcal{S} . Para isso \u00e9 \u00fatil usar o operador projetor, que j\u00e1 discutimos na se\u00e7\u00e3o 5.6 . Relembrando sua defini\u00e7\u00e3o: Operador de proje\u00e7\u00e3o: projetor Suponha um subespa\u00e7o de dimens\u00e3o m m , onde m<n m<n , que tem uma base ortonormal \\mathcal{U} = \\{ \\ket{u_1},\\ket{u_2},\\dots,\\ket{u_m} \\} \\mathcal{U} = \\{ \\ket{u_1},\\ket{u_2},\\dots,\\ket{u_m} \\} . O operador de proje\u00e7\u00e3o \\hat{P}_u \\hat{P}_u \u00e9 dado por \\hat{P}_u =\\sum_{i=1}^{m} \\ket{u_i]} \\bra{u_i}. \\hat{P}_u =\\sum_{i=1}^{m} \\ket{u_i]} \\bra{u_i}. O operador de proje\u00e7\u00e3o tem duas propriedades importantes: \\hat{P}=\\hat{P}^\\dagger \\hat{P}=\\hat{P}^\\dagger , isto \u00e9, ele \u00e9 um operador hermitiano. \\hat{P}=\\hat{P}\\hat{P} \\equiv \\hat{P}^2 \\hat{P}=\\hat{P}\\hat{P} \\equiv \\hat{P}^2 , \u00e9 igual ao seu quadrado. Podemos formar o projetor de apenas um vetor da base \\hat{P}_i=\\ket{u_i}\\bra{u_i} \\hat{P}_i=\\ket{u_i}\\bra{u_i} (imagine que o somat\u00f3rio estende-se apenas ao vetor \\ket{u_i} \\ket{u_i} ). Esse operador \u00e9 totalmente v\u00e1lico e obedece as duas propriedades acima. Ele claramente \u00e9 hermitiano, por exemplo, e tamb\u00e9m obedece a segunda propriedade ( verifique! ). Considere, por exemplo, o efeito de \\hat{P}_i \\ket{\\Psi} \\hat{P}_i \\ket{\\Psi} : \\hat{P}_i \\ket{\\Psi} = \\ket{u_i} \\bra{u_i} \\Psi\\rangle = (\\bra{U_i}\\Psi\\rangle) \\ket{u_i}. \\hat{P}_i \\ket{\\Psi} = \\ket{u_i} \\bra{u_i} \\Psi\\rangle = (\\bra{U_i}\\Psi\\rangle) \\ket{u_i}. Veja que isso efetivamente \"projeta\" a componente de \\ket{\\Psi} \\ket{\\Psi} na dire\u00e7\u00e3o do vetor da base \\ket{u_i} \\ket{u_i} . Observe tamb\u00e9m que o projetor de um dado espa\u00e7o (subespa\u00e7o) \u00e9 igual ao opeador identidade daquele espa\u00e7o (subespa\u00e7o), satisfazendo a rela\u00e7\u00e3o de completeza daquele espa\u00e7o (subespa\u00e7o): \\mathbf{I}_m = \\sum_{i=1}^{m} \\ket{u_i} \\bra{u_i} = \\sum_{i=1}^{m} P_i = \\mathbb{1}_m \\mathbf{I}_m = \\sum_{i=1}^{m} \\ket{u_i} \\bra{u_i} = \\sum_{i=1}^{m} P_i = \\mathbb{1}_m onde \\mathbf{I}_m=\\mathbb{1}_m \\mathbf{I}_m=\\mathbb{1}_m expressa explicitamente uma matriz identidade de ordem m m . 5.10 Fun\u00e7\u00f5es de operadores Assim como podemos expandir uma fun\u00e7\u00e3o ordin\u00e1ria f(x) f(x) numa s\u00e9rie de Taylor 2 : f(x) = \\sum_{i=0}^{\\infty} a_i x^i f(x) = \\sum_{i=0}^{\\infty} a_i x^i podemos, de forma similar, generalizar essa ideia para definir a fun\u00e7\u00e3o de um operador F(\\hat{A}) F(\\hat{A}) : F(\\hat{A}) = \\sum_{i=0}^{\\infty} a_i \\hat{A}^i F(\\hat{A}) = \\sum_{i=0}^{\\infty} a_i \\hat{A}^i Note, por\u00e9m, que em geral os coeficientes a_i a_i podem ser n\u00fameros complexos. Exemplo: exponencial de um operador Vamos considerar um exemplo bastante importante na mec\u00e2nica qu\u00e2ntica, que \u00e9 a exponencial de um operador. Usando a rela\u00e7\u00e3o que aprendemos entre operadores e matrizes, usaremos a nota\u00e7\u00e3o de matrizes, para simplificar. \\operatorname{exp}(H) = \\operatorname{e}^H = \\sum_{k=0}^{\\infty} \\frac{1}{k!} H^k = \\mathbf{I} + H + \\frac{1}{2} H^2 + \\frac{1}{6} H^3 + \\dots \\operatorname{exp}(H) = \\operatorname{e}^H = \\sum_{k=0}^{\\infty} \\frac{1}{k!} H^k = \\mathbf{I} + H + \\frac{1}{2} H^2 + \\frac{1}{6} H^3 + \\dots onde \\mathbf{I} \\mathbf{I} \u00e9 matriz identidade, e H^k H^k \u00e9 pot\u00eancia k k da matriz H H . 5.11 Generaliza\u00e7\u00e3o para espa\u00e7os cont\u00ednuos Para ajudar a fixar as ideias de espa\u00e7os vetorias, aproveitando a analogia com os vetores do espa\u00e7o euclidiano, que estamos acostumados na F\u00edsica, at\u00e9 este ponto temos nos limitado \u00e0 espa\u00e7os discretos de dimens\u00f5es finitas. Por\u00e9m, o formalismo de espa\u00e7os vetorias pode ser facilmente extendido tamb\u00e9m a espa\u00e7os mais gerais, como espa\u00e7os cont\u00ednuos e infinitos. Neste caso, em geral, estamos falando do espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas. Na verdade, mesmo sem perceber, j\u00e1 temos alguma familiaridade com esse tipo de espa\u00e7o, mesmo na mec\u00e2nica qu\u00e2ntica, pois come\u00e7amos a nossa discuss\u00e3o neste curso, falando da equa\u00e7\u00e3o de Schr\u00f6dinger e fun\u00e7\u00f5es de ondas. Pois ent\u00e3o, como seria de se esperar, as fun\u00e7\u00e3os de ondas da mec\u00e2nica qu\u00e2ntica formam um espa\u00e7o vetorial, com as mesmas propriedades dos espa\u00e7os discretos que temos discutidos at\u00e9 agora. A seguir fazeremos uma breve introdu\u00e7\u00e3o da nota\u00e7\u00e3o usada para descrever espa\u00e7os vetorias cont\u00ednuos na mec\u00e2nica qu\u00e2ntica. Vetores Assim como os vetores de um espa\u00e7o discreto e finito podem ser expandidos numa base ortonormal \\{ \\ket{u_i},\\ i=1,2,\\dots \\} \\{ \\ket{u_i},\\ i=1,2,\\dots \\} com um n\u00famero finito de componentes, os vetores de um espa\u00e7o cont\u00ednuo s\u00e3o representados numa base ortonomal \\{ \\ket{w_{\\alpha}},\\ x_1 \\le \\alpha \\le x_2 \\} \\{ \\ket{w_{\\alpha}},\\ x_1 \\le \\alpha \\le x_2 \\} por um n\u00famero infinito de pontos no intervalo [a,b] [a,b] . Isto \u00e9, os (infinitos) coeficientes da expans\u00e3o s\u00e3o representados por uma fun\u00e7\u00e3o c(\\alpha) c(\\alpha) , nesse intervalo. Neste caso a expans\u00e3o \u00e9 escrita como: \\ket{\\psi} = \\int c(\\alpha) \\ket{w_{\\alpha}} d \\alpha \\ket{\\psi} = \\int c(\\alpha) \\ket{w_{\\alpha}} d \\alpha Produto interno Sendo o vetor \\ket{\\phi}=\\int b(\\alpha) \\ket{w_{\\alpha}} d \\alpha \\ket{\\phi}=\\int b(\\alpha) \\ket{w_{\\alpha}} d \\alpha . O produto interno dos vetores \\ket{\\phi} \\ket{\\phi} e \\ket{\\psi} \\ket{\\psi} , sendo os dois vetores expressos na base \\ket{w_{\\alpha}} \\ket{w_{\\alpha}} \u00e9 dado por: \\bra{\\phi}\\psi\\rangle = \\int b(\\alpha)^* c(\\alpha) d\\alpha \\bra{\\phi}\\psi\\rangle = \\int b(\\alpha)^* c(\\alpha) d\\alpha Bases ortonormal No espa\u00e7o discreto temos: \\langle u_i | u_j \\rangle = \\delta_{ij} \\langle u_i | u_j \\rangle = \\delta_{ij} onde \\delta_{ij} \\delta_{ij} \u00e9 o delta de Kronecker. No espa\u00e7o cont\u00ednuo temos: \\langle w_{\\alpha} | w_{\\alpha'} \\rangle = \\delta(\\alpha-\\alpha') \\langle w_{\\alpha} | w_{\\alpha'} \\rangle = \\delta(\\alpha-\\alpha') onde \\delta(\\alpha-\\alpha') \\delta(\\alpha-\\alpha') \u00e9 a fun\u00e7\u00e3o delta de Dirac. Rela\u00e7\u00e3o de completeza \\sum_i \\ket{u_i} \\bra{u_i} = \\mathbf{\\hat{1}} \\sum_i \\ket{u_i} \\bra{u_i} = \\mathbf{\\hat{1}} \\int \\ket{w_{\\alpha}} \\bra{w_{\\alpha}} d\\alpha = \\mathbf{\\hat{1}} \\int \\ket{w_{\\alpha}} \\bra{w_{\\alpha}} d\\alpha = \\mathbf{\\hat{1}} Representa\u00e7\u00e3o de operadores Podemos tamb\u00e9m estender a representa\u00e7\u00e3o de um operador para um espa\u00e7o cont\u00ednuo de maneira bastante similar e natural. Se imaginarmos uma representa\u00e7\u00e3o matricial, o operador corresponderia a uma matriz \"infinita\", com um n\u00famero infito de linhas e colunas, e os elementos de matrizes s\u00e3o pontos: \\hat{A} \\Rightarrow\\left(\\begin{array}{ccc} \\ddots & \\vdots & \\cdots \\\\ \\cdots & A (\\alpha, \\alpha^{\\prime} ) & \\cdots \\\\ \\cdots & \\cdots & \\ddots \\end{array}\\right). \\hat{A} \\Rightarrow\\left(\\begin{array}{ccc} \\ddots & \\vdots & \\cdots \\\\ \\cdots & A (\\alpha, \\alpha^{\\prime} ) & \\cdots \\\\ \\cdots & \\cdots & \\ddots \\end{array}\\right). Seguindo numa analogia geom\u00e9trica, podemos imaginar um plano de pontos (elementos de matriz) cujos valores s\u00e3o dados por uma fun\u00e7\u00e3o de duas var\u00edaveis A(\\alpha, \\alpha^{\\prime}) A(\\alpha, \\alpha^{\\prime}) , determinador por: A(\\alpha, \\alpha^{\\prime}) = \\int \\bra{w_{\\alpha}} A \\ket{w_{\\alpha}} d \\alpha A(\\alpha, \\alpha^{\\prime}) = \\int \\bra{w_{\\alpha}} A \\ket{w_{\\alpha}} d \\alpha Fun\u00e7\u00f5es de ondas A generaliza\u00e7\u00e3o para espa\u00e7os cont\u00ednuos nos permite conectar as diferenes representa\u00e7\u00f5es da mec\u00e2nica qu\u00e2ntica, assim como v\u00e1rias ideias j\u00e1 discutidas neste curso, mas que pareciam estar, literalmente, em \"espa\u00e7os\" (contextos) diferentes. Por exemplo, nos permite conectar a mec\u00e2nica ondulat\u00f3ria com a mec\u00e2ncia matricial, assim como formula\u00e7\u00e3o em termos de fun\u00e7\u00f5es de ondas \\psi(x) \\psi(x) com o vetor \\ket{\\psi} \\ket{\\psi} . Na linguagem dos espa\u00e7os vetoriais cont\u00ednuos, a fun\u00e7\u00e3o cont\u00ednua x x \u00e9 uma representa\u00e7\u00e3o (coeficientes) do vetor \\ket{x} \\ket{x} , da mesma forma, o momento linear est\u00e1 relacionado a um vetor \\ket{p} \\ket{p} . Nesta descri\u00e7\u00e3o, as fun\u00e7\u00e3o de ondas \\psi(x) \\psi(x) e \\bar{\\psi}(p) \\bar{\\psi}(p) s\u00e3o dadas por \\psi(x) = \\bra{x} \\psi \\rangle, \\qquad \\bar{\\psi}(p) = \\bra{p} \\psi \\rangle \\psi(x) = \\bra{x} \\psi \\rangle, \\qquad \\bar{\\psi}(p) = \\bra{p} \\psi \\rangle e correspondem, respectivamente, \u00e0s representa\u00e7\u00f5es do vetor \\ket{\\psi} \\ket{\\psi} nas bases de posi\u00e7\u00e3o e momento, definidas pelas fun\u00e7\u00f5es \\xi_{x_0}(x)=\\delta(x-x_0), \\qquad v_{p_0}=\\frac{1}{\\sqrt{2\\pi}\\hbar}e^{ip_0x/\\hbar}. \\xi_{x_0}(x)=\\delta(x-x_0), \\qquad v_{p_0}=\\frac{1}{\\sqrt{2\\pi}\\hbar}e^{ip_0x/\\hbar}. Observe que essas fun\u00e7\u00f5es de base satisfazem as condi\u00e7\u00f5es necess\u00e1rias \\bra{x}x'\\rangle = \\delta(x-x'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{x} \\bra{x} dx \\bra{x}x'\\rangle = \\delta(x-x'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{x} \\bra{x} dx \\bra{p}p'\\rangle = \\delta(p-p'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{p} \\bra{p} dp \\bra{p}p'\\rangle = \\delta(p-p'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{p} \\bra{p} dp Al\u00e9m disso, o produto interno entre posi\u00e7\u00e3o e momento \u00e9 dado por \\bra{x}p\\rangle = \\bra{p}x\\rangle^* = \\frac{1}{\\sqrt{2\\pi}\\hbar}e^{i p_0 x /\\hbar}. \\bra{x}p\\rangle = \\bra{p}x\\rangle^* = \\frac{1}{\\sqrt{2\\pi}\\hbar}e^{i p_0 x /\\hbar}. Finalmente, usando a rela\u00e7\u00e3o de completeza podemos mostrar que as fun\u00e7\u00f5es de ondas nos espa\u00e7os da posi\u00e7\u00e3o e momento est\u00e3o relacionados atrav\u00e9s de uma transformada de Fourier: \\begin{aligned} \\psi(x) &=\\langle x | \\psi\\rangle=\\langle x|1| \\psi\\rangle=\\left\\langle x\\left|\\left(\\int|p\\rangle\\langle p| d p\\right)\\right| \\psi\\right> \\\\ &=\\int\\langle x | p\\rangle\\langle p | \\psi\\rangle d p \\\\ &=\\frac{1}{\\sqrt{2 \\pi} \\hbar} \\int e^{i \\frac{p x}{\\hbar}} \\bar{\\psi}(p) d p \\end{aligned} \\begin{aligned} \\psi(x) &=\\langle x | \\psi\\rangle=\\langle x|1| \\psi\\rangle=\\left\\langle x\\left|\\left(\\int|p\\rangle\\langle p| d p\\right)\\right| \\psi\\right> \\\\ &=\\int\\langle x | p\\rangle\\langle p | \\psi\\rangle d p \\\\ &=\\frac{1}{\\sqrt{2 \\pi} \\hbar} \\int e^{i \\frac{p x}{\\hbar}} \\bar{\\psi}(p) d p \\end{aligned} 5.12 Produto tensorial Se um \\mathcal{V} \\mathcal{V} \u00e9 um espa\u00e7o de dimens\u00e3o p p e \\mathcal{W} \\mathcal{W} \u00e9 um espa\u00e7o de dimens\u00e3o q q , ent\u00e3o o espa\u00e7o V \\otimes W V \\otimes W tem dimens\u00e3o p q p q . Seja |v\\rangle \\in V |v\\rangle \\in V e |w\\rangle \\in W |w\\rangle \\in W dois vetores desses espa\u00e7os. Ent\u00e3o o vetor |v\\rangle \\otimes|w\\rangle \\in V \\otimes W |v\\rangle \\otimes|w\\rangle \\in V \\otimes W . Nota\u00e7\u00f5es alternativas do vetor |v\\rangle \\otimes|w\\rangle |v\\rangle \\otimes|w\\rangle incluem |v\\rangle|w\\rangle |v\\rangle|w\\rangle e |v w\\rangle |v w\\rangle . Se A A \u00e9 um operador que atua no espa\u00e7o V V e B B \u00e9 um operador que atua no espa\u00e7o W, W, ent\u00e3o o operador formado pelo produto tensorial dado por A \\otimes B A \\otimes B atua nos vetores |v\\rangle \\otimes|w\\rangle |v\\rangle \\otimes|w\\rangle da seguinte maneira: (A \\otimes B)|v\\rangle|w\\rangle=(A|v\\rangle)(B|w\\rangle) (A \\otimes B)|v\\rangle|w\\rangle=(A|v\\rangle)(B|w\\rangle) Considerando agora a norma de um vetor que perten\u00e7a a V \\otimes W . V \\otimes W . Suponha que |\\psi\\rangle=|v\\rangle|w\\rangle. |\\psi\\rangle=|v\\rangle|w\\rangle. Neste caso, a norma \u00e9 dada por \\langle\\psi | \\psi\\rangle=\\langle v | v\\rangle\\langle w | w\\rangle \\langle\\psi | \\psi\\rangle=\\langle v | v\\rangle\\langle w | w\\rangle Representa\u00e7ao matricial do produto tensorial Seja A A uma matriz (m \\times n) (m \\times n) e B B uma matriz (p \\times q) (p \\times q) . O produto tensorial A \\otimes B A \\otimes B \u00e9 a matriz com m p m p linhas e n q n q colunas. Os elementos desta matriz s\u00e3o constru\u00eddos pelas submatrizes: A_{i j} B A_{i j} B ou seja, multiplique B B por cada componente de A A e arrange essas submatrizes numa matriz maior. A matriz completa, representando o produto tensorial \u00e9 dada por: A=\\left(\\begin{array}{cccc} A_{11} B & A_{12} B & \\dots & A_{1 n} B \\\\ A_{21} B & A_{22} B & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{m 1} B & \\dots & \\dots & A_{m n} B \\end{array}\\right) A=\\left(\\begin{array}{cccc} A_{11} B & A_{12} B & \\dots & A_{1 n} B \\\\ A_{21} B & A_{22} B & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{m 1} B & \\dots & \\dots & A_{m n} B \\end{array}\\right) Um vetor coluna em duas dimens\u00f5es possui duas linhas e uma coluna. Como o produto tensorial entre uma matriz (m \\times n) (m \\times n) e uma matriz (p \\times q) (p \\times q) tem m p m p linhas e n q n q colunas, o produto tensorial entre as duas matrizes (2 \\times 1) (2 \\times 1) ter\u00e1 (2 \\times 2) = 4 (2 \\times 2) = 4 linhas e (1 \\times 1) = 1 (1 \\times 1) = 1 coluna. A forma de calcular isso \u00e9: \\left(\\begin{array}{l} a \\\\ b \\end{array}\\right) \\otimes\\left(\\begin{array}{l} c \\\\ d \\end{array}\\right)=\\left(\\begin{array}{l} a c \\\\ a d \\\\ b c \\\\ b d \\end{array}\\right) \\left(\\begin{array}{l} a \\\\ b \\end{array}\\right) \\otimes\\left(\\begin{array}{l} c \\\\ d \\end{array}\\right)=\\left(\\begin{array}{l} a c \\\\ a d \\\\ b c \\\\ b d \\end{array}\\right) Agora considere o produto tensorial de duas matrizes 2 \\times 2 2 \\times 2 . A=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right), \\quad B=\\left(\\begin{array}{ll} w & x \\\\ y & z \\end{array}\\right) A=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right), \\quad B=\\left(\\begin{array}{ll} w & x \\\\ y & z \\end{array}\\right) Esse produto tensorial ser\u00e1 uma matriz (4 \\times 4) (4 \\times 4) : A \\otimes B=\\left(\\begin{array}{cc} a B & b B \\\\ c B & d B \\end{array}\\right)=\\left(\\begin{array}{cccc} a w & a x & b w & b x \\\\ a y & a z & b y & b z \\\\ c w & c x & d w & d x \\\\ c y & c z & d y & d z \\end{array}\\right) A \\otimes B=\\left(\\begin{array}{cc} a B & b B \\\\ c B & d B \\end{array}\\right)=\\left(\\begin{array}{cccc} a w & a x & b w & b x \\\\ a y & a z & b y & b z \\\\ c w & c x & d w & d x \\\\ c y & c z & d y & d z \\end{array}\\right) Como calcular a matriz inversa Se \\mathbf{A} \\mathbf{A} uma matriz n\u00e3o singular, sua inversa \u00e9 dada por: \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\mathbf{C}^{\\mathrm{T}}=\\frac{1}{|\\mathbf{A}|}\\left(\\begin{array}{cccc} \\mathbf{C}_{11} & \\mathbf{C}_{21} & \\cdots & \\mathbf{C}_{n 1} \\\\ \\mathbf{C}_{12} & \\mathbf{C}_{22} & \\cdots & \\mathbf{C}_{n 2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{C}_{1 n} & \\mathbf{C}_{2 n} & \\cdots & \\mathbf{C}_{n n} \\end{array}\\right) \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\mathbf{C}^{\\mathrm{T}}=\\frac{1}{|\\mathbf{A}|}\\left(\\begin{array}{cccc} \\mathbf{C}_{11} & \\mathbf{C}_{21} & \\cdots & \\mathbf{C}_{n 1} \\\\ \\mathbf{C}_{12} & \\mathbf{C}_{22} & \\cdots & \\mathbf{C}_{n 2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{C}_{1 n} & \\mathbf{C}_{2 n} & \\cdots & \\mathbf{C}_{n n} \\end{array}\\right) onde \\mathbf{C} \\mathbf{C} \u00e9 a matriz de cofatores \\mathbf{C_{ij}} = (-1)^{i+j}\\ \\mathbf{D_{ij}} \\mathbf{C_{ij}} = (-1)^{i+j}\\ \\mathbf{D_{ij}} , sendo \\mathbf{D_{ij}} \\mathbf{D_{ij}} o determinante da matriz menor \\mathbf{\\tilde{A}_{ij}} \\mathbf{\\tilde{A}_{ij}} , obtida a partir da exclus\u00e3o da linha \\mathbf{i} \\mathbf{i} e coluna \\mathbf{j} \\mathbf{j} da matriz \\mathbf{A} \\mathbf{A} . O termo |\\mathbf{A}|=\\operatorname{det}(\\mathbf{A}) |\\mathbf{A}|=\\operatorname{det}(\\mathbf{A}) representa o determinante de \\mathbf{A} \\mathbf{A} . Se \\mathbf{A} \\mathbf{A} for uma matriz 3x3, sua inversa \u00e9 dada por: \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\left(\\begin{array}{lll} \\left|\\begin{array}{ll} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{12} \\\\ a_{33} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{23} & a_{21} \\\\ a_{33} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{11} \\\\ a_{23} & a_{21} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{11} \\\\ a_{32} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array}\\right| \\end{array} \\right) \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\left(\\begin{array}{lll} \\left|\\begin{array}{ll} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{12} \\\\ a_{33} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{23} & a_{21} \\\\ a_{33} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{11} \\\\ a_{23} & a_{21} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{11} \\\\ a_{32} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array}\\right| \\end{array} \\right) Se \\mathbf{A} \\mathbf{A} for uma matriz 2x2, sua inversa \u00e9 simplesmente: \\mathbf{A}^{-1}=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right)^{-1}=\\frac{1}{\\operatorname{det}(\\mathbf{A})}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right)=\\frac{1}{a d-b c}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right) \\mathbf{A}^{-1}=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right)^{-1}=\\frac{1}{\\operatorname{det}(\\mathbf{A})}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right)=\\frac{1}{a d-b c}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right) \u21a9 S\u00e9ries de Taylor Para uma fun\u00e7\u00e3o cont\u00ednua f(x) f(x) , que seja infinitamente diferenci\u00e1vel, a s\u00e9rie de Taylor \u00e9 uma sua expans\u00e3o em s\u00e9rie de potencias do tipo: f(x_0) = \\sum_{k=1}^{\\infty} \\frac{1}{k!} \\left(\\frac{df}{dx}\\right)^k_{x=x_0} (x-x_0)^k = \\sum_{k=1}^{\\infty} \\frac{f^{('k)}(x_0)}{k!} (x-x_0)^k. f(x_0) = \\sum_{k=1}^{\\infty} \\frac{1}{k!} \\left(\\frac{df}{dx}\\right)^k_{x=x_0} (x-x_0)^k = \\sum_{k=1}^{\\infty} \\frac{f^{('k)}(x_0)}{k!} (x-x_0)^k. \u21a9","title":"Aulas S7"},{"location":"Aulas_S7/#5-estrutura-matematica-da-mq-parte-2","text":"Nesta semana continuaremos a discutir a estrutura matem\u00e1tica da teoria qu\u00e2ntica, complementando a discuss\u00e3o a respeito de transforma\u00e7\u00f5es lineares e, em particular, mudan\u00e7as de bases e o processo de diagonaliza\u00e7\u00e3o de um operador (matriz). Tamb\u00e9m iremos apresentar alguns casos importantes como os operadores projetores em subespa\u00e7os e o produto tensorial que permite expandir espa\u00e7os de Hilbert. Para retornar aos t\u00f3picos das aulas anteriores, use o menu de navega\u00e7\u00e3o ou clique aqui .","title":"5. Estrutura matem\u00e1tica da MQ (parte 2)"},{"location":"Aulas_S7/#57-transformacoes-lineares-e-mudancas-de-base","text":"Na aula anterior n\u00f3s discutimos como encontrar os autovetores de um operador. Surgiu, ent\u00e3o, a quest\u00e3o se os autovetores do operador deveriam ser sempre ortogonais e poderiam formar uma base. Aqui, vamos iniciar esta discuss\u00e3o com a quest\u00e3o, geral, se os autovetores de um operador podem sempre formam uma base do espa\u00e7o vetorial. Nos casos onde isso \u00e9 poss\u00edvel, como constru\u00ed-la? Primeiro, vamos relembrar que condi\u00e7\u00f5es um conjunto deve satisfazer para formar uma base. De maneira simples, para formar uma base do espa\u00e7o (ou subespa\u00e7o) um conjunto de autovetores devem satisfazer duas condi\u00e7\u00f5es: Condi\u00e7\u00f5es para formar uma base Ser ortonormais. Satisfazer a rela\u00e7\u00e3o de completeza. Para ajudar a fixar esses conceitos, considere o seguinte exemplo. Exemplo Considere o operador dado pela matriz Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} Observe que seus autovalores s\u00e3o \\lambda_{1,2}=\\pm 1 \\lambda_{1,2}=\\pm 1 . Tendo como autovetores: \\ket{u_1}= \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\ket{u_2}= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \\ket{u_1}= \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\ket{u_2}= \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}. \u00c9 f\u00e1cil verificar que esse autovetores s\u00e3o ortonomais, portanto satisfazendo a primeira condi\u00e7\u00e3o. Agora, precisamos verificar se a segunda condi\u00e7\u00e3o tamb\u00e9m \u00e9 satisfeita. Ou seja, se o conjunto \u00e9 completo. Para isso, fazemos: \\ket{u_1} \\bra{u_1} + \\ket{u_2} \\bra{u_2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\\\ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\mathbb{1}. \\ket{u_1} \\bra{u_1} + \\ket{u_2} \\bra{u_2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\\\ = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\mathbb{1}. Portanto, verificamos que a rela\u00e7\u00e3o de completeza tamb\u00e9m \u00e9 satisfeita. A conclus\u00e3o final \u00e9 que, neste caso, os autovetores formam uma base. De faot, \u00e9 f\u00e1cil verificar que qualquer vetor de dimens\u00e3o dois pode ser escrito numa expans\u00e3o em termos do conjunto \\{ \\ket{u_1},\\ket{u_2} \\} \\{ \\ket{u_1},\\ket{u_2} \\} . Seja, por exemplo, \\ket{\\psi} \\ket{\\psi} um vertor arbitr\u00e1rio \\ket{\\psi} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha \\ket{u_1} + \\beta \\ket{u_2}. \\ket{\\psi} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\alpha \\ket{u_1} + \\beta \\ket{u_2}.","title":"5.7 Transforma\u00e7\u00f5es lineares e mudan\u00e7as de base"},{"location":"Aulas_S7/#forma-diagonal","text":"O operador \\hat{Z} \\hat{Z} \u00e9 um exemplo de operador na forma diagonal. Representa\u00e7\u00e3o diagonal Um operador \\hat{A} \\hat{A} est\u00e1 na forma diagonal quando escrito \\hat{A} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} \\hat{A} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} onde os vetores \\ket{u_i} \\ket{u_i} formam uma base ortonormal. Neste caso, a matriz A A tem a forma A = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} A = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} Um operador \u00e9 dito diagonaliz\u00e1vel quando pode ser escrito na forma diagonal. A representa\u00e7\u00e3o diagonal tamb\u00e9m \u00e9 chamada de decomposi\u00e7\u00e3o ortogonal . Nem todos os operadores de um espa\u00e7o vetorial tem forma diagonal. Na \u00faltima aula, vimos tamb\u00e9m um exemplo em que M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} M = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} tem autovetores que n\u00e3o s\u00e3o ortonormais \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\Rightarrow \\quad \\bra{u_1} u_2\\rangle = \\frac{1}{\\sqrt{10}}. \\ket{u_1} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\ket{u_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\Rightarrow \\quad \\bra{u_1} u_2\\rangle = \\frac{1}{\\sqrt{10}}. Ser\u00e1 que \u00e9 poss\u00edvel reescrever o operador M M numa forma diagonal? Ali\u00e1s, qual seria o significado disso e quais propriedades tal opera\u00e7\u00e3o deveria satisfazer, se existisse? Mais importante ainda, qual seria sua utilidade? Mais adiante veremos quais s\u00e3o as condi\u00e7\u00f5es necess\u00e1rias para um operador ser diagonaliz\u00e1vel. Veremos tamb\u00e9m como fazer para diagonaliz\u00e1-lo. Antes, vamos introduzir mais dois conceitos importantes. Os conceitos de mudan\u00e7a de base e transforma\u00e7\u00f5es de similaridade .","title":"Forma diagonal"},{"location":"Aulas_S7/#mudanca-de-base","text":"Para efeito de compara\u00e7\u00e3o e analogia, podemos pensar na mudan\u00e7a de base como algo parecido \u00e0 mudan\u00e7a de sistema de coordenadas, com o qual j\u00e1 estamos acostumados na F\u00edsica. Como sabemos, a representa\u00e7\u00e3o de um vetor (ou operador), por exemplo, na forma matricial ir\u00e1 depender da base usada para represent\u00e1-lo, pois em geral os valores das compenentes ser\u00e3o diferentes, mas isso n\u00e3o altera o significado de cada um desses objetos. Dado um vetor qualquer \\ket{v} = \\sum_i b_i \\ket{b_i} = \\sum_i c_i \\ket{c_i}, \\ket{v} = \\sum_i b_i \\ket{b_i} = \\sum_i c_i \\ket{c_i}, expresso nas bases \\{ \\ket{b_i} \\} \\{ \\ket{b_i} \\} e \\{ \\ket{c_i} \\} \\{ \\ket{c_i} \\} , \u00e9 sempre poss\u00edvel achar uma transforma\u00e7\u00e3o de coordenadas que permita expressar as coordenadas c_i c_i desse vetor a partir das coordenadas b_i b_i . Para isso, basta encontrar a matriz (operador linear), S S , que leva cada vetor \\ket{b_i} \\ket{b_i} no correspondente vetor \\ket{c_i} \\ket{c_i} : \\ket{c_i} = S \\ket{b_i} \\rightarrow \\ket{b_i} = S^{-1} \\ket{c_i} \\Rightarrow c_i = b_i (S^{-1}). \\ket{c_i} = S \\ket{b_i} \\rightarrow \\ket{b_i} = S^{-1} \\ket{c_i} \\Rightarrow c_i = b_i (S^{-1}). Uma forma de pensar nisso \u00e9 que a transforma\u00e7\u00e3o S S leva os vetores da base \\ket{b_i} \\ket{b_i} nos vetores da base \\ket{c_i} \\ket{c_i} . Neste caso, S S \u00e9 efetivamente um mapa de como fazer essa transforma\u00e7\u00e3o dos vetores da base, e \u00e9 uma receita (mapa) de como \"levar\" cada ponto de um sistema de coordenadas no ponto correspondente no outro sistema de coordenadas. Na verdade, \u00e9 importante lembrar que aqui estamos interessados nas coordenadas (representa\u00e7\u00e3o) do vetor, que est\u00e1 sendo representado nas diferentes bases (sistemas de coordenadas). Figura 1: Efeito de uma tranforma\u00e7\u00e3o linear (S) num grid de pontos e nos versores da base Note, por\u00e9m, que nessa interpreta\u00e7\u00e3o todo o sistema de coordenadas \u00e9 transformado. Ou seja, o grid de pontos do espa\u00e7o, onde cada ponto \u00e9 espa\u00e7ado pelos versores (vetores unit\u00e1rios) da base \u00e9 transformado. Isso, em geral, representa \"deforma\u00e7\u00e3o\" do espa\u00e7o (mais estritamente, do grid de prontos representando o espa\u00e7o). Se for linear, essa tranforma\u00e7\u00e3o far\u00e1 com que um conjunto de pontos igualmente espa\u00e7ados continue igualmente espa\u00e7ados, mas como eles podem sofrer uma mudan\u00e7a de escala, os comprimentos e \u00e1reas do grid n\u00e3o s\u00e3o necessariamente conservados. Na verdade, pode-se demonstar que as \u00e1reas ser\u00e3o escaladas por uma fator exatamente igual ao determinante da matriz de transforma\u00e7\u00e3o. Como determinar a matriz de transforma\u00e7\u00e3o? Na verdade, \u00e9 bem simples. Basta considerar o efeito nos vetores da base. Usaremos a Fig. 1, acima, para ilustrar com um exemplo que nos ajudar\u00e1 a entender o processo. Imaginando que estamos indo da base \\ket{b_i} \\ket{b_i} , com os vetores indicados pelas setas potilhadas, e cujas coordenadas s\u00e3o \\ket{b_1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\end{pmatrix}; \\quad \\ket{b_2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\end{pmatrix} \\ket{b_1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\end{pmatrix}; \\quad \\ket{b_2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ \\end{pmatrix} os novos vetores da base \\ket{c_i} \\ket{c_i} s\u00e3o dados por \\ket{c_1}= \\left( \\begin{array}{c} \\frac{1}{2} \\\\ \\frac{3}{4} \\\\ \\end{array} \\right); \\quad \\ket{c_2}= \\left( \\begin{array}{c} -\\frac{1}{3} \\\\ \\frac{1}{2} \\\\ \\end{array} \\right) \\Rightarrow \\quad S = \\left( \\begin{array}{cc} \\frac{1}{2} & -\\frac{1}{3} \\\\ \\frac{3}{4} & \\frac{1}{2} \\\\ \\end{array} \\right) \\ket{c_1}= \\left( \\begin{array}{c} \\frac{1}{2} \\\\ \\frac{3}{4} \\\\ \\end{array} \\right); \\quad \\ket{c_2}= \\left( \\begin{array}{c} -\\frac{1}{3} \\\\ \\frac{1}{2} \\\\ \\end{array} \\right) \\Rightarrow \\quad S = \\left( \\begin{array}{cc} \\frac{1}{2} & -\\frac{1}{3} \\\\ \\frac{3}{4} & \\frac{1}{2} \\\\ \\end{array} \\right) Portanto, as colunas da matriz de transforma\u00e7\u00e3o correspondem aos vetores transformados da nova \"base\" (agora n\u00e3o mais ortonormal). Dada a matriz de transforma\u00e7\u00e3o S S , sabemos como todos os vetores do espa\u00e7o se transformam, pois sabemos como os vetores da base se transformam. Assim, podemos facilmente traduzir as coordenadas de uma base na outra, com facilidade. Mas como s\u00e3o transformados (ou representado) os operadores lineares de uma base para outra? A resposta simples \u00e9, novamente, ver o efeito da a\u00e7\u00e3o sobre os vetores expressos nas duas representa\u00e7\u00f5es (bases) e usar as rela\u00e7\u00f5es de transforma\u00e7\u00e3o entre os vetores das buas bases para expressar os elementos de matriz do operador na nova base. Pode-se demonstrar ( verifique! ) que, se L L for a representa\u00e7\u00e3o matricial de um operador \\hat{L} \\hat{L} na primeira base \\ket{b_i} \\ket{b_i} , a sua representa\u00e7\u00e3o matricial na segunda base ser\u00e1 dada por \\tilde{L} = S^{-1}L\\,S. \\tilde{L} = S^{-1}L\\,S.","title":"Mudan\u00e7a de base"},{"location":"Aulas_S7/#transformacoes-de-similaridade","text":"Para simplificar a nota\u00e7\u00e3o, vamos expressar os operadores por matrizes, mas o resultado discutido aqui \u00e9 geral. Matrizes (transforma\u00e7\u00f5es) similares Uma matriz B B \u00e9 dita similar a uma matriz A A se B = S^{-1}A\\,S B = S^{-1}A\\,S para qualquer matriz invers\u00edvel S S . Se B B \u00e9 similar a A A , ent\u00e3o B B tem os mesmo autovalores de A A . Isso pode ser facilmente demonstrado ( verifique! ). Como consequ\u00eancia de ter os mesmos autovalores, as duas matrizes ter\u00e3o tamb\u00e9m o mesmo tra\u00e7o (soma dos autovalores) e determinante (produto dos autovalores). Se a matriz S S for unit\u00e1ria ela preservar\u00e1 normas e \u00e2ngulos, de modo que se A A for uma representa\u00e7\u00e3o do operador \\hat{A} \\hat{A} numa base ortonormal, ent\u00e3o B B tamb\u00e9m o ser\u00e1. Como sempre trabalhamos com bases ortogonais, estaremos interessados geralmente em transforma\u00e7\u00f5es unit\u00e1rias de similaridade. Uma quest\u00e3o interessante agora \u00e9 se seria poss\u00edvel, em geral, encontrar uma transforma\u00e7\u00e3o S S que transforme os autovetores de uma matriz M M , geral, de tal modo que a eles passem a formar uma base do espa\u00e7o. Algo, por exemplo, inverso ao mostrado na Figura 1. Em outras palavras, ser\u00e1 que \u00e9 poss\u00edvel encontrar uma transforma\u00e7\u00e3o de similiaridade S S que coloque a matriz M M numa forma diagonal (i.e., que diagonalize M M )? Como vimos, nesta formula\u00e7\u00e3o da MQ usamos operadores lineares para representar grandezas f\u00edsicas observ\u00e1veis. Nesse contexto, vimos que os autovalores do operador est\u00e3o associados aos valores das medidas daquele observ\u00e1vel. Portanto, \u00e9 muito desej\u00e1vel preservar os autovalores de um operador que represente grandezas f\u00edsicas, se quiseremos buscar tranforma\u00e7\u00e3o que o diagonalize. Para esse efeito, portanto, usaremos transforma\u00e7\u00f5es de similidares. Mas como encontrar tais transforma\u00e7\u00f5es? Como diagonalizar M M ?","title":"Transforma\u00e7\u00f5es de similaridade"},{"location":"Aulas_S7/#58-diagonalizacao-de-operadores","text":"Como estamos interessados e na diagonaliza\u00e7\u00e3o de operadores observ\u00e1veis, vamos coniderar aqui uma operador Hermitiano C C , qualquer, que representa uma dada grandeza f\u00edsica. Estamos interessados em escrever esse operador numa forma diagonal D D , seguindo uma transforma\u00e7\u00e3o de similiaridade S S : D = S^{-1}C\\,S. D = S^{-1}C\\,S. Como fazemos para encontrar a transforma\u00e7\u00e3o S S ? Procedimento para encontrar a transforma\u00e7\u00e3o S Encontre os autovalore e autovetores da matriz C C Normalize os autovetores de C C Forme a matriz S S de modo que as colunas dessa matriz sejam os autovetores (colunas) normalizados de C C A matriz S^{-1} S^{-1} \u00e9 a matriz inversa 1 de S S , tal que S^{-1}S=SS^{-1}=\\mathbb{1} S^{-1}S=SS^{-1}=\\mathbb{1} . Fica como um exerc\u00edcio sugerido demonstrar que este procedimento resulta na forma diagonal D = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} D = \\begin{pmatrix} \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\end{pmatrix} = \\sum_i \\lambda_i \\ket{u_i} \\bra{u_i} onde \\lambda_i \\lambda_i s\u00e3o os autovalores de C C e D D . Ser\u00e1 que esse procedimento funciona sempre? N\u00e3o, nem sempre! Nem todas as matrizes s\u00e3o diagnaliz\u00e1veis. Para isso algumas condi\u00e7\u00f5es devem ser satisfeitas pela matriz C C . A primeira condi\u00e7\u00e3o \u00e9 que para ser diagonaliz\u00e1vel uma matriz deve ter autovetores que geram o espa\u00e7o. Assim, se todos os autovetores forem distintos, h\u00e1 uma boa chance dela ser diagonaliz\u00e1vel. Mesmo quando h\u00e1 ra\u00edzes m\u00faltiplias (degeneresc\u00eancia), em alguns casos, a matriz ainda pode ser diagonalizada, mas n\u00e3o sempre. Pode-se mostrar, por\u00e9m, que toda matriz normal \u00e9 diagonaliz\u00e1vel. Embora, essa n\u00e3o \u00e9 uma condi\u00e7\u00e3o necess\u00e1ria. Al\u00e9m disso, todas as matrizes hermitianas e todas as matrizes unit\u00e1rias tamb\u00e9m s\u00e3o diagonaliz\u00e1veis.","title":"5.8 Diagonaliza\u00e7\u00e3o de operadores"},{"location":"Aulas_S7/#59-subespacos-e-projetores","text":"Considere um espa\u00e7o n-dimensional \\mathcal{H} \\mathcal{H} com um base ortonormal \\ket{1},\\ket{2},\\dots,\\ket{n} \\ket{1},\\ket{2},\\dots,\\ket{n} . Um subespa\u00e7o \\mathcal{S} \\mathcal{S} de \\mathcal{H} \\mathcal{H} \u00e9 um subconjunto de \\mathcal{H} \\mathcal{H} tal que \\mathcal{S} \\mathcal{S} \u00e9 ele pr\u00f3prio um espa\u00e7o vetorial, com respeito as opera\u00e7\u00f5es de soma e multplica\u00e7\u00e3o por um escalar. Podemos verificar que \\mathcal{S} \\mathcal{S} \u00e9 um espa\u00e7o vetorial usando os mesmos crit\u00e9rios usuais, mas se \\mathcal{S} \\mathcal{S} for um subconjunto de \\mathcal{H} \\mathcal{H} , basta verificar dois crit\u00e9rios O vetor zero (nulo) pertence a \\mathcal{S} \\mathcal{S} Para todos os vetores \\ket{u} \\ket{u} , \\ket{v} \\ket{v} em \\mathcal{S} \\mathcal{S} e escalar \\alpha \\alpha , temos que \\ket{u}+\\ket{v} \\ket{u}+\\ket{v} e \\alpha \\ket{u} \\alpha \\ket{u} pertencem a \\mathcal{S} \\mathcal{S} . Para isso \u00e9 \u00fatil usar o operador projetor, que j\u00e1 discutimos na se\u00e7\u00e3o 5.6 . Relembrando sua defini\u00e7\u00e3o: Operador de proje\u00e7\u00e3o: projetor Suponha um subespa\u00e7o de dimens\u00e3o m m , onde m<n m<n , que tem uma base ortonormal \\mathcal{U} = \\{ \\ket{u_1},\\ket{u_2},\\dots,\\ket{u_m} \\} \\mathcal{U} = \\{ \\ket{u_1},\\ket{u_2},\\dots,\\ket{u_m} \\} . O operador de proje\u00e7\u00e3o \\hat{P}_u \\hat{P}_u \u00e9 dado por \\hat{P}_u =\\sum_{i=1}^{m} \\ket{u_i]} \\bra{u_i}. \\hat{P}_u =\\sum_{i=1}^{m} \\ket{u_i]} \\bra{u_i}. O operador de proje\u00e7\u00e3o tem duas propriedades importantes: \\hat{P}=\\hat{P}^\\dagger \\hat{P}=\\hat{P}^\\dagger , isto \u00e9, ele \u00e9 um operador hermitiano. \\hat{P}=\\hat{P}\\hat{P} \\equiv \\hat{P}^2 \\hat{P}=\\hat{P}\\hat{P} \\equiv \\hat{P}^2 , \u00e9 igual ao seu quadrado. Podemos formar o projetor de apenas um vetor da base \\hat{P}_i=\\ket{u_i}\\bra{u_i} \\hat{P}_i=\\ket{u_i}\\bra{u_i} (imagine que o somat\u00f3rio estende-se apenas ao vetor \\ket{u_i} \\ket{u_i} ). Esse operador \u00e9 totalmente v\u00e1lico e obedece as duas propriedades acima. Ele claramente \u00e9 hermitiano, por exemplo, e tamb\u00e9m obedece a segunda propriedade ( verifique! ). Considere, por exemplo, o efeito de \\hat{P}_i \\ket{\\Psi} \\hat{P}_i \\ket{\\Psi} : \\hat{P}_i \\ket{\\Psi} = \\ket{u_i} \\bra{u_i} \\Psi\\rangle = (\\bra{U_i}\\Psi\\rangle) \\ket{u_i}. \\hat{P}_i \\ket{\\Psi} = \\ket{u_i} \\bra{u_i} \\Psi\\rangle = (\\bra{U_i}\\Psi\\rangle) \\ket{u_i}. Veja que isso efetivamente \"projeta\" a componente de \\ket{\\Psi} \\ket{\\Psi} na dire\u00e7\u00e3o do vetor da base \\ket{u_i} \\ket{u_i} . Observe tamb\u00e9m que o projetor de um dado espa\u00e7o (subespa\u00e7o) \u00e9 igual ao opeador identidade daquele espa\u00e7o (subespa\u00e7o), satisfazendo a rela\u00e7\u00e3o de completeza daquele espa\u00e7o (subespa\u00e7o): \\mathbf{I}_m = \\sum_{i=1}^{m} \\ket{u_i} \\bra{u_i} = \\sum_{i=1}^{m} P_i = \\mathbb{1}_m \\mathbf{I}_m = \\sum_{i=1}^{m} \\ket{u_i} \\bra{u_i} = \\sum_{i=1}^{m} P_i = \\mathbb{1}_m onde \\mathbf{I}_m=\\mathbb{1}_m \\mathbf{I}_m=\\mathbb{1}_m expressa explicitamente uma matriz identidade de ordem m m .","title":"5.9 Subespa\u00e7os e projetores"},{"location":"Aulas_S7/#510-funcoes-de-operadores","text":"Assim como podemos expandir uma fun\u00e7\u00e3o ordin\u00e1ria f(x) f(x) numa s\u00e9rie de Taylor 2 : f(x) = \\sum_{i=0}^{\\infty} a_i x^i f(x) = \\sum_{i=0}^{\\infty} a_i x^i podemos, de forma similar, generalizar essa ideia para definir a fun\u00e7\u00e3o de um operador F(\\hat{A}) F(\\hat{A}) : F(\\hat{A}) = \\sum_{i=0}^{\\infty} a_i \\hat{A}^i F(\\hat{A}) = \\sum_{i=0}^{\\infty} a_i \\hat{A}^i Note, por\u00e9m, que em geral os coeficientes a_i a_i podem ser n\u00fameros complexos. Exemplo: exponencial de um operador Vamos considerar um exemplo bastante importante na mec\u00e2nica qu\u00e2ntica, que \u00e9 a exponencial de um operador. Usando a rela\u00e7\u00e3o que aprendemos entre operadores e matrizes, usaremos a nota\u00e7\u00e3o de matrizes, para simplificar. \\operatorname{exp}(H) = \\operatorname{e}^H = \\sum_{k=0}^{\\infty} \\frac{1}{k!} H^k = \\mathbf{I} + H + \\frac{1}{2} H^2 + \\frac{1}{6} H^3 + \\dots \\operatorname{exp}(H) = \\operatorname{e}^H = \\sum_{k=0}^{\\infty} \\frac{1}{k!} H^k = \\mathbf{I} + H + \\frac{1}{2} H^2 + \\frac{1}{6} H^3 + \\dots onde \\mathbf{I} \\mathbf{I} \u00e9 matriz identidade, e H^k H^k \u00e9 pot\u00eancia k k da matriz H H .","title":"5.10 Fun\u00e7\u00f5es de operadores"},{"location":"Aulas_S7/#511-generalizacao-para-espacos-continuos","text":"Para ajudar a fixar as ideias de espa\u00e7os vetorias, aproveitando a analogia com os vetores do espa\u00e7o euclidiano, que estamos acostumados na F\u00edsica, at\u00e9 este ponto temos nos limitado \u00e0 espa\u00e7os discretos de dimens\u00f5es finitas. Por\u00e9m, o formalismo de espa\u00e7os vetorias pode ser facilmente extendido tamb\u00e9m a espa\u00e7os mais gerais, como espa\u00e7os cont\u00ednuos e infinitos. Neste caso, em geral, estamos falando do espa\u00e7o das fun\u00e7\u00f5es cont\u00ednuas. Na verdade, mesmo sem perceber, j\u00e1 temos alguma familiaridade com esse tipo de espa\u00e7o, mesmo na mec\u00e2nica qu\u00e2ntica, pois come\u00e7amos a nossa discuss\u00e3o neste curso, falando da equa\u00e7\u00e3o de Schr\u00f6dinger e fun\u00e7\u00f5es de ondas. Pois ent\u00e3o, como seria de se esperar, as fun\u00e7\u00e3os de ondas da mec\u00e2nica qu\u00e2ntica formam um espa\u00e7o vetorial, com as mesmas propriedades dos espa\u00e7os discretos que temos discutidos at\u00e9 agora. A seguir fazeremos uma breve introdu\u00e7\u00e3o da nota\u00e7\u00e3o usada para descrever espa\u00e7os vetorias cont\u00ednuos na mec\u00e2nica qu\u00e2ntica. Vetores Assim como os vetores de um espa\u00e7o discreto e finito podem ser expandidos numa base ortonormal \\{ \\ket{u_i},\\ i=1,2,\\dots \\} \\{ \\ket{u_i},\\ i=1,2,\\dots \\} com um n\u00famero finito de componentes, os vetores de um espa\u00e7o cont\u00ednuo s\u00e3o representados numa base ortonomal \\{ \\ket{w_{\\alpha}},\\ x_1 \\le \\alpha \\le x_2 \\} \\{ \\ket{w_{\\alpha}},\\ x_1 \\le \\alpha \\le x_2 \\} por um n\u00famero infinito de pontos no intervalo [a,b] [a,b] . Isto \u00e9, os (infinitos) coeficientes da expans\u00e3o s\u00e3o representados por uma fun\u00e7\u00e3o c(\\alpha) c(\\alpha) , nesse intervalo. Neste caso a expans\u00e3o \u00e9 escrita como: \\ket{\\psi} = \\int c(\\alpha) \\ket{w_{\\alpha}} d \\alpha \\ket{\\psi} = \\int c(\\alpha) \\ket{w_{\\alpha}} d \\alpha Produto interno Sendo o vetor \\ket{\\phi}=\\int b(\\alpha) \\ket{w_{\\alpha}} d \\alpha \\ket{\\phi}=\\int b(\\alpha) \\ket{w_{\\alpha}} d \\alpha . O produto interno dos vetores \\ket{\\phi} \\ket{\\phi} e \\ket{\\psi} \\ket{\\psi} , sendo os dois vetores expressos na base \\ket{w_{\\alpha}} \\ket{w_{\\alpha}} \u00e9 dado por: \\bra{\\phi}\\psi\\rangle = \\int b(\\alpha)^* c(\\alpha) d\\alpha \\bra{\\phi}\\psi\\rangle = \\int b(\\alpha)^* c(\\alpha) d\\alpha Bases ortonormal No espa\u00e7o discreto temos: \\langle u_i | u_j \\rangle = \\delta_{ij} \\langle u_i | u_j \\rangle = \\delta_{ij} onde \\delta_{ij} \\delta_{ij} \u00e9 o delta de Kronecker. No espa\u00e7o cont\u00ednuo temos: \\langle w_{\\alpha} | w_{\\alpha'} \\rangle = \\delta(\\alpha-\\alpha') \\langle w_{\\alpha} | w_{\\alpha'} \\rangle = \\delta(\\alpha-\\alpha') onde \\delta(\\alpha-\\alpha') \\delta(\\alpha-\\alpha') \u00e9 a fun\u00e7\u00e3o delta de Dirac. Rela\u00e7\u00e3o de completeza \\sum_i \\ket{u_i} \\bra{u_i} = \\mathbf{\\hat{1}} \\sum_i \\ket{u_i} \\bra{u_i} = \\mathbf{\\hat{1}} \\int \\ket{w_{\\alpha}} \\bra{w_{\\alpha}} d\\alpha = \\mathbf{\\hat{1}} \\int \\ket{w_{\\alpha}} \\bra{w_{\\alpha}} d\\alpha = \\mathbf{\\hat{1}} Representa\u00e7\u00e3o de operadores Podemos tamb\u00e9m estender a representa\u00e7\u00e3o de um operador para um espa\u00e7o cont\u00ednuo de maneira bastante similar e natural. Se imaginarmos uma representa\u00e7\u00e3o matricial, o operador corresponderia a uma matriz \"infinita\", com um n\u00famero infito de linhas e colunas, e os elementos de matrizes s\u00e3o pontos: \\hat{A} \\Rightarrow\\left(\\begin{array}{ccc} \\ddots & \\vdots & \\cdots \\\\ \\cdots & A (\\alpha, \\alpha^{\\prime} ) & \\cdots \\\\ \\cdots & \\cdots & \\ddots \\end{array}\\right). \\hat{A} \\Rightarrow\\left(\\begin{array}{ccc} \\ddots & \\vdots & \\cdots \\\\ \\cdots & A (\\alpha, \\alpha^{\\prime} ) & \\cdots \\\\ \\cdots & \\cdots & \\ddots \\end{array}\\right). Seguindo numa analogia geom\u00e9trica, podemos imaginar um plano de pontos (elementos de matriz) cujos valores s\u00e3o dados por uma fun\u00e7\u00e3o de duas var\u00edaveis A(\\alpha, \\alpha^{\\prime}) A(\\alpha, \\alpha^{\\prime}) , determinador por: A(\\alpha, \\alpha^{\\prime}) = \\int \\bra{w_{\\alpha}} A \\ket{w_{\\alpha}} d \\alpha A(\\alpha, \\alpha^{\\prime}) = \\int \\bra{w_{\\alpha}} A \\ket{w_{\\alpha}} d \\alpha","title":"5.11 Generaliza\u00e7\u00e3o para espa\u00e7os cont\u00ednuos"},{"location":"Aulas_S7/#funcoes-de-ondas","text":"A generaliza\u00e7\u00e3o para espa\u00e7os cont\u00ednuos nos permite conectar as diferenes representa\u00e7\u00f5es da mec\u00e2nica qu\u00e2ntica, assim como v\u00e1rias ideias j\u00e1 discutidas neste curso, mas que pareciam estar, literalmente, em \"espa\u00e7os\" (contextos) diferentes. Por exemplo, nos permite conectar a mec\u00e2nica ondulat\u00f3ria com a mec\u00e2ncia matricial, assim como formula\u00e7\u00e3o em termos de fun\u00e7\u00f5es de ondas \\psi(x) \\psi(x) com o vetor \\ket{\\psi} \\ket{\\psi} . Na linguagem dos espa\u00e7os vetoriais cont\u00ednuos, a fun\u00e7\u00e3o cont\u00ednua x x \u00e9 uma representa\u00e7\u00e3o (coeficientes) do vetor \\ket{x} \\ket{x} , da mesma forma, o momento linear est\u00e1 relacionado a um vetor \\ket{p} \\ket{p} . Nesta descri\u00e7\u00e3o, as fun\u00e7\u00e3o de ondas \\psi(x) \\psi(x) e \\bar{\\psi}(p) \\bar{\\psi}(p) s\u00e3o dadas por \\psi(x) = \\bra{x} \\psi \\rangle, \\qquad \\bar{\\psi}(p) = \\bra{p} \\psi \\rangle \\psi(x) = \\bra{x} \\psi \\rangle, \\qquad \\bar{\\psi}(p) = \\bra{p} \\psi \\rangle e correspondem, respectivamente, \u00e0s representa\u00e7\u00f5es do vetor \\ket{\\psi} \\ket{\\psi} nas bases de posi\u00e7\u00e3o e momento, definidas pelas fun\u00e7\u00f5es \\xi_{x_0}(x)=\\delta(x-x_0), \\qquad v_{p_0}=\\frac{1}{\\sqrt{2\\pi}\\hbar}e^{ip_0x/\\hbar}. \\xi_{x_0}(x)=\\delta(x-x_0), \\qquad v_{p_0}=\\frac{1}{\\sqrt{2\\pi}\\hbar}e^{ip_0x/\\hbar}. Observe que essas fun\u00e7\u00f5es de base satisfazem as condi\u00e7\u00f5es necess\u00e1rias \\bra{x}x'\\rangle = \\delta(x-x'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{x} \\bra{x} dx \\bra{x}x'\\rangle = \\delta(x-x'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{x} \\bra{x} dx \\bra{p}p'\\rangle = \\delta(p-p'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{p} \\bra{p} dp \\bra{p}p'\\rangle = \\delta(p-p'), \\qquad \\mathbf{\\hat{1}}=\\int \\ket{p} \\bra{p} dp Al\u00e9m disso, o produto interno entre posi\u00e7\u00e3o e momento \u00e9 dado por \\bra{x}p\\rangle = \\bra{p}x\\rangle^* = \\frac{1}{\\sqrt{2\\pi}\\hbar}e^{i p_0 x /\\hbar}. \\bra{x}p\\rangle = \\bra{p}x\\rangle^* = \\frac{1}{\\sqrt{2\\pi}\\hbar}e^{i p_0 x /\\hbar}. Finalmente, usando a rela\u00e7\u00e3o de completeza podemos mostrar que as fun\u00e7\u00f5es de ondas nos espa\u00e7os da posi\u00e7\u00e3o e momento est\u00e3o relacionados atrav\u00e9s de uma transformada de Fourier: \\begin{aligned} \\psi(x) &=\\langle x | \\psi\\rangle=\\langle x|1| \\psi\\rangle=\\left\\langle x\\left|\\left(\\int|p\\rangle\\langle p| d p\\right)\\right| \\psi\\right> \\\\ &=\\int\\langle x | p\\rangle\\langle p | \\psi\\rangle d p \\\\ &=\\frac{1}{\\sqrt{2 \\pi} \\hbar} \\int e^{i \\frac{p x}{\\hbar}} \\bar{\\psi}(p) d p \\end{aligned} \\begin{aligned} \\psi(x) &=\\langle x | \\psi\\rangle=\\langle x|1| \\psi\\rangle=\\left\\langle x\\left|\\left(\\int|p\\rangle\\langle p| d p\\right)\\right| \\psi\\right> \\\\ &=\\int\\langle x | p\\rangle\\langle p | \\psi\\rangle d p \\\\ &=\\frac{1}{\\sqrt{2 \\pi} \\hbar} \\int e^{i \\frac{p x}{\\hbar}} \\bar{\\psi}(p) d p \\end{aligned}","title":"Fun\u00e7\u00f5es de ondas"},{"location":"Aulas_S7/#512-produto-tensorial","text":"Se um \\mathcal{V} \\mathcal{V} \u00e9 um espa\u00e7o de dimens\u00e3o p p e \\mathcal{W} \\mathcal{W} \u00e9 um espa\u00e7o de dimens\u00e3o q q , ent\u00e3o o espa\u00e7o V \\otimes W V \\otimes W tem dimens\u00e3o p q p q . Seja |v\\rangle \\in V |v\\rangle \\in V e |w\\rangle \\in W |w\\rangle \\in W dois vetores desses espa\u00e7os. Ent\u00e3o o vetor |v\\rangle \\otimes|w\\rangle \\in V \\otimes W |v\\rangle \\otimes|w\\rangle \\in V \\otimes W . Nota\u00e7\u00f5es alternativas do vetor |v\\rangle \\otimes|w\\rangle |v\\rangle \\otimes|w\\rangle incluem |v\\rangle|w\\rangle |v\\rangle|w\\rangle e |v w\\rangle |v w\\rangle . Se A A \u00e9 um operador que atua no espa\u00e7o V V e B B \u00e9 um operador que atua no espa\u00e7o W, W, ent\u00e3o o operador formado pelo produto tensorial dado por A \\otimes B A \\otimes B atua nos vetores |v\\rangle \\otimes|w\\rangle |v\\rangle \\otimes|w\\rangle da seguinte maneira: (A \\otimes B)|v\\rangle|w\\rangle=(A|v\\rangle)(B|w\\rangle) (A \\otimes B)|v\\rangle|w\\rangle=(A|v\\rangle)(B|w\\rangle) Considerando agora a norma de um vetor que perten\u00e7a a V \\otimes W . V \\otimes W . Suponha que |\\psi\\rangle=|v\\rangle|w\\rangle. |\\psi\\rangle=|v\\rangle|w\\rangle. Neste caso, a norma \u00e9 dada por \\langle\\psi | \\psi\\rangle=\\langle v | v\\rangle\\langle w | w\\rangle \\langle\\psi | \\psi\\rangle=\\langle v | v\\rangle\\langle w | w\\rangle","title":"5.12 Produto tensorial"},{"location":"Aulas_S7/#representacao-matricial-do-produto-tensorial","text":"Seja A A uma matriz (m \\times n) (m \\times n) e B B uma matriz (p \\times q) (p \\times q) . O produto tensorial A \\otimes B A \\otimes B \u00e9 a matriz com m p m p linhas e n q n q colunas. Os elementos desta matriz s\u00e3o constru\u00eddos pelas submatrizes: A_{i j} B A_{i j} B ou seja, multiplique B B por cada componente de A A e arrange essas submatrizes numa matriz maior. A matriz completa, representando o produto tensorial \u00e9 dada por: A=\\left(\\begin{array}{cccc} A_{11} B & A_{12} B & \\dots & A_{1 n} B \\\\ A_{21} B & A_{22} B & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{m 1} B & \\dots & \\dots & A_{m n} B \\end{array}\\right) A=\\left(\\begin{array}{cccc} A_{11} B & A_{12} B & \\dots & A_{1 n} B \\\\ A_{21} B & A_{22} B & \\dots & \\vdots \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{m 1} B & \\dots & \\dots & A_{m n} B \\end{array}\\right) Um vetor coluna em duas dimens\u00f5es possui duas linhas e uma coluna. Como o produto tensorial entre uma matriz (m \\times n) (m \\times n) e uma matriz (p \\times q) (p \\times q) tem m p m p linhas e n q n q colunas, o produto tensorial entre as duas matrizes (2 \\times 1) (2 \\times 1) ter\u00e1 (2 \\times 2) = 4 (2 \\times 2) = 4 linhas e (1 \\times 1) = 1 (1 \\times 1) = 1 coluna. A forma de calcular isso \u00e9: \\left(\\begin{array}{l} a \\\\ b \\end{array}\\right) \\otimes\\left(\\begin{array}{l} c \\\\ d \\end{array}\\right)=\\left(\\begin{array}{l} a c \\\\ a d \\\\ b c \\\\ b d \\end{array}\\right) \\left(\\begin{array}{l} a \\\\ b \\end{array}\\right) \\otimes\\left(\\begin{array}{l} c \\\\ d \\end{array}\\right)=\\left(\\begin{array}{l} a c \\\\ a d \\\\ b c \\\\ b d \\end{array}\\right) Agora considere o produto tensorial de duas matrizes 2 \\times 2 2 \\times 2 . A=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right), \\quad B=\\left(\\begin{array}{ll} w & x \\\\ y & z \\end{array}\\right) A=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right), \\quad B=\\left(\\begin{array}{ll} w & x \\\\ y & z \\end{array}\\right) Esse produto tensorial ser\u00e1 uma matriz (4 \\times 4) (4 \\times 4) : A \\otimes B=\\left(\\begin{array}{cc} a B & b B \\\\ c B & d B \\end{array}\\right)=\\left(\\begin{array}{cccc} a w & a x & b w & b x \\\\ a y & a z & b y & b z \\\\ c w & c x & d w & d x \\\\ c y & c z & d y & d z \\end{array}\\right) A \\otimes B=\\left(\\begin{array}{cc} a B & b B \\\\ c B & d B \\end{array}\\right)=\\left(\\begin{array}{cccc} a w & a x & b w & b x \\\\ a y & a z & b y & b z \\\\ c w & c x & d w & d x \\\\ c y & c z & d y & d z \\end{array}\\right) Como calcular a matriz inversa Se \\mathbf{A} \\mathbf{A} uma matriz n\u00e3o singular, sua inversa \u00e9 dada por: \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\mathbf{C}^{\\mathrm{T}}=\\frac{1}{|\\mathbf{A}|}\\left(\\begin{array}{cccc} \\mathbf{C}_{11} & \\mathbf{C}_{21} & \\cdots & \\mathbf{C}_{n 1} \\\\ \\mathbf{C}_{12} & \\mathbf{C}_{22} & \\cdots & \\mathbf{C}_{n 2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{C}_{1 n} & \\mathbf{C}_{2 n} & \\cdots & \\mathbf{C}_{n n} \\end{array}\\right) \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\mathbf{C}^{\\mathrm{T}}=\\frac{1}{|\\mathbf{A}|}\\left(\\begin{array}{cccc} \\mathbf{C}_{11} & \\mathbf{C}_{21} & \\cdots & \\mathbf{C}_{n 1} \\\\ \\mathbf{C}_{12} & \\mathbf{C}_{22} & \\cdots & \\mathbf{C}_{n 2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf{C}_{1 n} & \\mathbf{C}_{2 n} & \\cdots & \\mathbf{C}_{n n} \\end{array}\\right) onde \\mathbf{C} \\mathbf{C} \u00e9 a matriz de cofatores \\mathbf{C_{ij}} = (-1)^{i+j}\\ \\mathbf{D_{ij}} \\mathbf{C_{ij}} = (-1)^{i+j}\\ \\mathbf{D_{ij}} , sendo \\mathbf{D_{ij}} \\mathbf{D_{ij}} o determinante da matriz menor \\mathbf{\\tilde{A}_{ij}} \\mathbf{\\tilde{A}_{ij}} , obtida a partir da exclus\u00e3o da linha \\mathbf{i} \\mathbf{i} e coluna \\mathbf{j} \\mathbf{j} da matriz \\mathbf{A} \\mathbf{A} . O termo |\\mathbf{A}|=\\operatorname{det}(\\mathbf{A}) |\\mathbf{A}|=\\operatorname{det}(\\mathbf{A}) representa o determinante de \\mathbf{A} \\mathbf{A} . Se \\mathbf{A} \\mathbf{A} for uma matriz 3x3, sua inversa \u00e9 dada por: \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\left(\\begin{array}{lll} \\left|\\begin{array}{ll} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{12} \\\\ a_{33} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{23} & a_{21} \\\\ a_{33} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{11} \\\\ a_{23} & a_{21} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{11} \\\\ a_{32} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array}\\right| \\end{array} \\right) \\mathbf{A}^{-1}=\\frac{1}{|\\mathbf{A}|} \\left(\\begin{array}{lll} \\left|\\begin{array}{ll} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{12} \\\\ a_{33} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{13} \\\\ a_{22} & a_{23} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{23} & a_{21} \\\\ a_{33} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{13} & a_{11} \\\\ a_{23} & a_{21} \\end{array}\\right| \\\\ \\left|\\begin{array}{ll} a_{21} & a_{22} \\\\ a_{31} & a_{32} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{12} & a_{11} \\\\ a_{32} & a_{31} \\end{array}\\right| & \\left|\\begin{array}{ll} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{array}\\right| \\end{array} \\right) Se \\mathbf{A} \\mathbf{A} for uma matriz 2x2, sua inversa \u00e9 simplesmente: \\mathbf{A}^{-1}=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right)^{-1}=\\frac{1}{\\operatorname{det}(\\mathbf{A})}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right)=\\frac{1}{a d-b c}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right) \\mathbf{A}^{-1}=\\left(\\begin{array}{ll} a & b \\\\ c & d \\end{array}\\right)^{-1}=\\frac{1}{\\operatorname{det}(\\mathbf{A})}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right)=\\frac{1}{a d-b c}\\left(\\begin{array}{rr} d & -b \\\\ -c & a \\end{array}\\right) \u21a9 S\u00e9ries de Taylor Para uma fun\u00e7\u00e3o cont\u00ednua f(x) f(x) , que seja infinitamente diferenci\u00e1vel, a s\u00e9rie de Taylor \u00e9 uma sua expans\u00e3o em s\u00e9rie de potencias do tipo: f(x_0) = \\sum_{k=1}^{\\infty} \\frac{1}{k!} \\left(\\frac{df}{dx}\\right)^k_{x=x_0} (x-x_0)^k = \\sum_{k=1}^{\\infty} \\frac{f^{('k)}(x_0)}{k!} (x-x_0)^k. f(x_0) = \\sum_{k=1}^{\\infty} \\frac{1}{k!} \\left(\\frac{df}{dx}\\right)^k_{x=x_0} (x-x_0)^k = \\sum_{k=1}^{\\infty} \\frac{f^{('k)}(x_0)}{k!} (x-x_0)^k. \u21a9","title":"Representa\u00e7ao matricial do produto tensorial"},{"location":"about/","text":"Informa\u00e7\u00f5es desta disciplinas Sigla: SFI5774 Tipo: P\u00f3s-Gradua\u00e7\u00e3o Nome: Mec\u00e2nica Qu\u00e2ntica Aplicada \u00c1rea: F\u00edsica Te\u00f3rica e Experimental (76134) Carga hor\u00e1ria: Total: 225 h Te\u00f3rica: 4 h Pr\u00e1tica: 3 h Estudo: 8 h Cr\u00e9ditos: 15 Dura\u00e7\u00e3o: 15 Semanas Conte\u00fado na ementa: Operadores em mec\u00e2nica qu\u00e2ntica. Postulados da mec\u00e2nica qu\u00e2ntica e equa\u00e7\u00e3o de Schrodinger. Mec\u00e2nica qu\u00e2ntica matricial. Movimento linear e oscilador harm\u00f4nico. Momento angular e \u00e1tomo de hidrog\u00eanio. Teoria de perturba\u00e7\u00e3o e m\u00e9todo variacional. No\u00e7\u00f5es sobre simetrias e representa\u00e7\u00e3o de grupos. Estruturas at\u00f4micas e moleculares. Rota\u00e7\u00f5es e vibra\u00e7\u00f5es moleculares. Transi\u00e7\u00f5es eletr\u00f4nicas moleculares. Propriedades el\u00e9tricas e \u00f3pticas de mol\u00e9culas. Bibliografia oficial: ATKINS, P. W.; FRIEDMAN, R. S. Molecular quantum mechanics. 3 rd ed. Oxford: Oxford University Press, 2001. LEVINE, I. N. Quantum chemistry. 3 rd ed. Boston: Allyn and Bacon, 1983. Bibliografia complementar:","title":"About"},{"location":"about/#informacoes-desta-disciplinas","text":"Sigla: SFI5774 Tipo: P\u00f3s-Gradua\u00e7\u00e3o Nome: Mec\u00e2nica Qu\u00e2ntica Aplicada \u00c1rea: F\u00edsica Te\u00f3rica e Experimental (76134)","title":"Informa\u00e7\u00f5es desta disciplinas"},{"location":"about/#carga-horaria","text":"Total: 225 h Te\u00f3rica: 4 h Pr\u00e1tica: 3 h Estudo: 8 h","title":"Carga hor\u00e1ria:"},{"location":"about/#creditos-15","text":"Dura\u00e7\u00e3o: 15 Semanas","title":"Cr\u00e9ditos:  15"},{"location":"about/#conteudo-na-ementa","text":"Operadores em mec\u00e2nica qu\u00e2ntica. Postulados da mec\u00e2nica qu\u00e2ntica e equa\u00e7\u00e3o de Schrodinger. Mec\u00e2nica qu\u00e2ntica matricial. Movimento linear e oscilador harm\u00f4nico. Momento angular e \u00e1tomo de hidrog\u00eanio. Teoria de perturba\u00e7\u00e3o e m\u00e9todo variacional. No\u00e7\u00f5es sobre simetrias e representa\u00e7\u00e3o de grupos. Estruturas at\u00f4micas e moleculares. Rota\u00e7\u00f5es e vibra\u00e7\u00f5es moleculares. Transi\u00e7\u00f5es eletr\u00f4nicas moleculares. Propriedades el\u00e9tricas e \u00f3pticas de mol\u00e9culas. Bibliografia oficial: ATKINS, P. W.; FRIEDMAN, R. S. Molecular quantum mechanics. 3 rd ed. Oxford: Oxford University Press, 2001. LEVINE, I. N. Quantum chemistry. 3 rd ed. Boston: Allyn and Bacon, 1983. Bibliografia complementar:","title":"Conte\u00fado na ementa:"}]}